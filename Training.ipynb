{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11328691,"sourceType":"datasetVersion","datasetId":7086528},{"sourceId":10059115,"sourceType":"datasetVersion","datasetId":5303547}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport csv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, ConfusionMatrixDisplay\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Masking, InputLayer, Conv3D, MaxPooling3D, Flatten, BatchNormalization, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Paths","metadata":{}},{"cell_type":"code","source":"features_audio_input_path = \"/kaggle/input/truthlie-audio-dwt-features/audio_20\"\n\nfeatures_video_input_path = \"/kaggle/input/truth-lie-features\"\ntrain_features_path = features_video_input_path + \"/train_features.csv\"\nval_features_path = features_video_input_path + \"/val_features.csv\"\ntest_features_path = features_video_input_path + \"/test_features.csv\"\n\n# define path to save model\nmodel_path = 'binary_model.weights.h5'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Audio","metadata":{}},{"cell_type":"markdown","source":"## Metrics - Audio","metadata":{}},{"cell_type":"code","source":"# Dictionary for Holdout metrics\nmetrics_holdout = {\n    \"Accuracy\": [],\n    \"Precision (0)\": [], \"Precision (1)\": [],\n    \"Recall (0)\": [], \"Recall (1)\": [],\n    \"F1 (0)\": [], \"F1 (1)\": [],\n    \"AUC\": []\n}\n\n# Dictionary for Cross-Validation metrics\nmetrics_cross = {\n    \"Mean Accuracy\": [], \"Std Accuracy\": [],\n    \"Mean Precision (0)\": [], \"Std Precision (0)\": [],\n    \"Mean Precision (1)\": [], \"Std Precision (1)\": [],\n    \"Mean Recall (0)\": [], \"Std Recall (0)\": [],\n    \"Mean Recall (1)\": [], \"Std Recall (1)\": [],\n    \"Mean F1 (0)\": [], \"Std F1 (0)\": [],\n    \"Mean F1 (1)\": [], \"Std F1 (1)\": [],\n    \"Mean AUC\": [], \"Std AUC\": []\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Features - Audio","metadata":{}},{"cell_type":"code","source":"# Holdout\ntrain_features_audio_holdout = np.load(features_audio_input_path + \"/train_features.npy\")\nval_features_audio_holdout = np.load(features_audio_input_path + \"/val_features.npy\")\ntest_features_audio_holdout = np.load(features_audio_input_path + \"/test_features.npy\")\ntrain_labels_audio_holdout = np.load(features_audio_input_path + \"/train_labels.npy\")\nval_labels_audio_holdout = np.load(features_audio_input_path + \"/val_labels.npy\")\ntest_labels_audio_holdout = np.load(features_audio_input_path + \"/test_labels.npy\")\n\nprint(train_features_audio_holdout.shape, train_labels_audio_holdout.shape)\nprint(val_features_audio_holdout.shape, val_labels_audio_holdout.shape)\nprint(test_features_audio_holdout.shape, test_labels_audio_holdout.shape)\n\nprint(\"\\n\")\n\n# Cross-Validation\ndata_audio_cross = []\nfor i in range(4):\n    fold_features = np.load(features_audio_input_path + f\"/fold_{i}_features.npy\")\n    fold_labels = np.load(features_audio_input_path + f\"/fold_{i}_labels.npy\")\n    print(fold_features.shape, fold_labels.shape)\n    data_audio_cross.append((fold_features, fold_labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model - Audio","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(conf_matrix):\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Lie', 'Truth'], yticklabels=['Lie', 'Truth'])\n    plt.xlabel(\"Predict\")\n    plt.ylabel(\"Real\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\ndef lstm_model(input_shape, hidden_size, learning_rate):\n    model = Sequential([\n        InputLayer(shape=input_shape),\n        Masking(mask_value=0.0),\n        LSTM(hidden_size, return_sequences=False, use_cudnn=False),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer=Adam(learning_rate=learning_rate),\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\ndef train_and_evaluate_lstm_sequences(\n    train_features, train_labels,\n    val_features, val_labels,\n    test_features, test_labels,\n    hidden_size=128,\n    num_epochs=20,\n    batch_size=32,\n    learning_rate=0.001,\n    save_res=True,\n    metrics_dict=None\n):\n    \"\"\"\n    Trains and evaluates an LSTM model on sequential data.\n    \n    Parameters:\n    - train_features: numpy array of training features (num_samples, time_steps, num_features)\n    - train_labels: numpy array of training labels\n    - val_features: numpy array of validation features\n    - val_labels: numpy array of validation labels\n    - test_features: numpy array of test features\n    - test_labels: numpy array of test labels\n    - hidden_size: size of the LSTM hidden state\n    - num_epochs: number of training epochs\n    - batch_size: batch size\n    - learning_rate: learning rate\n    - save_res: whether to display and save training results (True/False)\n    \n    Returns:\n    - test_loss: loss value on the test set\n    - test_accuracy: accuracy on the test set\n    \"\"\"\n    # Define the LSTM model\n    model = lstm_model(input_shape=(train_features.shape[1], train_features.shape[2]),\n                       hidden_size=hidden_size, learning_rate=learning_rate)\n    \n    # Train the model\n    history = model.fit(\n        train_features, train_labels,\n        validation_data=(val_features, val_labels),\n        epochs=num_epochs,\n        batch_size=batch_size,\n        verbose=save_res\n    )\n    \n    # Make predictions on the test set\n    test_predictions_proba = model.predict(test_features) \n    test_predictions = (test_predictions_proba > 0.5).astype(\"int32\") \n    \n    # Compute classification metrics\n    accuracy = accuracy_score(test_labels, test_predictions)\n    precision = precision_score(test_labels, test_predictions, zero_division=0, average=None)\n    recall = recall_score(test_labels, test_predictions, zero_division=0, average=None)\n    f1 = f1_score(test_labels, test_predictions, average=None)\n    auc = roc_auc_score(test_labels, test_predictions_proba)\n    \n    if save_res:        \n        # Save metrics to the dictionary\n        metrics_dict[\"Accuracy\"].append(accuracy)\n        metrics_dict[\"Precision (0)\"].append(precision[0])\n        metrics_dict[\"Precision (1)\"].append(precision[1])\n        metrics_dict[\"Recall (0)\"].append(recall[0])\n        metrics_dict[\"Recall (1)\"].append(recall[1])\n        metrics_dict[\"F1 (0)\"].append(f1[0])\n        metrics_dict[\"F1 (1)\"].append(f1[1])\n        metrics_dict[\"AUC\"].append(auc)\n    \n        # Confusion Matrix\n        conf_matrix = confusion_matrix(test_labels, test_predictions)\n        plot_confusion_matrix(conf_matrix)\n \n    return accuracy, precision[0], precision[1], recall[0], recall[1], f1[0], f1[1], auc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Holdout - Audio","metadata":{}},{"cell_type":"code","source":"# Execute Training and Evaluation\nhidden_size = 128\nnum_epochs = 20\nbatch_size = 32\nlearning_rate = 0.001\n    \ntrain_and_evaluate_lstm_sequences(\n    train_features_audio_holdout,\n    train_labels_audio_holdout,\n    val_features_audio_holdout,\n    val_labels_audio_holdout,\n    test_features_audio_holdout,\n    test_labels_audio_holdout,\n    hidden_size=hidden_size,\n    num_epochs=num_epochs,\n    batch_size=batch_size,\n    learning_rate=learning_rate,\n    metrics_dict=metrics_holdout\n)\n\n# Dataframe creation\ndf = pd.DataFrame.from_dict(metrics_holdout, orient='index', columns=[\"NOISE FILTER + DWT + MFCC + LSTM\"])\nprint(df)\nmetrics_holdout = {key: [] for key in metrics_holdout}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cross-Validation - Audio","metadata":{}},{"cell_type":"code","source":"# Function for cross-validation\ndef cross_validation_lstm_sequences(metrics_dict, features_by_fold, num_epochs=50, hidden_size=128, batch_size=32, learning_rate=0.001):\n    accuracy_scores = [] \n    precision_scores_0, recall_scores_0, f1_scores_0 = [], [], []\n    precision_scores_1, recall_scores_1, f1_scores_1 = [], [], []\n    auc_scores = []\n    \n    for fold_idx in range(4):\n        print(f\"---------- Fold {fold_idx + 1} ----------\")\n        \n        # Combine data from other folds for training\n        train_features, train_labels = [], []\n        for i in range(4):\n            if i != fold_idx:\n                train_features.append(features_by_fold[i][0])\n                train_labels.append(features_by_fold[i][1])\n\n        train_features = np.vstack(train_features)\n        train_labels = np.hstack(train_labels)\n        \n        # Process the test fold\n        test_features, test_labels = features_by_fold[fold_idx]\n        \n        # Train and evaluate the LSTM model\n        acc, precision_0, precision_1, recall_0, recall_1, f1_0, f1_1, auc = train_and_evaluate_lstm_sequences(\n            train_features=train_features,  \n            train_labels=train_labels,\n            val_features=test_features,  \n            val_labels=test_labels,\n            test_features=test_features,    \n            test_labels=test_labels,        \n            hidden_size=hidden_size,\n            num_epochs=num_epochs,\n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            save_res=False  \n        )\n        \n        # Save metrics for each fold\n        accuracy_scores.append(acc)\n        precision_scores_0.append(precision_0)\n        recall_scores_0.append(recall_0)\n        f1_scores_0.append(f1_0)\n        precision_scores_1.append(precision_1)\n        recall_scores_1.append(recall_1)\n        f1_scores_1.append(f1_1)\n        auc_scores.append(auc)\n\n        print(f\"Accuracy: {acc:.2f}\")\n\n    # Mean metrics across all folds\n    avg_accuracy = np.mean(accuracy_scores)\n    avg_precision_0, avg_recall_0, avg_f1_0 = np.mean(precision_scores_0), np.mean(recall_scores_0), np.mean(f1_scores_0)\n    avg_precision_1, avg_recall_1, avg_f1_1 = np.mean(precision_scores_1), np.mean(recall_scores_1), np.mean(f1_scores_1)\n    avg_auc = np.mean(auc_scores)\n\n    metrics_dict[\"Mean Accuracy\"].append(avg_accuracy)\n    metrics_dict[\"Mean Precision (0)\"].append(avg_precision_0)\n    metrics_dict[\"Mean Precision (1)\"].append(avg_precision_1)\n    metrics_dict[\"Mean Recall (0)\"].append(avg_recall_0)\n    metrics_dict[\"Mean Recall (1)\"].append(avg_recall_1)\n    metrics_dict[\"Mean F1 (0)\"].append(avg_f1_0)\n    metrics_dict[\"Mean F1 (1)\"].append(avg_f1_1)\n    metrics_dict[\"Mean AUC\"].append(avg_auc)\n\n    # Standard deviation of metrics across all folds\n    std_accuracy = np.std(accuracy_scores)\n    std_precision_0, std_recall_0, std_f1_0 = np.std(precision_scores_0), np.std(recall_scores_0), np.std(f1_scores_0)\n    std_precision_1, std_recall_1, std_f1_1 = np.std(precision_scores_1), np.std(recall_scores_1), np.std(f1_scores_1)\n    std_auc = np.std(auc_scores)\n    \n    metrics_dict[\"Std Accuracy\"].append(std_accuracy)\n    metrics_dict[\"Std Precision (0)\"].append(std_precision_0)\n    metrics_dict[\"Std Precision (1)\"].append(std_precision_1)\n    metrics_dict[\"Std Recall (0)\"].append(std_recall_0)\n    metrics_dict[\"Std Recall (1)\"].append(std_recall_1)\n    metrics_dict[\"Std F1 (0)\"].append(std_f1_0)\n    metrics_dict[\"Std F1 (1)\"].append(std_f1_1)\n    metrics_dict[\"Std AUC\"].append(std_auc)\n\n    print(f\"\\nAvg Accuracy: {avg_accuracy:.2f}\")\n    print(f\"Std Dev Accuracy: {std_accuracy:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cross_validation_lstm_sequences(metrics_cross, data_audio_cross,\n                                    num_epochs=20, hidden_size=128, \n                                    batch_size=32, learning_rate=0.001)\n\n# Dataframe creation\ndf = pd.DataFrame.from_dict(metrics_cross, orient='index', columns=[\"NOISE FILTER + DWT + MFCC + LSTM\"])\nprint(\"\\n\")\nprint(df)\nmetrics_cross = {key: [] for key in metrics_cross}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Video","metadata":{}},{"cell_type":"markdown","source":"## Metrics - Video","metadata":{}},{"cell_type":"code","source":"# Dictionaries for Holdout metrics\nframe_metrics_holdout = {\n    \"Accuracy\": [],\n    \"Precision (0)\": [], \"Precision (1)\": [],\n    \"Recall (0)\": [], \"Recall (1)\": [],\n    \"F1 (0)\": [], \"F1 (1)\": [],\n    \"AUC\": []\n}\nvideo_metrics_holdout_mean = {\n    \"Accuracy\": [],\n    \"Precision (0)\": [], \"Precision (1)\": [],\n    \"Recall (0)\": [], \"Recall (1)\": [],\n    \"F1 (0)\": [], \"F1 (1)\": [],\n    \"AUC\": []\n}\nvideo_metrics_holdout_majority = {\n    \"Accuracy\": [],\n    \"Precision (0)\": [], \"Precision (1)\": [],\n    \"Recall (0)\": [], \"Recall (1)\": [],\n    \"F1 (0)\": [], \"F1 (1)\": [],\n    \"AUC\": []\n}\nvideo_metrics_holdout_threshold = {\n    \"Accuracy\": [],\n    \"Precision (0)\": [], \"Precision (1)\": [],\n    \"Recall (0)\": [], \"Recall (1)\": [],\n    \"F1 (0)\": [], \"F1 (1)\": [],\n    \"AUC\": []\n}\n\n# Dictionaries for Cross-Validation metrics\nframe_metrics_cross = {\n    \"Mean Accuracy\": [], \"Std Accuracy\": [],\n    \"Mean Precision (0)\": [], \"Std Precision (0)\": [],\n    \"Mean Precision (1)\": [], \"Std Precision (1)\": [],\n    \"Mean Recall (0)\": [], \"Std Recall (0)\": [],\n    \"Mean Recall (1)\": [], \"Std Recall (1)\": [],\n    \"Mean F1 (0)\": [], \"Std F1 (0)\": [],\n    \"Mean F1 (1)\": [], \"Std F1 (1)\": [],\n    \"Mean AUC\": [], \"Std AUC\": []\n}\nvideo_metrics_cross_mean = {\n    \"Mean Accuracy\": [], \"Std Accuracy\": [],\n    \"Mean Precision (0)\": [], \"Std Precision (0)\": [],\n    \"Mean Precision (1)\": [], \"Std Precision (1)\": [],\n    \"Mean Recall (0)\": [], \"Std Recall (0)\": [],\n    \"Mean Recall (1)\": [], \"Std Recall (1)\": [],\n    \"Mean F1 (0)\": [], \"Std F1 (0)\": [],\n    \"Mean F1 (1)\": [], \"Std F1 (1)\": [],\n    \"Mean AUC\": [], \"Std AUC\": []\n}\nvideo_metrics_cross_majority = {\n    \"Mean Accuracy\": [], \"Std Accuracy\": [],\n    \"Mean Precision (0)\": [], \"Std Precision (0)\": [],\n    \"Mean Precision (1)\": [], \"Std Precision (1)\": [],\n    \"Mean Recall (0)\": [], \"Std Recall (0)\": [],\n    \"Mean Recall (1)\": [], \"Std Recall (1)\": [],\n    \"Mean F1 (0)\": [], \"Std F1 (0)\": [],\n    \"Mean F1 (1)\": [], \"Std F1 (1)\": [],\n    \"Mean AUC\": [], \"Std AUC\": []\n}\nvideo_metrics_cross_threshold = {\n    \"Mean Accuracy\": [], \"Std Accuracy\": [],\n    \"Mean Precision (0)\": [], \"Std Precision (0)\": [],\n    \"Mean Precision (1)\": [], \"Std Precision (1)\": [],\n    \"Mean Recall (0)\": [], \"Std Recall (0)\": [],\n    \"Mean Recall (1)\": [], \"Std Recall (1)\": [],\n    \"Mean F1 (0)\": [], \"Std F1 (0)\": [],\n    \"Mean F1 (1)\": [], \"Std F1 (1)\": [],\n    \"Mean AUC\": [], \"Std AUC\": []\n}\n\n# Function to aggregate frames considering the average\ndef calculate_mean(preds):\n    return np.mean(preds)\n\n# Function to aggregate frames considering the majority\ndef calculate_majority(preds):\n    return int(np.sum(preds > 0.5) > len(preds) / 2)\n\n# Function to aggregate frames considering a threshold\ndef aggregate_by_threshold(preds):\n    threshold = 0.25 * len(preds)\n    count_zeros = (preds < 0.5).sum()\n    return 0 if count_zeros >= threshold else 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Features - Video","metadata":{}},{"cell_type":"code","source":"def upload_dataset(features_path):\n    df_feature = pd.read_csv(features_path)\n    \n    # Remove rows that contain NaN\n    dataset = df_feature.dropna()\n\n    return dataset\n\ntrain_dataset = upload_dataset(train_features_path)\nval_dataset = upload_dataset(val_features_path)\ntest_dataset = upload_dataset(test_features_path)\n\ncross_datasets=[]\nfor i in range(4):\n    fold_features_path = features_video_input_path + f\"/fold_{i}_features.csv\"\n    cross_datasets.append(upload_dataset(fold_features_path))\n\ntrain_list = train_dataset['input'].unique().tolist()\nval_list = val_dataset['input'].unique().tolist()\ntest_list = test_dataset['input'].unique().tolist()\n\ncross_lists = [fold_dataset['input'].unique().tolist() for fold_dataset in cross_datasets]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define coloumns of action units and emotions\naus_cols_aux = list(train_dataset.columns[144:164])\naus_cols_emo = list(train_dataset.columns[164:171])\ncols_groups = [     \n    aus_cols_aux,\n    aus_cols_emo,\n]\ncols_groups_string = \"AUX+EMO\"\n\n# Make sure all groups are the same size, otherwise select the most correlated columns\nmin_features = min(len(group) for group in cols_groups)\ncols_groups = [\n    train_dataset[group + ['label']].corr()['label'].drop('label')\n    .abs().sort_values(ascending=False).head(min_features).index.tolist()\n    for group in cols_groups\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mirror_padding(frames, sequence_length):\n    \"\"\"\n    Applies mirror padding to a sequence of frames if it's shorter than the required sequence length.\n    \n    :param frames: Numpy array of frames (num_frames, feature_dim)\n    :param sequence_length: Desired length of the output sequence\n    :return: Numpy array with mirror padding applied\n    \"\"\"    \n    # Compute the number of padding frames needed\n    padding_needed = sequence_length - len(frames)\n    \n    # Compute how to split padding between the beginning and the end\n    pad_start = padding_needed // 2\n    pad_end = padding_needed - pad_start\n    \n    # Apply mirror (reflect) padding along the time axis\n    padded_frames = np.pad(\n        frames,\n        pad_width=((pad_start, pad_end), (0, 0)),  # Padding along the first dimension (time)\n        mode='reflect'\n    )\n    \n    return padded_frames\n    \n\ndef gen_sequence(id_df, seq_length, cols_groups, frame_step=1, padding=None):\n    \"\"\"\n    Generates sequences of fixed length from a dataframe representing a single sample.\n\n    :param id_df: DataFrame containing the data of a single video sample\n    :param seq_length: Length of the output sequences\n    :param cols_groups: List of lists, each sublist containing column names that belong to a group (e.g., audio, facial)\n    :param frame_step: Step between frames when sliding the window\n    :param padding: Padding method to apply ('const' for zero-padding, 'mirror' for reflective padding)\n    :yield: Tuple of (stacked sequence, label, video_name)\n    \"\"\"\n    data_matrices = []\n    \n    # Extract and preprocess each group of columns\n    for group in cols_groups:\n        group_df = id_df[group]\n        data_matrices.append(group_df.values)\n\n    # Apply padding to each group if needed\n    if padding == 'const':\n        padded_matrices = []\n        for data_matrix in data_matrices:\n            if data_matrix.shape[0] < seq_length:\n                padding_needed = seq_length - data_matrix.shape[0]\n                pre_padding = padding_needed // 3\n                post_padding = padding_needed - pre_padding\n                pad = np.full((pre_padding, data_matrix.shape[1]), 0)\n                data_matrix = np.vstack([pad, data_matrix, np.full((post_padding, data_matrix.shape[1]), 0)])\n            padded_matrices.append(data_matrix)\n        data_matrices = padded_matrices\n    elif padding == 'mirror':\n        data_matrices = [mirror_padding(data_matrix, seq_length) if data_matrix.shape[0] < seq_length else data_matrix\n                         for data_matrix in data_matrices]\n\n    label = id_df['label'].values[0]\n    video_name = id_df['input'].values[0]\n\n    num_elements = data_matrices[0].shape[0]\n    for start, stop in zip(range(0, num_elements - seq_length + 1, frame_step), range(seq_length, num_elements + 1, frame_step)):\n        # Stack the sequence from all groups along a new axis (group-wise)\n        stacked_sequence = np.stack([matrix[start:stop, :] for matrix in data_matrices], axis=1)\n        yield stacked_sequence, label, video_name","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sequence_length = 64\nframe_step = 1\npadding = 'const'\n\n# Generate sequences for train, val and test\nseq_gen_train = list(list(gen_sequence(train_dataset[train_dataset['input'] == id], \n                                       sequence_length, cols_groups, frame_step, \n                                       padding=None))\n                     for id in train_list)\n\nseq_gen_val = list(list(gen_sequence(val_dataset[val_dataset['input'] == id], \n                                     sequence_length, cols_groups, frame_step, \n                                     padding))\n                   for id in val_list)\n\nseq_gen_test = list(list(gen_sequence(test_dataset[test_dataset['input'] == id], \n                                      sequence_length, cols_groups, frame_step, \n                                      padding))\n                    for id in test_list)\n\n# Remove empty lists\nseq_gen_train = [x for x in seq_gen_train if len(x) > 0]\nseq_gen_val = [x for x in seq_gen_val if len(x) > 0]\nseq_gen_test = [x for x in seq_gen_test if len(x) > 0]\n\n# Extract data from generators\nseq_array_train = [[t[0] for t in sublist] for sublist in seq_gen_train]\nlabel_array_train = [[t[1] for t in sublist] for sublist in seq_gen_train]\nvideo_array_train = [[t[2] for t in sublist] for sublist in seq_gen_train]\n\nseq_array_val = [[t[0] for t in sublist] for sublist in seq_gen_val]\nlabel_array_val = [[t[1] for t in sublist] for sublist in seq_gen_val]\nvideo_array_val = [[t[2] for t in sublist] for sublist in seq_gen_val]\n\nseq_array_test = [[t[0] for t in sublist] for sublist in seq_gen_test]\nlabel_array_test = [[t[1] for t in sublist] for sublist in seq_gen_test]\nvideo_array_test = [[t[2] for t in sublist] for sublist in seq_gen_test]\n\n# Transform lists in arrays\nseq_array_train = np.concatenate(seq_array_train).astype(np.float32)\nlabel_array_train = np.concatenate(label_array_train).astype(np.float32).reshape(-1)\nvideo_array_train = np.concatenate(video_array_train)\n\nseq_array_val = np.concatenate(seq_array_val).astype(np.float32)\nlabel_array_val = np.concatenate(label_array_val).astype(np.float32).reshape(-1)\nvideo_array_val = np.concatenate(video_array_val)\n\nseq_array_test = np.concatenate(seq_array_test).astype(np.float32)\nlabel_array_test = np.concatenate(label_array_test).astype(np.float32).reshape(-1)\nvideo_array_test = np.concatenate(video_array_test)\n\n# Transpose and expand arrays\nseq_array_train = np.transpose(seq_array_train, (0, 1, 3, 2))\nseq_array_train = np.expand_dims(seq_array_train, axis=-1)\nprint(seq_array_train.shape, label_array_train.shape)\n\nseq_array_val = np.transpose(seq_array_val, (0, 1, 3, 2))\nseq_array_val = np.expand_dims(seq_array_val, axis=-1)\nprint(seq_array_val.shape, label_array_val.shape)\n\nseq_array_test = np.transpose(seq_array_test, (0, 1, 3, 2))\nseq_array_test = np.expand_dims(seq_array_test, axis=-1)\nprint(seq_array_test.shape, label_array_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cross_sequences, cross_labels, cross_video_names = [],[],[]\ncross_sequences_pad, cross_labels_pad, cross_video_names_pad = [],[],[]\n\nfor i in range(4):\n    fold_dataset = cross_datasets[i]\n    fold_list = cross_lists[i]\n    \n    seq_gen_fold = list(list(gen_sequence(fold_dataset[fold_dataset['input'] == id], \n                                          sequence_length, cols_groups, frame_step, \n                                          padding=None))\n                 for id in fold_list)\n    seq_gen_fold = [x for x in seq_gen_fold if len(x)>0]\n\n    # Extract data from generators\n    seq_array_fold = [[t[0] for t in sublist] for sublist in seq_gen_fold]\n    label_array_fold = [[t[1] for t in sublist] for sublist in seq_gen_fold]\n    video_array_fold = [[t[2] for t in sublist] for sublist in seq_gen_fold]\n    \n    # Transform lists in arrays\n    fold_sequences = np.concatenate(seq_array_fold).astype(np.float32)\n    fold_labels = np.concatenate(label_array_fold).astype(np.float32).reshape(-1)\n    fold_video_names = np.concatenate(video_array_fold)\n\n    # Transpose and expand\n    fold_sequences = np.transpose(fold_sequences, (0, 1, 3, 2))\n    fold_sequences = np.expand_dims(fold_sequences, axis=-1)\n    \n    print(fold_sequences.shape, fold_labels.shape)    \n    \n    cross_sequences.append(fold_sequences)\n    cross_labels.append(fold_labels)\n    cross_video_names.append(fold_video_names)\n\n    # Padding\n    seq_gen_fold = list(list(gen_sequence(fold_dataset[fold_dataset['input'] == id], \n                                          sequence_length, cols_groups, frame_step, \n                                          padding))\n                 for id in fold_list)\n    seq_gen_fold = [x for x in seq_gen_fold if len(x)>0]\n\n    # Extract data from generators\n    seq_array_fold = [[t[0] for t in sublist] for sublist in seq_gen_fold]\n    label_array_fold = [[t[1] for t in sublist] for sublist in seq_gen_fold]\n    video_array_fold = [[t[2] for t in sublist] for sublist in seq_gen_fold]\n    \n    # Transform lists in arrays\n    fold_sequences = np.concatenate(seq_array_fold).astype(np.float32)\n    fold_labels = np.concatenate(label_array_fold).astype(np.float32).reshape(-1)\n    fold_video_names = np.concatenate(video_array_fold)\n\n    # Transpose and expand\n    fold_sequences = np.transpose(fold_sequences, (0, 1, 3, 2))\n    fold_sequences = np.expand_dims(fold_sequences, axis=-1)\n    \n    print(fold_sequences.shape, fold_labels.shape)    \n    \n    cross_sequences_pad.append(fold_sequences)\n    cross_labels_pad.append(fold_labels)\n    cross_video_names_pad.append(fold_video_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model - Video","metadata":{}},{"cell_type":"code","source":"def create_model_Conv3D(input_shape, learning_rate=1e-4, p=0):\n    model = Sequential([\n        InputLayer(shape=input_shape),\n        \n        # Conv3D Block\n        Conv3D(filters=32, kernel_size=(3, 3, 2), activation='relu'),\n        BatchNormalization(),\n        MaxPooling3D(pool_size=(2, 2, 1)),  \n\n        # Flatten e Fully Connected\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')  \n    ])\n\n    optimizer = Adam(learning_rate=learning_rate)\n    \n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    if p:\n        print(model.summary())\n        \n    return model\n\n\ndef plot_history(history):\n    # Summarize history for Accuracy\n    fig_acc = plt.figure(figsize=(10,3))\n    plt.subplot(1,2,1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred):\n    # Confusion Matrix\n    fig, ax = plt.subplots(figsize=(5,5))\n    cm = confusion_matrix(y_true, y_pred)\n    cm_display = ConfusionMatrixDisplay(confusion_matrix = cm)\n    \n    cm_display.plot(ax=ax)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Holdout - Video","metadata":{}},{"cell_type":"code","source":"input_shape_holdout = seq_array_train.shape[1:]\nlearning_rate = 1e-4\nepochs = 20\nbatch_size = 256\n\ncallbacks = [ModelCheckpoint(model_path, monitor='val_accuracy', mode='max',\n                             save_best_only=True, save_weights_only=True)]\n\nmodel_holdout = create_model_Conv3D(input_shape_holdout, learning_rate)\n\n# Fit the network\nhistory_holdout = model_holdout.fit(seq_array_train, label_array_train, \n                              epochs=epochs, batch_size=batch_size, \n                              validation_data=(seq_array_val, label_array_val),\n                              callbacks=callbacks\n          )\n\nplot_history(history_holdout)\n\nmodel_holdout.load_weights(model_path)\n\n# Make predictions\npred = model_holdout.predict(seq_array_test)\ny_pred = (pred > 0.5).astype(int)\ny_true = label_array_test\n\n# Calculate metrics for frames\nframe_accuracy = accuracy_score(y_true, y_pred)\nframe_f1 = f1_score(y_true, y_pred, zero_division=0, average=None)\nframe_precision = precision_score(y_true, y_pred, zero_division=0, average=None)\nframe_recall = recall_score(y_true, y_pred, zero_division=0, average=None)\nframe_auc = roc_auc_score(y_true, pred)\n\nframe_metrics_holdout[\"Accuracy\"].append(frame_accuracy)\nframe_metrics_holdout[\"Precision (0)\"].append(frame_precision[0])\nframe_metrics_holdout[\"Precision (1)\"].append(frame_precision[1])\nframe_metrics_holdout[\"Recall (0)\"].append(frame_recall[0])\nframe_metrics_holdout[\"Recall (1)\"].append(frame_recall[1])\nframe_metrics_holdout[\"F1 (0)\"].append(frame_f1[0])\nframe_metrics_holdout[\"F1 (1)\"].append(frame_f1[1])\nframe_metrics_holdout[\"AUC\"].append(frame_auc)\n\nprint(\"***** FRAME LEVEL RESULTS *****\\n\")\nplot_confusion_matrix(y_true, y_pred)\n\n# Create a dataframe with predictions and video ids\npred_df = pd.DataFrame({'video_id': video_array_test, 'prediction': pred.flatten(), 'label': label_array_test})\n \n# Group by video and calculate the average\nvideo_predictions_mean = pred_df.groupby('video_id')['prediction'].apply(calculate_mean).values\nvideo_predictions_binary_mean = (video_predictions_mean > 0.5).astype(int)\n\n# Group by video and apply the majority voting rule\nvideo_predictions_binary_majority = pred_df.groupby('video_id')['prediction'].apply(calculate_majority).values\n\n# Group by video and apply the aggregation rule\nvideo_predictions_binary_threshold = pred_df.groupby('video_id')['prediction'].apply(aggregate_by_threshold).values    \n\n# Group by video and get labels\nvideo_labels = pred_df.groupby('video_id')['label'].first().values\n\n# Calculate metrics for video\nvideo_accuracy = accuracy_score(video_labels, video_predictions_binary_mean)\nvideo_f1 = f1_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\nvideo_precision = precision_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\nvideo_recall = recall_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\nvideo_auc = roc_auc_score(video_labels, video_predictions_mean)\n\nvideo_metrics_holdout_mean[\"Accuracy\"].append(video_accuracy)\nvideo_metrics_holdout_mean[\"Precision (0)\"].append(video_precision[0])\nvideo_metrics_holdout_mean[\"Precision (1)\"].append(video_precision[1])\nvideo_metrics_holdout_mean[\"Recall (0)\"].append(video_recall[0])\nvideo_metrics_holdout_mean[\"Recall (1)\"].append(video_recall[1])\nvideo_metrics_holdout_mean[\"F1 (0)\"].append(video_f1[0])\nvideo_metrics_holdout_mean[\"F1 (1)\"].append(video_f1[1])\nvideo_metrics_holdout_mean[\"AUC\"].append(video_auc)\n\nprint(\"\\n***** VIDEO LEVEL RESULTS (MEAN) *****\\n\")\nplot_confusion_matrix(video_labels, video_predictions_binary_mean)\n\n# Calculate metrics for video\nvideo_accuracy = accuracy_score(video_labels, video_predictions_binary_majority)\nvideo_f1 = f1_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\nvideo_precision = precision_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\nvideo_recall = recall_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\nvideo_auc = roc_auc_score(video_labels, video_predictions_binary_majority)\n\nvideo_metrics_holdout_majority[\"Accuracy\"].append(video_accuracy)\nvideo_metrics_holdout_majority[\"Precision (0)\"].append(video_precision[0])\nvideo_metrics_holdout_majority[\"Precision (1)\"].append(video_precision[1])\nvideo_metrics_holdout_majority[\"Recall (0)\"].append(video_recall[0])\nvideo_metrics_holdout_majority[\"Recall (1)\"].append(video_recall[1])\nvideo_metrics_holdout_majority[\"F1 (0)\"].append(video_f1[0])\nvideo_metrics_holdout_majority[\"F1 (1)\"].append(video_f1[1])\nvideo_metrics_holdout_majority[\"AUC\"].append(video_auc)\n\nprint(\"\\n***** VIDEO LEVEL RESULTS (MAJORITY) *****\\n\")\nplot_confusion_matrix(video_labels, video_predictions_binary_majority)\n\n# Calculate metrics for video\nvideo_accuracy = accuracy_score(video_labels, video_predictions_binary_threshold)\nvideo_f1 = f1_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\nvideo_precision = precision_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\nvideo_recall = recall_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\nvideo_auc = roc_auc_score(video_labels, video_predictions_binary_threshold)\n\nvideo_metrics_holdout_threshold[\"Accuracy\"].append(video_accuracy)\nvideo_metrics_holdout_threshold[\"Precision (0)\"].append(video_precision[0])\nvideo_metrics_holdout_threshold[\"Precision (1)\"].append(video_precision[1])\nvideo_metrics_holdout_threshold[\"Recall (0)\"].append(video_recall[0])\nvideo_metrics_holdout_threshold[\"Recall (1)\"].append(video_recall[1])\nvideo_metrics_holdout_threshold[\"F1 (0)\"].append(video_f1[0])\nvideo_metrics_holdout_threshold[\"F1 (1)\"].append(video_f1[1])\nvideo_metrics_holdout_threshold[\"AUC\"].append(video_auc)\n\nprint(\"\\n***** VIDEO LEVEL RESULTS (THRESHOLD) *****\\n\")\nplot_confusion_matrix(video_labels, video_predictions_binary_threshold)\n\n# Dataframe creation\ndf = pd.DataFrame.from_dict(frame_metrics_holdout, orient='index', columns=[\"SL 64 3D CONV \" + cols_groups_string])\nprint(\"***** FRAME LEVEL RESULTS *****\\n\")\nprint(df)\nframe_metrics_holdout = {key: [] for key in frame_metrics_holdout}\n\n# Dataframe creation\ndf = pd.DataFrame.from_dict(video_metrics_holdout_mean, orient='index', columns=[\"SL 64 3D CONV \" + cols_groups_string])\nprint(\"***** VIDEO LEVEL RESULTS (MEAN) *****\\n\")\nprint(df)\nvideo_metrics_holdout_mean = {key: [] for key in video_metrics_holdout_mean}\n\n# Dataframe creation\ndf = pd.DataFrame.from_dict(video_metrics_holdout_majority, orient='index', columns=[\"SL 64 3D CONV \" + cols_groups_string])\nprint(\"***** VIDEO LEVEL RESULTS (MAJORITY) *****\\n\")\nprint(df)\nvideo_metrics_holdout_majority = {key: [] for key in video_metrics_holdout_majority}\n\n# Dataframe creation\ndf = pd.DataFrame.from_dict(video_metrics_holdout_threshold, orient='index', columns=[\"SL 64 3D CONV \" + cols_groups_string])\nprint(\"***** VIDEO LEVEL RESULTS (THRESHOLD) *****\\n\")\nprint(df)\nvideo_metrics_holdout_threshold = {key: [] for key in video_metrics_holdout_threshold}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cross-Validation - Video","metadata":{}},{"cell_type":"code","source":"def cross_validation(frame_metrics_dict, video_metrics_dict_mean,\n                     video_metrics_dict_majority, video_metrics_dict_threshold,\n                     cross_sequences, cross_labels, cross_video_names,\n                     cross_sequences_pad, cross_labels_pad, cross_video_names_pad,\n                     num_fold=4, num_epoche=30, batch_size=200, learning_rate=1e-4, \n                     create_model=create_model_Conv3D):\n    # Performance List Videos\n    frame_accuracy_score = [] \n    frame_f1_score = [] \n    frame_precision_score = [] \n    frame_recall_score = []\n    frame_auc_score = []\n    \n    # Performance List Videos\n    video_accuracy_score_mean = [] \n    video_f1_score_mean = [] \n    video_precision_score_mean = [] \n    video_recall_score_mean = []\n    video_auc_score_mean = []\n\n    # Performance List Videos\n    video_accuracy_score_majority = [] \n    video_f1_score_majority = [] \n    video_precision_score_majority = [] \n    video_recall_score_majority = []\n    video_auc_score_majority = []\n\n    # Performance List Videos\n    video_accuracy_score_threshold = [] \n    video_f1_score_threshold = [] \n    video_precision_score_threshold = [] \n    video_recall_score_threshold = []\n    video_auc_score_threshold = []\n\n    # Iteration on the folds of cross-validation\n    for fold_idx in range(num_fold):\n        print(f\"Fold {fold_idx+1}/{num_fold}\")\n        train_sequences, train_labels, train_video_names = [],[],[]\n \n        # File path for the current fold\n        for i in range(num_fold):\n            if i != fold_idx:\n                train_sequences.append(cross_sequences[i])\n                train_labels.append(cross_labels[i])\n                train_video_names.extend(cross_video_names[i])\n        train_sequences = np.vstack(train_sequences)\n        train_labels = np.hstack(train_labels)\n        \n        val_sequences = cross_sequences_pad[fold_idx]\n        val_labels = cross_labels_pad[fold_idx]\n        val_video_names = cross_video_names_pad[fold_idx]\n\n        input_shape = train_sequences.shape[1:]\n\n        # Create model\n        model = create_model(input_shape, learning_rate=learning_rate)\n\n        # Train model\n        history = model.fit(train_sequences, train_labels, \n                            validation_data = (val_sequences, val_labels),\n                            epochs=num_epoche, batch_size=batch_size, verbose=0)\n\n        plot_history(history)\n\n        # Predictions\n        val_predictions = model.predict(val_sequences)\n        y_pred = (val_predictions > 0.5).astype(int)\n        y_true = val_labels\n\n        plot_confusion_matrix(y_true, y_pred)\n \n        # Create a dataframe with predictions and video ids\n        pred_df = pd.DataFrame({'video_id': val_video_names, 'prediction': val_predictions.flatten(), 'label': val_labels})\n    \n        # Group by video and calculate the average\n        video_predictions_mean = pred_df.groupby('video_id')['prediction'].apply(calculate_mean).values\n        video_predictions_binary_mean = (video_predictions_mean > 0.5).astype(int)\n        \n        # Group by video and apply the majority voting rule\n        video_predictions_binary_majority = pred_df.groupby('video_id')['prediction'].apply(calculate_majority).values\n    \n        # Group by video and apply the aggregation rule\n        video_predictions_binary_threshold = pred_df.groupby('video_id')['prediction'].apply(aggregate_by_threshold).values    \n        \n        # Group by video and get labels\n        video_labels = pred_df.groupby('video_id')['label'].first().values\n\n        # Calculate metrics for frames\n        frame_accuracy = accuracy_score(y_true, y_pred)\n        frame_f1 = f1_score(y_true, y_pred, zero_division=0, average=None)\n        frame_precision = precision_score(y_true, y_pred, zero_division=0, average=None)\n        frame_recall = recall_score(y_true, y_pred, zero_division=0, average=None)\n        frame_auc = roc_auc_score(y_true, val_predictions)\n        \n        frame_accuracy_score.append(frame_accuracy)\n        frame_f1_score.append(frame_f1)\n        frame_precision_score.append(frame_precision)\n        frame_recall_score.append(frame_recall)\n        frame_auc_score.append(frame_auc)\n\n        print(f\"Accuracy fold (frame-based): {frame_accuracy}\")\n\n        # Calculate accuracy for videos\n        video_accuracy = accuracy_score(video_labels, video_predictions_binary_mean)\n        video_f1 = f1_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n        video_precision = precision_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n        video_recall = recall_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n        video_auc = roc_auc_score(video_labels, video_predictions_mean)\n\n        video_accuracy_score_mean.append(video_accuracy)\n        video_f1_score_mean.append(video_f1)\n        video_precision_score_mean.append(video_precision)\n        video_recall_score_mean.append(video_recall)\n        video_auc_score_mean.append(video_auc)\n\n        print(f\"Accuracy fold (video-based - mean): {video_accuracy}\")\n\n        video_accuracy = accuracy_score(video_labels, video_predictions_binary_majority)\n        video_f1 = f1_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n        video_precision = precision_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n        video_recall = recall_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n        video_auc = roc_auc_score(video_labels, video_predictions_binary_majority)\n\n        video_accuracy_score_majority.append(video_accuracy)\n        video_f1_score_majority.append(video_f1)\n        video_precision_score_majority.append(video_precision)\n        video_recall_score_majority.append(video_recall)\n        video_auc_score_majority.append(video_auc)\n\n        print(f\"Accuracy fold (video-based - majority): {video_accuracy}\")\n\n        video_accuracy = accuracy_score(video_labels, video_predictions_binary_threshold)\n        video_f1 = f1_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n        video_precision = precision_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n        video_recall = recall_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n        video_auc = roc_auc_score(video_labels, video_predictions_binary_threshold)\n\n        video_accuracy_score_threshold.append(video_accuracy)\n        video_f1_score_threshold.append(video_f1)\n        video_precision_score_threshold.append(video_precision)\n        video_recall_score_threshold.append(video_recall)\n        video_auc_score_threshold.append(video_auc)\n\n        print(f\"Accuracy fold (video-based - threshold): {video_accuracy}\")\n\n    # Metrics average on all frames\n    frame_avg_accuracy = np.mean(frame_accuracy_score)\n    frame_avg_precision_0, frame_avg_recall_0, frame_avg_f1_0 = np.mean(frame_precision_score[0]), np.mean(frame_recall_score[0]), np.mean(frame_f1_score[0])\n    frame_avg_precision_1, frame_avg_recall_1, frame_avg_f1_1 = np.mean(frame_precision_score[1]), np.mean(frame_recall_score[1]), np.mean(frame_f1_score[1])\n    frame_avg_auc = np.mean(frame_auc_score)\n\n    frame_metrics_dict[\"Mean Accuracy\"].append(frame_avg_accuracy)\n    frame_metrics_dict[\"Mean Precision (0)\"].append(frame_avg_precision_0)\n    frame_metrics_dict[\"Mean Precision (1)\"].append(frame_avg_precision_1)\n    frame_metrics_dict[\"Mean Recall (0)\"].append(frame_avg_recall_0)\n    frame_metrics_dict[\"Mean Recall (1)\"].append(frame_avg_recall_1)\n    frame_metrics_dict[\"Mean F1 (0)\"].append(frame_avg_f1_0)\n    frame_metrics_dict[\"Mean F1 (1)\"].append(frame_avg_f1_1)\n    frame_metrics_dict[\"Mean AUC\"].append(frame_avg_auc)\n\n    # Metrics standard deviation on all frames\n    frame_std_accuracy = np.std(frame_accuracy_score)\n    frame_std_precision_0, frame_std_recall_0, frame_std_f1_0 = np.std(frame_precision_score[0]), np.std(frame_recall_score[0]), np.std(frame_f1_score[0])\n    frame_std_precision_1, frame_std_recall_1, frame_std_f1_1 = np.std(frame_precision_score[1]), np.std(frame_recall_score[1]), np.std(frame_f1_score[1])\n    frame_std_auc = np.std(frame_auc_score)\n\n    frame_metrics_dict[\"Std Accuracy\"].append(frame_std_accuracy)\n    frame_metrics_dict[\"Std Precision (0)\"].append(frame_std_precision_0)\n    frame_metrics_dict[\"Std Precision (1)\"].append(frame_std_precision_1)\n    frame_metrics_dict[\"Std Recall (0)\"].append(frame_std_recall_0)\n    frame_metrics_dict[\"Std Recall (1)\"].append(frame_std_recall_1)\n    frame_metrics_dict[\"Std F1 (0)\"].append(frame_std_f1_0)\n    frame_metrics_dict[\"Std F1 (1)\"].append(frame_std_f1_1)\n    frame_metrics_dict[\"Std AUC\"].append(frame_std_auc)\n        \n    print(\"\\n\\nRESULTS on FRAMES:\\n\")\n\n    print(\"Mean Accuracy:\", frame_avg_accuracy)\n    print(\"Accuracy std:\", frame_std_accuracy)\n\n    # Metrics average on all videos\n    video_avg_accuracy = np.mean(video_accuracy_score_mean)\n    video_avg_precision_0, video_avg_recall_0, video_avg_f1_0 = np.mean(video_precision_score_mean[0]), np.mean(video_recall_score_mean[0]), np.mean(video_f1_score_mean[0])\n    video_avg_precision_1, video_avg_recall_1, video_avg_f1_1 = np.mean(video_precision_score_mean[1]), np.mean(video_recall_score_mean[1]), np.mean(video_f1_score_mean[1])\n    video_avg_auc = np.mean(video_auc_score_mean)\n\n    video_metrics_dict_mean[\"Mean Accuracy\"].append(video_avg_accuracy)\n    video_metrics_dict_mean[\"Mean Precision (0)\"].append(video_avg_precision_0)\n    video_metrics_dict_mean[\"Mean Precision (1)\"].append(video_avg_precision_1)\n    video_metrics_dict_mean[\"Mean Recall (0)\"].append(video_avg_recall_0)\n    video_metrics_dict_mean[\"Mean Recall (1)\"].append(video_avg_recall_1)\n    video_metrics_dict_mean[\"Mean F1 (0)\"].append(video_avg_f1_0)\n    video_metrics_dict_mean[\"Mean F1 (1)\"].append(video_avg_f1_1)\n    video_metrics_dict_mean[\"Mean AUC\"].append(video_avg_auc)\n\n    # Metrics standard deviation on all videos\n    video_std_accuracy = np.std(video_accuracy_score_mean)\n    video_std_precision_0, video_std_recall_0, video_std_f1_0 = np.std(video_precision_score_mean[0]), np.std(video_recall_score_mean[0]), np.std(video_f1_score_mean[0])\n    video_std_precision_1, video_std_recall_1, video_std_f1_1 = np.std(video_precision_score_mean[1]), np.std(video_recall_score_mean[1]), np.std(video_f1_score_mean[1])\n    video_std_auc = np.std(video_auc_score_mean)\n\n    video_metrics_dict_mean[\"Std Accuracy\"].append(video_std_accuracy)\n    video_metrics_dict_mean[\"Std Precision (0)\"].append(video_std_precision_0)\n    video_metrics_dict_mean[\"Std Precision (1)\"].append(video_std_precision_1)\n    video_metrics_dict_mean[\"Std Recall (0)\"].append(video_std_recall_0)\n    video_metrics_dict_mean[\"Std Recall (1)\"].append(video_std_recall_1)\n    video_metrics_dict_mean[\"Std F1 (0)\"].append(video_std_f1_0)\n    video_metrics_dict_mean[\"Std F1 (1)\"].append(video_std_f1_1)\n    video_metrics_dict_mean[\"Std AUC\"].append(video_std_auc)\n    \n    print(\"\\n\\nRESULTS on VIDEOS (MEAN):\\n\")\n    \n    print(\"Mean Accuracy:\", video_avg_accuracy)\n    print(\"Accuracy std:\", video_std_accuracy)\n\n    # Metrics average on all videos\n    video_avg_accuracy = np.mean(video_accuracy_score_majority)\n    video_avg_precision_0, video_avg_recall_0, video_avg_f1_0 = np.mean(video_precision_score_majority[0]), np.mean(video_recall_score_majority[0]), np.mean(video_f1_score_majority[0])\n    video_avg_precision_1, video_avg_recall_1, video_avg_f1_1 = np.mean(video_precision_score_majority[1]), np.mean(video_recall_score_majority[1]), np.mean(video_f1_score_majority[1])\n    video_avg_auc = np.mean(video_auc_score_majority)\n\n    video_metrics_dict_majority[\"Mean Accuracy\"].append(video_avg_accuracy)\n    video_metrics_dict_majority[\"Mean Precision (0)\"].append(video_avg_precision_0)\n    video_metrics_dict_majority[\"Mean Precision (1)\"].append(video_avg_precision_1)\n    video_metrics_dict_majority[\"Mean Recall (0)\"].append(video_avg_recall_0)\n    video_metrics_dict_majority[\"Mean Recall (1)\"].append(video_avg_recall_1)\n    video_metrics_dict_majority[\"Mean F1 (0)\"].append(video_avg_f1_0)\n    video_metrics_dict_majority[\"Mean F1 (1)\"].append(video_avg_f1_1)\n    video_metrics_dict_majority[\"Mean AUC\"].append(video_avg_auc)\n\n    # Metrics standard deviation on all videos\n    video_std_accuracy = np.std(video_accuracy_score_majority)\n    video_std_precision_0, video_std_recall_0, video_std_f1_0 = np.std(video_precision_score_majority[0]), np.std(video_recall_score_majority[0]), np.std(video_f1_score_majority[0])\n    video_std_precision_1, video_std_recall_1, video_std_f1_1 = np.std(video_precision_score_majority[1]), np.std(video_recall_score_majority[1]), np.std(video_f1_score_majority[1])\n    video_std_auc = np.std(video_auc_score_majority)\n\n    video_metrics_dict_majority[\"Std Accuracy\"].append(video_std_accuracy)\n    video_metrics_dict_majority[\"Std Precision (0)\"].append(video_std_precision_0)\n    video_metrics_dict_majority[\"Std Precision (1)\"].append(video_std_precision_1)\n    video_metrics_dict_majority[\"Std Recall (0)\"].append(video_std_recall_0)\n    video_metrics_dict_majority[\"Std Recall (1)\"].append(video_std_recall_1)\n    video_metrics_dict_majority[\"Std F1 (0)\"].append(video_std_f1_0)\n    video_metrics_dict_majority[\"Std F1 (1)\"].append(video_std_f1_1)\n    video_metrics_dict_majority[\"Std AUC\"].append(video_std_auc)\n    \n    print(\"\\n\\nRESULTS on VIDEOS (MAJORITY):\\n\")\n    \n    print(\"Mean Accuracy:\", video_avg_accuracy)\n    print(\"Accuracy std:\", video_std_accuracy)\n\n    # Metrics average on all videos\n    video_avg_accuracy = np.mean(video_accuracy_score_threshold)\n    video_avg_precision_0, video_avg_recall_0, video_avg_f1_0 = np.mean(video_precision_score_threshold[0]), np.mean(video_recall_score_threshold[0]), np.mean(video_f1_score_threshold[0])\n    video_avg_precision_1, video_avg_recall_1, video_avg_f1_1 = np.mean(video_precision_score_threshold[1]), np.mean(video_recall_score_threshold[1]), np.mean(video_f1_score_threshold[1])\n    video_avg_auc = np.mean(video_auc_score_threshold)\n\n    video_metrics_dict_threshold[\"Mean Accuracy\"].append(video_avg_accuracy)\n    video_metrics_dict_threshold[\"Mean Precision (0)\"].append(video_avg_precision_0)\n    video_metrics_dict_threshold[\"Mean Precision (1)\"].append(video_avg_precision_1)\n    video_metrics_dict_threshold[\"Mean Recall (0)\"].append(video_avg_recall_0)\n    video_metrics_dict_threshold[\"Mean Recall (1)\"].append(video_avg_recall_1)\n    video_metrics_dict_threshold[\"Mean F1 (0)\"].append(video_avg_f1_0)\n    video_metrics_dict_threshold[\"Mean F1 (1)\"].append(video_avg_f1_1)\n    video_metrics_dict_threshold[\"Mean AUC\"].append(video_avg_auc)\n\n    # Metrics standard deviation on all videos\n    video_std_accuracy = np.std(video_accuracy_score_threshold)\n    video_std_precision_0, video_std_recall_0, video_std_f1_0 = np.std(video_precision_score_threshold[0]), np.std(video_recall_score_threshold[0]), np.std(video_f1_score_threshold[0])\n    video_std_precision_1, video_std_recall_1, video_std_f1_1 = np.std(video_precision_score_threshold[1]), np.std(video_recall_score_threshold[1]), np.std(video_f1_score_threshold[1])\n    video_std_auc = np.std(video_auc_score_threshold)\n\n    video_metrics_dict_threshold[\"Std Accuracy\"].append(video_std_accuracy)\n    video_metrics_dict_threshold[\"Std Precision (0)\"].append(video_std_precision_0)\n    video_metrics_dict_threshold[\"Std Precision (1)\"].append(video_std_precision_1)\n    video_metrics_dict_threshold[\"Std Recall (0)\"].append(video_std_recall_0)\n    video_metrics_dict_threshold[\"Std Recall (1)\"].append(video_std_recall_1)\n    video_metrics_dict_threshold[\"Std F1 (0)\"].append(video_std_f1_0)\n    video_metrics_dict_threshold[\"Std F1 (1)\"].append(video_std_f1_1)\n    video_metrics_dict_threshold[\"Std AUC\"].append(video_std_auc)\n    \n    print(\"\\n\\nRESULTS on VIDEOS (THRESHOLD):\\n\")\n    \n    print(\"Mean Accuracy:\", video_avg_accuracy)\n    print(\"Accuracy std:\", video_std_accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cross_validation(frame_metrics_cross, video_metrics_cross_mean,\n                 video_metrics_cross_majority, video_metrics_cross_threshold,\n                 cross_sequences, cross_labels, cross_video_names,\n                 cross_sequences_pad, cross_labels_pad, cross_video_names_pad,\n                 num_fold=4, num_epoche=epochs, batch_size=batch_size, learning_rate=learning_rate, \n                 create_model=create_model_Conv3D)\n\n# Dataframe creation\ndf = pd.DataFrame.from_dict(frame_metrics_cross, orient='index', columns=[\"SL 64 3D CONV \" + cols_groups_string])\nprint(\"***** FRAME LEVEL RESULTS *****\\n\")\nprint(df)\nframe_metrics_cross = {key: [] for key in frame_metrics_cross}\n\n# Dataframe creation\ndf = pd.DataFrame.from_dict(video_metrics_cross_mean, orient='index', columns=[\"SL 64 3D CONV \" + cols_groups_string])\nprint(\"***** VIDEO LEVEL RESULTS (MEAN) *****\\n\")\nprint(df)\nvideo_metrics_cross_mean = {key: [] for key in video_metrics_cross_mean}\n\n# Dataframe creation\ndf = pd.DataFrame.from_dict(video_metrics_cross_majority, orient='index', columns=[\"SL 64 3D CONV \" + cols_groups_string])\nprint(\"***** VIDEO LEVEL RESULTS (MAJORITY) *****\\n\")\nprint(df)\nvideo_metrics_cross_majority = {key: [] for key in video_metrics_cross_majority}\n\n# Dataframe creation\ndf = pd.DataFrame.from_dict(video_metrics_cross_threshold, orient='index', columns=[\"SL 64 3D CONV \" + cols_groups_string])\nprint(\"***** VIDEO LEVEL RESULTS (THRESHOLD) *****\\n\")\nprint(df)\nvideo_metrics_cross_threshold = {key: [] for key in video_metrics_cross_threshold}","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}