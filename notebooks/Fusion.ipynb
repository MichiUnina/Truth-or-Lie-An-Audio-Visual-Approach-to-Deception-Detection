{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:19:18.016772Z",
     "iopub.status.busy": "2025-04-08T18:19:18.016427Z",
     "iopub.status.idle": "2025-04-08T18:19:33.358556Z",
     "shell.execute_reply": "2025-04-08T18:19:33.357839Z",
     "shell.execute_reply.started": "2025-04-08T18:19:18.016738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import librosa\n",
    "import pywt\n",
    "import scipy\n",
    "from scipy.signal.windows import hamming \n",
    " \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import InputLayer, LSTM, Conv3D, MaxPooling3D, Flatten, Dense, Masking, Dropout, Input, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_fscore_support, roc_auc_score, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:19:33.360399Z",
     "iopub.status.busy": "2025-04-08T18:19:33.359914Z",
     "iopub.status.idle": "2025-04-08T18:19:33.364856Z",
     "shell.execute_reply": "2025-04-08T18:19:33.363854Z",
     "shell.execute_reply.started": "2025-04-08T18:19:33.360370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define path to save model\n",
    "model_path = 'binary_model.weights.h5'\n",
    "\n",
    "input_video_path = \"/kaggle/input/truth-lie-features\"\n",
    "\n",
    "train_video_path = input_video_path + \"/train_features.csv\"\n",
    "val_video_path = input_video_path + \"/val_features.csv\"\n",
    "test_video_path = input_video_path + \"/test_features.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:19:33.366199Z",
     "iopub.status.busy": "2025-04-08T18:19:33.365869Z",
     "iopub.status.idle": "2025-04-08T18:19:33.393532Z",
     "shell.execute_reply": "2025-04-08T18:19:33.392655Z",
     "shell.execute_reply.started": "2025-04-08T18:19:33.366165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "holdout_audio_path = '/kaggle/input/truthlie-clean-split/TruthLie_Holdout_Stratified'\n",
    "train_audio_path = holdout_audio_path + '/train'\n",
    "val_audio_path = holdout_audio_path + '/val'\n",
    "test_audio_path = holdout_audio_path + '/test'\n",
    "\n",
    "cross_audio_path = '/kaggle/input/truthlie-clean-crossvalidation/TruthLie_CrossVal_Stratified'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:19:33.396053Z",
     "iopub.status.busy": "2025-04-08T18:19:33.395764Z",
     "iopub.status.idle": "2025-04-08T18:19:33.408882Z",
     "shell.execute_reply": "2025-04-08T18:19:33.407977Z",
     "shell.execute_reply.started": "2025-04-08T18:19:33.396027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dictionaries for Holdout metrics\n",
    "metrics_holdout = {\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision (0)\": [], \"Precision (1)\": [],\n",
    "    \"Recall (0)\": [], \"Recall (1)\": [],\n",
    "    \"F1 (0)\": [], \"F1 (1)\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "frame_metrics_holdout = {\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision (0)\": [], \"Precision (1)\": [],\n",
    "    \"Recall (0)\": [], \"Recall (1)\": [],\n",
    "    \"F1 (0)\": [], \"F1 (1)\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "video_metrics_holdout_mean = {\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision (0)\": [], \"Precision (1)\": [],\n",
    "    \"Recall (0)\": [], \"Recall (1)\": [],\n",
    "    \"F1 (0)\": [], \"F1 (1)\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "video_metrics_holdout_majority = {\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision (0)\": [], \"Precision (1)\": [],\n",
    "    \"Recall (0)\": [], \"Recall (1)\": [],\n",
    "    \"F1 (0)\": [], \"F1 (1)\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "video_metrics_holdout_threshold = {\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision (0)\": [], \"Precision (1)\": [],\n",
    "    \"Recall (0)\": [], \"Recall (1)\": [],\n",
    "    \"F1 (0)\": [], \"F1 (1)\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Dictionaries for Cross-Validation metrics\n",
    "metrics_cross = {\n",
    "    \"Mean Accuracy\": [], \"Std Accuracy\": [],\n",
    "    \"Mean Precision (0)\": [], \"Std Precision (0)\": [],\n",
    "    \"Mean Precision (1)\": [], \"Std Precision (1)\": [],\n",
    "    \"Mean Recall (0)\": [], \"Std Recall (0)\": [],\n",
    "    \"Mean Recall (1)\": [], \"Std Recall (1)\": [],\n",
    "    \"Mean F1 (0)\": [], \"Std F1 (0)\": [],\n",
    "    \"Mean F1 (1)\": [], \"Std F1 (1)\": [],\n",
    "    \"Mean AUC\": [], \"Std AUC\": []\n",
    "}\n",
    "\n",
    "frame_metrics_cross = {\n",
    "    \"Mean Accuracy\": [], \"Std Accuracy\": [],\n",
    "    \"Mean Precision (0)\": [], \"Std Precision (0)\": [],\n",
    "    \"Mean Precision (1)\": [], \"Std Precision (1)\": [],\n",
    "    \"Mean Recall (0)\": [], \"Std Recall (0)\": [],\n",
    "    \"Mean Recall (1)\": [], \"Std Recall (1)\": [],\n",
    "    \"Mean F1 (0)\": [], \"Std F1 (0)\": [],\n",
    "    \"Mean F1 (1)\": [], \"Std F1 (1)\": [],\n",
    "    \"Mean AUC\": [], \"Std AUC\": []\n",
    "}\n",
    "video_metrics_cross_mean = {\n",
    "    \"Mean Accuracy\": [], \"Std Accuracy\": [],\n",
    "    \"Mean Precision (0)\": [], \"Std Precision (0)\": [],\n",
    "    \"Mean Precision (1)\": [], \"Std Precision (1)\": [],\n",
    "    \"Mean Recall (0)\": [], \"Std Recall (0)\": [],\n",
    "    \"Mean Recall (1)\": [], \"Std Recall (1)\": [],\n",
    "    \"Mean F1 (0)\": [], \"Std F1 (0)\": [],\n",
    "    \"Mean F1 (1)\": [], \"Std F1 (1)\": [],\n",
    "    \"Mean AUC\": [], \"Std AUC\": []\n",
    "}\n",
    "video_metrics_cross_majority = {\n",
    "    \"Mean Accuracy\": [], \"Std Accuracy\": [],\n",
    "    \"Mean Precision (0)\": [], \"Std Precision (0)\": [],\n",
    "    \"Mean Precision (1)\": [], \"Std Precision (1)\": [],\n",
    "    \"Mean Recall (0)\": [], \"Std Recall (0)\": [],\n",
    "    \"Mean Recall (1)\": [], \"Std Recall (1)\": [],\n",
    "    \"Mean F1 (0)\": [], \"Std F1 (0)\": [],\n",
    "    \"Mean F1 (1)\": [], \"Std F1 (1)\": [],\n",
    "    \"Mean AUC\": [], \"Std AUC\": []\n",
    "}\n",
    "video_metrics_cross_threshold = {\n",
    "    \"Mean Accuracy\": [], \"Std Accuracy\": [],\n",
    "    \"Mean Precision (0)\": [], \"Std Precision (0)\": [],\n",
    "    \"Mean Precision (1)\": [], \"Std Precision (1)\": [],\n",
    "    \"Mean Recall (0)\": [], \"Std Recall (0)\": [],\n",
    "    \"Mean Recall (1)\": [], \"Std Recall (1)\": [],\n",
    "    \"Mean F1 (0)\": [], \"Std F1 (0)\": [],\n",
    "    \"Mean F1 (1)\": [], \"Std F1 (1)\": [],\n",
    "    \"Mean AUC\": [], \"Std AUC\": []\n",
    "}\n",
    "\n",
    "# Function to aggregate frames considering the average\n",
    "def calculate_mean(preds):\n",
    "    return np.mean(preds)\n",
    "\n",
    "# Function to aggregate frames considering the majority\n",
    "def calculate_majority(preds):\n",
    "    return int(np.sum(preds > 0.5) > len(preds) / 2)\n",
    "\n",
    "# Function to aggregate frames considering a threshold\n",
    "def aggregate_by_threshold(preds):\n",
    "    threshold = 0.25 * len(preds)\n",
    "    count_zeros = (preds < 0.5).sum()\n",
    "    return 0 if count_zeros >= threshold else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T18:16:39.408737Z",
     "iopub.status.busy": "2024-11-25T18:16:39.408445Z",
     "iopub.status.idle": "2024-11-25T18:16:39.436252Z",
     "shell.execute_reply": "2024-11-25T18:16:39.435453Z",
     "shell.execute_reply.started": "2024-11-25T18:16:39.40871Z"
    }
   },
   "source": [
    "## VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:19:33.410424Z",
     "iopub.status.busy": "2025-04-08T18:19:33.410192Z",
     "iopub.status.idle": "2025-04-08T18:19:33.425424Z",
     "shell.execute_reply": "2025-04-08T18:19:33.424658Z",
     "shell.execute_reply.started": "2025-04-08T18:19:33.410401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def upload_dataset(features_path):\n",
    "    df_feature = pd.read_csv(features_path)\n",
    "    \n",
    "    # Remove rows that contain NaN\n",
    "    dataset = df_feature.dropna()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def mirror_padding(frames, sequence_length):\n",
    "    \"\"\"\n",
    "    Applies mirror padding to a sequence of frames if it's shorter than the required sequence length.\n",
    "    \n",
    "    :param frames: Numpy array of frames (num_frames, feature_dim)\n",
    "    :param sequence_length: Desired length of the output sequence\n",
    "    :return: Numpy array with mirror padding applied\n",
    "    \"\"\"    \n",
    "    # Compute the number of padding frames needed\n",
    "    padding_needed = sequence_length - len(frames)\n",
    "    \n",
    "    # Compute how to split padding between the beginning and the end\n",
    "    pad_start = padding_needed // 2\n",
    "    pad_end = padding_needed - pad_start\n",
    "    \n",
    "    # Apply mirror (reflect) padding along the time axis\n",
    "    padded_frames = np.pad(\n",
    "        frames,\n",
    "        pad_width=((pad_start, pad_end), (0, 0)),  # Padding along the first dimension (time)\n",
    "        mode='reflect'\n",
    "    )\n",
    "    \n",
    "    return padded_frames\n",
    "    \n",
    "\n",
    "def gen_sequence(id_df, seq_length, cols_groups, frame_step=1, padding=None):\n",
    "    \"\"\"\n",
    "    Generates sequences of fixed length from a dataframe representing a single sample.\n",
    "\n",
    "    :param id_df: DataFrame containing the data of a single video sample\n",
    "    :param seq_length: Length of the output sequences\n",
    "    :param cols_groups: List of lists, each sublist containing column names that belong to a group (e.g., audio, facial)\n",
    "    :param frame_step: Step between frames when sliding the window\n",
    "    :param padding: Padding method to apply ('const' for zero-padding, 'mirror' for reflective padding)\n",
    "    :yield: Tuple of (stacked sequence, label, video_name)\n",
    "    \"\"\"\n",
    "    data_matrices = []\n",
    "    \n",
    "    # Extract and preprocess each group of columns\n",
    "    for group in cols_groups:\n",
    "        group_df = id_df[group]\n",
    "        data_matrices.append(group_df.values)\n",
    "\n",
    "    # Apply padding to each group if needed\n",
    "    if padding == 'const':\n",
    "        padded_matrices = []\n",
    "        for data_matrix in data_matrices:\n",
    "            if data_matrix.shape[0] < seq_length:\n",
    "                padding_needed = seq_length - data_matrix.shape[0]\n",
    "                pre_padding = padding_needed // 3\n",
    "                post_padding = padding_needed - pre_padding\n",
    "                pad = np.full((pre_padding, data_matrix.shape[1]), 0)\n",
    "                data_matrix = np.vstack([pad, data_matrix, np.full((post_padding, data_matrix.shape[1]), 0)])\n",
    "            padded_matrices.append(data_matrix)\n",
    "        data_matrices = padded_matrices\n",
    "    elif padding == 'mirror':\n",
    "        data_matrices = [mirror_padding(data_matrix, seq_length) if data_matrix.shape[0] < seq_length else data_matrix\n",
    "                         for data_matrix in data_matrices]\n",
    "\n",
    "    label = id_df['label'].values[0]\n",
    "    video_name = id_df['input'].values[0]\n",
    "\n",
    "    num_elements = data_matrices[0].shape[0]\n",
    "    for start, stop in zip(range(0, num_elements - seq_length + 1, frame_step), range(seq_length, num_elements + 1, frame_step)):\n",
    "        # Stack the sequence from all groups along a new axis (group-wise)\n",
    "        stacked_sequence = np.stack([matrix[start:stop, :] for matrix in data_matrices], axis=1)\n",
    "        yield stacked_sequence, label, video_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:19:33.427098Z",
     "iopub.status.busy": "2025-04-08T18:19:33.426735Z",
     "iopub.status.idle": "2025-04-08T18:19:40.332586Z",
     "shell.execute_reply": "2025-04-08T18:19:40.331784Z",
     "shell.execute_reply.started": "2025-04-08T18:19:33.427062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = upload_dataset(train_video_path)\n",
    "val_dataset = upload_dataset(val_video_path)\n",
    "test_dataset = upload_dataset(test_video_path)\n",
    "\n",
    "aus_cols_aux = list(train_dataset.columns[144:164])\n",
    "aus_cols_emotions = list(train_dataset.columns[164:171])\n",
    "\n",
    "train_list = train_dataset['input'].unique().tolist()\n",
    "val_list = val_dataset['input'].unique().tolist()\n",
    "test_list = test_dataset['input'].unique().tolist()\n",
    "\n",
    "random.shuffle(train_list)\n",
    "random.shuffle(val_list)\n",
    "random.shuffle(test_list)\n",
    "\n",
    "sequence_length = 64\n",
    "frame_step = 1\n",
    "cols_groups = [  \n",
    "    aus_cols_aux,\n",
    "    aus_cols_emotions\n",
    "]  \n",
    "padding = 'const'\n",
    "\n",
    "# Make sure all groups are the same size, otherwise select the most correlated columns\n",
    "min_features = min(len(group) for group in cols_groups)\n",
    "cols_groups = [\n",
    "    train_dataset[group + ['label']].corr()['label'].drop('label')\n",
    "    .abs().sort_values(ascending=False).head(min_features).index.tolist()\n",
    "    for group in cols_groups\n",
    "]\n",
    "\n",
    "# Generate sequences for train, val and test\n",
    "seq_gen_train = list(list(gen_sequence(train_dataset[train_dataset['input'] == id], \n",
    "                                       sequence_length, cols_groups, frame_step, \n",
    "                                       padding=None))\n",
    "                     for id in train_list)\n",
    "\n",
    "seq_gen_val = list(list(gen_sequence(val_dataset[val_dataset['input'] == id], \n",
    "                                     sequence_length, cols_groups, frame_step, \n",
    "                                     padding))\n",
    "                   for id in val_list)\n",
    "\n",
    "seq_gen_test = list(list(gen_sequence(test_dataset[test_dataset['input'] == id], \n",
    "                                      sequence_length, cols_groups, frame_step, \n",
    "                                      padding))\n",
    "                    for id in test_list)\n",
    "\n",
    "# Remove all empty lists\n",
    "seq_gen_train = [x for x in seq_gen_train if len(x) > 0]\n",
    "seq_gen_val = [x for x in seq_gen_val if len(x) > 0]\n",
    "seq_gen_test = [x for x in seq_gen_test if len(x) > 0]\n",
    "\n",
    "# Extract data from generators\n",
    "seq_array_train = [[t[0] for t in sublist] for sublist in seq_gen_train]\n",
    "label_array_train = [[t[1] for t in sublist] for sublist in seq_gen_train]\n",
    "video_array_train = [[t[2] for t in sublist] for sublist in seq_gen_train]\n",
    "\n",
    "seq_array_val = [[t[0] for t in sublist] for sublist in seq_gen_val]\n",
    "label_array_val = [[t[1] for t in sublist] for sublist in seq_gen_val]\n",
    "video_array_val = [[t[2] for t in sublist] for sublist in seq_gen_val]\n",
    "\n",
    "seq_array_test = [[t[0] for t in sublist] for sublist in seq_gen_test]\n",
    "label_array_test = [[t[1] for t in sublist] for sublist in seq_gen_test]\n",
    "video_array_test = [[t[2] for t in sublist] for sublist in seq_gen_test]\n",
    "\n",
    "# Transform lists in arrays\n",
    "seq_array_train = np.concatenate(seq_array_train).astype(np.float32)\n",
    "train_labels_video_holdout = np.concatenate(label_array_train).astype(np.float32).reshape(-1)\n",
    "train_video_names_holdout = np.concatenate(video_array_train)\n",
    "\n",
    "seq_array_val = np.concatenate(seq_array_val).astype(np.float32)\n",
    "val_labels_video_holdout = np.concatenate(label_array_val).astype(np.float32).reshape(-1)\n",
    "val_video_names_holdout = np.concatenate(video_array_val)\n",
    "\n",
    "seq_array_test = np.concatenate(seq_array_test).astype(np.float32)\n",
    "test_labels_video_holdout = np.concatenate(label_array_test).astype(np.float32).reshape(-1)\n",
    "test_video_names_holdout = np.concatenate(video_array_test)\n",
    "\n",
    "# Transpose and expand arrays\n",
    "train_features_video_holdout = np.transpose(seq_array_train, (0, 1, 3, 2))\n",
    "train_features_video_holdout = np.expand_dims(train_features_video_holdout, axis=-1)\n",
    "print(train_features_video_holdout.shape, train_labels_video_holdout.shape)\n",
    "\n",
    "val_features_video_holdout = np.transpose(seq_array_val, (0, 1, 3, 2))\n",
    "val_features_video_holdout = np.expand_dims(val_features_video_holdout, axis=-1)\n",
    "print(val_features_video_holdout.shape, val_labels_video_holdout.shape)\n",
    "\n",
    "test_features_video_holdout = np.transpose(seq_array_test, (0, 1, 3, 2))\n",
    "test_features_video_holdout = np.expand_dims(test_features_video_holdout, axis=-1)\n",
    "print(test_features_video_holdout.shape, test_labels_video_holdout.shape)\n",
    "\n",
    "new_train_list, indices = np.unique(train_video_names_holdout, return_index=True)\n",
    "new_train_list = new_train_list[np.argsort(indices)].tolist()\n",
    "new_val_list, indices = np.unique(val_video_names_holdout, return_index=True)\n",
    "new_val_list = new_val_list[np.argsort(indices)].tolist()\n",
    "new_test_list, indices = np.unique(test_video_names_holdout, return_index=True)\n",
    "new_test_list = new_test_list[np.argsort(indices)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:19:40.334657Z",
     "iopub.status.busy": "2025-04-08T18:19:40.334134Z",
     "iopub.status.idle": "2025-04-08T18:19:47.526602Z",
     "shell.execute_reply": "2025-04-08T18:19:47.525746Z",
     "shell.execute_reply.started": "2025-04-08T18:19:40.334617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cross_video_datasets = []\n",
    "new_cross_list, new_cross_list_pad = [],[]\n",
    "for i in range(4):\n",
    "    fold_features_path = input_video_path + f\"/fold_{i}_features.csv\"\n",
    "    cross_video_datasets.append(upload_dataset(fold_features_path))\n",
    "\n",
    "cross_video_lists = [fold_dataset['input'].unique().tolist() for fold_dataset in cross_video_datasets]\n",
    "for i in range(4):\n",
    "    random.shuffle(cross_video_lists[i])\n",
    "\n",
    "data_video_cross, labels_video_cross, names_video_cross = [],[],[]\n",
    "data_video_cross_pad, labels_video_cross_pad, names_video_cross_pad = [],[],[]\n",
    "\n",
    "for i in range(4):\n",
    "    fold_dataset = cross_video_datasets[i]\n",
    "    fold_list = cross_video_lists[i]\n",
    "    \n",
    "    seq_gen_fold = list(list(gen_sequence(fold_dataset[fold_dataset['input'] == id], \n",
    "                                          sequence_length, cols_groups, frame_step, \n",
    "                                          padding=None))\n",
    "                 for id in fold_list)\n",
    "    seq_gen_fold = [x for x in seq_gen_fold if len(x)>0]\n",
    "\n",
    "    # Extract data from generators\n",
    "    seq_array_fold = [[t[0] for t in sublist] for sublist in seq_gen_fold]\n",
    "    label_array_fold = [[t[1] for t in sublist] for sublist in seq_gen_fold]\n",
    "    video_array_fold = [[t[2] for t in sublist] for sublist in seq_gen_fold]\n",
    "    \n",
    "    # Transform lists in arrays\n",
    "    fold_sequences = np.concatenate(seq_array_fold).astype(np.float32)\n",
    "    fold_labels = np.concatenate(label_array_fold).astype(np.float32).reshape(-1)\n",
    "    fold_video_names = np.concatenate(video_array_fold)\n",
    "\n",
    "    # Transpose and expand\n",
    "    fold_sequences = np.transpose(fold_sequences, (0, 1, 3, 2))\n",
    "    fold_sequences = np.expand_dims(fold_sequences, axis=-1)\n",
    "    \n",
    "    print(fold_sequences.shape, fold_labels.shape)    \n",
    "    \n",
    "    data_video_cross.append(fold_sequences)\n",
    "    labels_video_cross.append(fold_labels)\n",
    "    names_video_cross.append(fold_video_names)\n",
    "\n",
    "    fold_video_names, indices = np.unique(fold_video_names, return_index=True)\n",
    "    fold_video_names = fold_video_names[np.argsort(indices)].tolist()\n",
    "    new_cross_list.append(fold_video_names)\n",
    "    print(len(new_cross_list[i]))\n",
    "\n",
    "    # Padding\n",
    "    seq_gen_fold = list(list(gen_sequence(fold_dataset[fold_dataset['input'] == id], \n",
    "                                          sequence_length, cols_groups, frame_step, \n",
    "                                          padding))\n",
    "                 for id in fold_list)\n",
    "    seq_gen_fold = [x for x in seq_gen_fold if len(x)>0]\n",
    "\n",
    "    # Extract data from generators\n",
    "    seq_array_fold = [[t[0] for t in sublist] for sublist in seq_gen_fold]\n",
    "    label_array_fold = [[t[1] for t in sublist] for sublist in seq_gen_fold]\n",
    "    video_array_fold = [[t[2] for t in sublist] for sublist in seq_gen_fold]\n",
    "    \n",
    "    # Transform lists in arrays\n",
    "    fold_sequences = np.concatenate(seq_array_fold).astype(np.float32)\n",
    "    fold_labels = np.concatenate(label_array_fold).astype(np.float32).reshape(-1)\n",
    "    fold_video_names = np.concatenate(video_array_fold)\n",
    "\n",
    "    # Transpose and expand\n",
    "    fold_sequences = np.transpose(fold_sequences, (0, 1, 3, 2))\n",
    "    fold_sequences = np.expand_dims(fold_sequences, axis=-1)\n",
    "    \n",
    "    print(fold_sequences.shape, fold_labels.shape)    \n",
    "    \n",
    "    data_video_cross_pad.append(fold_sequences)\n",
    "    labels_video_cross_pad.append(fold_labels)\n",
    "    names_video_cross_pad.append(fold_video_names)\n",
    "\n",
    "    fold_video_names, indices = np.unique(fold_video_names, return_index=True)\n",
    "    fold_video_names = fold_video_names[np.argsort(indices)].tolist()\n",
    "    new_cross_list_pad.append(fold_video_names)\n",
    "    print(len(new_cross_list_pad[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T18:16:39.408737Z",
     "iopub.status.busy": "2024-11-25T18:16:39.408445Z",
     "iopub.status.idle": "2024-11-25T18:16:39.436252Z",
     "shell.execute_reply": "2024-11-25T18:16:39.435453Z",
     "shell.execute_reply.started": "2024-11-25T18:16:39.40871Z"
    }
   },
   "source": [
    "## AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:19:47.529075Z",
     "iopub.status.busy": "2025-04-08T18:19:47.528811Z",
     "iopub.status.idle": "2025-04-08T18:19:47.758793Z",
     "shell.execute_reply": "2025-04-08T18:19:47.757928Z",
     "shell.execute_reply.started": "2025-04-08T18:19:47.529049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_audio_data(audio_dataset_path, videos_list):\n",
    "    \"\"\"\n",
    "    Loads raw audio signals and their corresponding labels from a dataset directory.\n",
    "\n",
    "    :param audio_dataset_path: Path to the directory containing audio data and metadata (.xlsx)\n",
    "    :param videos_list: List of video file paths to match with audio entries\n",
    "    :return: Tuple (list of (signal, sample_rate), numpy array of labels)\n",
    "    \"\"\"\n",
    "    audios = []\n",
    "    audio_labels = []\n",
    "    audio_dict = {}\n",
    "\n",
    "    split = audio_dataset_path.split('/')[-1]\n",
    "\n",
    "    # Extract audio data\n",
    "    for root, dirs, files in os.walk(audio_dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.xlsx'):\n",
    "                # Path to the Excel metadata file\n",
    "                excel_path = os.path.join(root, file)\n",
    "                df = pd.read_excel(excel_path)\n",
    "                \n",
    "                # Extract audio names, labels, and corresponding video names\n",
    "                label_list = df['label'].tolist()\n",
    "                audio_names = df['audio name'].tolist()\n",
    "                video_names = df['video name'].tolist()\n",
    "                \n",
    "                # Load the corresponding audio files\n",
    "                for audio_name, video_name, label in zip(audio_names, video_names, label_list):\n",
    "                    audio_path = os.path.join(audio_dataset_path, 'Audio', audio_name)\n",
    "                    \n",
    "                    if os.path.exists(audio_path):\n",
    "                        # Load the raw audio without preprocessing\n",
    "                        signal, sample_rate = librosa.load(audio_path, sr=None)\n",
    "                        audio_dict[video_name] = ((signal, sample_rate), label)\n",
    "                    else:\n",
    "                        print(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "                # Match audio to each video in the list\n",
    "                for video in videos_list:\n",
    "                    video_name = video.split('/')[-1]\n",
    "                    audios.append(audio_dict[video_name][0])\n",
    "                    audio_labels.append(audio_dict[video_name][1])\n",
    "    \n",
    "    return audios, np.array(audio_labels)\n",
    "\n",
    "\n",
    "def calculate_min_length(wav2vec_model):\n",
    "    \"\"\"\n",
    "    Calculates the minimum input length required for a Wav2Vec2 model based on its convolutional layers.\n",
    "\n",
    "    :param wav2vec_model: Pretrained Wav2Vec2 model\n",
    "    :return: Minimum number of audio samples required\n",
    "    \"\"\"\n",
    "    kernel_sizes = wav2vec_model.config.conv_kernel\n",
    "    strides = wav2vec_model.config.conv_stride\n",
    "    min_length = 1\n",
    "    for k, s in zip(kernel_sizes, strides):\n",
    "        min_length = (min_length - 1) * s + k\n",
    "    return min_length\n",
    "\n",
    "\n",
    "def pad_audio(signal, target_length):\n",
    "    \"\"\"\n",
    "    Pads an audio signal with zeros to reach a target length.\n",
    "\n",
    "    :param signal: Input 1D audio signal\n",
    "    :param target_length: Desired length\n",
    "    :return: Zero-padded signal of target_length\n",
    "    \"\"\"\n",
    "    return np.pad(signal, (0, max(0, target_length - len(signal))), mode=\"constant\")\n",
    "\n",
    "\n",
    "def preprocess_audio_sequential_fixed_segments(signal, sample_rate, num_segments=20, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Preprocesses an audio signal into fixed-length segments and extracts DWT and MFCC features.\n",
    "\n",
    "    :param signal: Raw audio signal\n",
    "    :param sample_rate: Sampling rate of the signal\n",
    "    :param num_segments: Number of fixed-length segments to divide the signal into\n",
    "    :param n_mfcc: Number of MFCC coefficients to extract\n",
    "    :return: Numpy array of extracted features for each segment\n",
    "    \"\"\"\n",
    "    # Noise filtering using STFT thresholding\n",
    "    stft = np.abs(librosa.stft(signal))\n",
    "    threshold = np.median(stft)\n",
    "    stft_filtered = np.where(stft > threshold, stft, 0)\n",
    "    signal_filtered = librosa.istft(stft_filtered)\n",
    "\n",
    "    # Normalize signal between -1 and 1\n",
    "    normalized_signal = signal_filtered / np.max(np.abs(signal_filtered))\n",
    "\n",
    "    window_length = len(normalized_signal) // num_segments\n",
    "\n",
    "    # Segment the signal into equal-length windows\n",
    "    segmented_signal = []\n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * window_length\n",
    "        end_idx = start_idx + window_length\n",
    "        segment = normalized_signal[start_idx:end_idx]\n",
    "        segment = pad_audio(segment, window_length)\n",
    "        segmented_signal.append(segment)\n",
    "    segmented_signal = np.array(segmented_signal)\n",
    "\n",
    "    # Apply Hamming window\n",
    "    hamming_window = hamming(segmented_signal.shape[1])\n",
    "    windowed_signal = segmented_signal * hamming_window\n",
    "\n",
    "    # Extract features from each segment\n",
    "    sequence_features = []\n",
    "    for segment in windowed_signal:\n",
    "        segment_features = []\n",
    "\n",
    "        # DWT feature extraction\n",
    "        coeffs = pywt.wavedec(segment, 'db4', level=5)\n",
    "        for coeff in coeffs:\n",
    "            variance = np.var(coeff)\n",
    "            energy = np.sum(np.square(coeff))\n",
    "            entropy = -np.sum(coeff * np.log2(np.abs(coeff) + 1e-12))\n",
    "            kurtosis = 0 if variance == 0 else np.mean((coeff - np.mean(coeff))**4) / (variance**2)\n",
    "            skewness = 0 if variance == 0 else np.mean((coeff - np.mean(coeff))**3) / (variance**1.5)\n",
    "            std_dev = np.std(coeff)\n",
    "            segment_features.extend([energy, entropy, kurtosis, skewness, std_dev])\n",
    "\n",
    "        # MFCC feature extraction\n",
    "        n_fft = min(2048, max(256, 2 ** int(np.floor(np.log2(len(segment))))))\n",
    "        mfccs = librosa.feature.mfcc(y=segment, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft)\n",
    "        mfcc_features = np.mean(mfccs, axis=1)\n",
    "        segment_features.extend(mfcc_features.tolist())        \n",
    "\n",
    "        sequence_features.append(segment_features)\n",
    "\n",
    "    return np.array(sequence_features)\n",
    "\n",
    "\n",
    "def audio_features_extract(audio_data, num_segments=20, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extracts sequential audio features (DWT + MFCC) from a list of audio signals.\n",
    "\n",
    "    :param audio_data: List of (signal, sample_rate) tuples\n",
    "    :param num_segments: Number of segments per audio file\n",
    "    :param n_mfcc: Number of MFCC coefficients to compute\n",
    "    :return: Numpy array of shape (num_samples, num_segments, num_features)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for signal, sample_rate in tqdm(audio_data, desc=\"Extracting audio features\"):\n",
    "        sequence_features = preprocess_audio_sequential_fixed_segments(\n",
    "            signal, sample_rate, num_segments=num_segments, n_mfcc=n_mfcc\n",
    "        )\n",
    "        features.append(sequence_features)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:19:47.760513Z",
     "iopub.status.busy": "2025-04-08T18:19:47.759981Z",
     "iopub.status.idle": "2025-04-08T18:21:14.910371Z",
     "shell.execute_reply": "2025-04-08T18:21:14.909471Z",
     "shell.execute_reply.started": "2025-04-08T18:19:47.760484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load data from dataset\n",
    "train_audio_holdout, train_labels_audio_holdout = load_audio_data(train_audio_path, new_train_list)\n",
    "val_audio_holdout, val_labels_audio_holdout = load_audio_data(val_audio_path, new_val_list)\n",
    "test_audio_holdout, test_labels_audio_holdout = load_audio_data(test_audio_path, new_test_list)\n",
    "\n",
    "# Extract features for each audio\n",
    "train_features_audio_holdout = audio_features_extract(train_audio_holdout)\n",
    "val_features_audio_holdout = audio_features_extract(val_audio_holdout)\n",
    "test_features_audio_holdout = audio_features_extract(test_audio_holdout)\n",
    "\n",
    "# Print dimensions\n",
    "print(train_features_audio_holdout.shape, train_labels_audio_holdout.shape)\n",
    "print(val_features_audio_holdout.shape, val_labels_audio_holdout.shape)\n",
    "print(test_features_audio_holdout.shape, test_labels_audio_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:21:14.920616Z",
     "iopub.status.busy": "2025-04-08T18:21:14.917157Z",
     "iopub.status.idle": "2025-04-08T18:23:45.274108Z",
     "shell.execute_reply": "2025-04-08T18:23:45.271427Z",
     "shell.execute_reply.started": "2025-04-08T18:21:14.920551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fold_dirs = [f\"{cross_audio_path}/fold_{index}\" for index in range(4)]\n",
    " \n",
    "data_audio_cross, labels_audio_cross = [], []\n",
    "data_audio_cross_pad, labels_audio_cross_pad = [], []\n",
    " \n",
    "# Loading and feature extraction for each fold\n",
    "for i in range(4):\n",
    "    print(f\"Processing fold {i}...\")\n",
    "\n",
    "    # Load data from original fold\n",
    "    fold_audio, fold_audio_labels = load_audio_data(fold_dirs[i], new_cross_list[i])\n",
    "    \n",
    "    # Extracting audio features\n",
    "    fold_audio_features = audio_features_extract(fold_audio)\n",
    "\n",
    "    print(fold_audio_features.shape, fold_audio_labels.shape)\n",
    "    \n",
    "    # Saving into respective arrays\n",
    "    data_audio_cross.append(fold_audio_features)\n",
    "    labels_audio_cross.append(fold_audio_labels)\n",
    "\n",
    "    # Load data from padded fold\n",
    "    fold_audio, fold_audio_labels = load_audio_data(fold_dirs[i], new_cross_list_pad[i])\n",
    "    \n",
    "    # Extract audio features\n",
    "    fold_audio_features = audio_features_extract(fold_audio)\n",
    "\n",
    "    print(fold_audio_features.shape, fold_audio_labels.shape)\n",
    "    \n",
    "    # Saving into respective arrays\n",
    "    data_audio_cross_pad.append(fold_audio_features)\n",
    "    labels_audio_cross_pad.append(fold_audio_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:23:45.280304Z",
     "iopub.status.busy": "2025-04-08T18:23:45.277000Z",
     "iopub.status.idle": "2025-04-08T18:23:45.298044Z",
     "shell.execute_reply": "2025-04-08T18:23:45.297003Z",
     "shell.execute_reply.started": "2025-04-08T18:23:45.280244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to create video model\n",
    "def video_Conv3D_model(input_shape, learning_rate=1e-4, p=0):\n",
    "    model = Sequential([\n",
    "        InputLayer(shape=input_shape),\n",
    "        \n",
    "        # Conv3D Block\n",
    "        Conv3D(filters=32, kernel_size=(3, 3, 2), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling3D(pool_size=(2, 2, 1)),  # Riduce solo altezza e larghezza\n",
    "\n",
    "        # Flatten e Fully Connected\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')  # Output per classificazione binaria\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    if p:\n",
    "        print(model.summary())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:23:45.302979Z",
     "iopub.status.busy": "2025-04-08T18:23:45.299234Z",
     "iopub.status.idle": "2025-04-08T18:23:45.323048Z",
     "shell.execute_reply": "2025-04-08T18:23:45.321992Z",
     "shell.execute_reply.started": "2025-04-08T18:23:45.302927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def audio_lstm_model(input_shape, hidden_size, learning_rate):\n",
    "    model = Sequential([\n",
    "        InputLayer(shape=input_shape),\n",
    "        Masking(mask_value=0.0),\n",
    "        LSTM(hidden_size, return_sequences=False, use_cudnn=False),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_audio_lstm_model(\n",
    "    train_features, train_labels,\n",
    "    val_features, val_labels,\n",
    "    test_features, test_labels,\n",
    "    hidden_size=128,\n",
    "    num_epochs=20,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    view_res=True\n",
    "):\n",
    "    # Definition of LSTM model\n",
    "    model = audio_lstm_model(input_shape=(train_features.shape[1], train_features.shape[2]),\n",
    "                       hidden_size = hidden_size, learning_rate=learning_rate)\n",
    "    \n",
    "    # Model Training\n",
    "    history = model.fit(\n",
    "        train_features, train_labels,\n",
    "        validation_data=(val_features, val_labels),\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=view_res\n",
    "    )\n",
    "\n",
    "    if view_res:\n",
    "        plot_history(history)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:23:45.326110Z",
     "iopub.status.busy": "2025-04-08T18:23:45.325078Z",
     "iopub.status.idle": "2025-04-08T18:23:45.341577Z",
     "shell.execute_reply": "2025-04-08T18:23:45.340662Z",
     "shell.execute_reply.started": "2025-04-08T18:23:45.326056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plots training and validation accuracy and loss over epochs.\n",
    "\n",
    "    :param history: Keras History object returned by model.fit()\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the confusion matrix.\n",
    "\n",
    "    :param conf_matrix: 2D array-like confusion matrix (e.g. from sklearn.metrics.confusion_matrix)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=['Lie', 'Truth'], yticklabels=['Lie', 'Truth'])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes and displays the confusion matrix using sklearn's ConfusionMatrixDisplay.\n",
    "\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    cm_display.plot(ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Late Fusion (Mean):\n",
    "The best models of the Video and Audio classification tasks are taken, after which the probabilities associated with each prediction (at the video level) are taken and combined linearly to obtain a combined answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:23:45.344088Z",
     "iopub.status.busy": "2025-04-08T18:23:45.343041Z",
     "iopub.status.idle": "2025-04-08T18:23:45.356829Z",
     "shell.execute_reply": "2025-04-08T18:23:45.355659Z",
     "shell.execute_reply.started": "2025-04-08T18:23:45.344032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine_predictions_weighted(video_predictions, audio_predictions, audio_weight=0.5):\n",
    "    video_weight = 1 - audio_weight\n",
    "    combined_predictions = (video_predictions * video_weight) + (audio_predictions * audio_weight)\n",
    "    combined_classes = (combined_predictions > 0.5).astype(int)\n",
    "    return combined_classes, combined_predictions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:23:45.362820Z",
     "iopub.status.busy": "2025-04-08T18:23:45.361650Z",
     "iopub.status.idle": "2025-04-08T18:24:02.642187Z",
     "shell.execute_reply": "2025-04-08T18:24:02.641344Z",
     "shell.execute_reply.started": "2025-04-08T18:23:45.362773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Video model training and prediction\n",
    "print(f\"\\nVIDEO TRAINING\\n\")\n",
    "input_shape_video_holdout = train_features_video_holdout.shape[1:]\n",
    "learning_rate = 1e-4\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "callbacks = [ModelCheckpoint(model_path, monitor='val_accuracy', mode='max',\n",
    "                             save_best_only=True, save_weights_only=True)]\n",
    "\n",
    "video_model_holdout_weight = video_Conv3D_model(input_shape_video_holdout, learning_rate)\n",
    "\n",
    "# Fit the network\n",
    "history_video_holdout_weight = video_model_holdout_weight.fit(\n",
    "    train_features_video_holdout,\n",
    "    train_labels_video_holdout,\n",
    "    epochs=epochs, batch_size=batch_size,\n",
    "    validation_data=(val_features_video_holdout, val_labels_video_holdout),\n",
    "    callbacks=callbacks)\n",
    "\n",
    "plot_history(history_video_holdout_weight)\n",
    "video_model_holdout_weight.load_weights(model_path)\n",
    "\n",
    "# Make predictions\n",
    "pred = video_model_holdout_weight.predict(test_features_video_holdout)\n",
    "\n",
    "# Create a dataframe with predictions and video ids\n",
    "pred_df = pd.DataFrame({'video_id': test_video_names_holdout, 'prediction': pred.flatten(), 'label': test_labels_video_holdout})\n",
    "\n",
    "# Function to calculate the average\n",
    "def calculate_mean(preds):\n",
    "    return np.mean(preds)\n",
    "\n",
    "# Group by video and compute the average prediction\n",
    "video_predictions_holdout_weight = pred_df.groupby('video_id', sort=False)['prediction'].apply(calculate_mean).values\n",
    "video_predictions_holdout_weight = np.array([[x] for x in video_predictions_holdout_weight])\n",
    "\n",
    "\n",
    "# Audio model training and prediction\n",
    "print(f\"\\n\\n\\nAUDIO TRAINING\\n\")\n",
    "audio_model_holdout_weight = train_and_evaluate_audio_lstm_model(\n",
    "    train_features_audio_holdout, train_labels_audio_holdout,\n",
    "    val_features_audio_holdout, val_labels_audio_holdout,\n",
    "    test_features_audio_holdout, test_labels_audio_holdout,\n",
    "    hidden_size=128, num_epochs=20, batch_size=32, learning_rate=0.001, view_res=True\n",
    ")\n",
    "audio_predictions_holdout_weight = audio_model_holdout_weight.predict(test_features_audio_holdout)  # Predicted probabilities\n",
    "\n",
    "\n",
    "# Combine predictions with specific weights\n",
    "print(f\"\\n\\n\\nCOMBINING RESULTS\\n\")\n",
    "audio_weight = 0  # Weight assigned to audio predictions\n",
    "combined_classes_holdout_weight, combined_probs_holdout_weight = combine_predictions_weighted(\n",
    "    video_predictions_holdout_weight, audio_predictions_holdout_weight, audio_weight=audio_weight\n",
    ")\n",
    "\n",
    "# Metrics calculation\n",
    "accuracy_holdout_weight = accuracy_score(test_labels_audio_holdout, combined_classes_holdout_weight)\n",
    "precision_holdout_weight = precision_score(test_labels_audio_holdout, combined_classes_holdout_weight, zero_division=0, average=None)\n",
    "recall_holdout_weight = recall_score(test_labels_audio_holdout, combined_classes_holdout_weight, zero_division=0, average=None)\n",
    "f1_holdout_weight = f1_score(test_labels_audio_holdout, combined_classes_holdout_weight, zero_division=0, average=None)\n",
    "auc_holdout_weight = roc_auc_score(test_labels_audio_holdout, combined_probs_holdout_weight)\n",
    "\n",
    "# Results visualization\n",
    "conf_matrix_holdout_weight = confusion_matrix(test_labels_audio_holdout, combined_classes_holdout_weight)\n",
    "plot_confusion_matrix(conf_matrix_holdout_weight)\n",
    "\n",
    "# Save metrics into the dictionary\n",
    "metrics_holdout[\"Accuracy\"].append(accuracy_holdout_weight)\n",
    "metrics_holdout[\"Precision (0)\"].append(precision_holdout_weight[0])\n",
    "metrics_holdout[\"Precision (1)\"].append(precision_holdout_weight[1])\n",
    "metrics_holdout[\"Recall (0)\"].append(recall_holdout_weight[0])\n",
    "metrics_holdout[\"Recall (1)\"].append(recall_holdout_weight[1])\n",
    "metrics_holdout[\"F1 (0)\"].append(f1_holdout_weight[0])\n",
    "metrics_holdout[\"F1 (1)\"].append(f1_holdout_weight[1])\n",
    "metrics_holdout[\"AUC\"].append(auc_holdout_weight)\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(metrics_holdout, orient='index', columns=[\"LATE FUSION (Mean)\"])\n",
    "print(df)\n",
    "metrics_holdout = {key: [] for key in metrics_holdout}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:24:02.643961Z",
     "iopub.status.busy": "2025-04-08T18:24:02.643566Z",
     "iopub.status.idle": "2025-04-08T18:24:02.663561Z",
     "shell.execute_reply": "2025-04-08T18:24:02.662863Z",
     "shell.execute_reply.started": "2025-04-08T18:24:02.643918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_video_audio(metrics_dict, data_video, data_video_pad, data_audio, data_audio_pad, \n",
    "                                 labels_video, labels_video_pad, labels_audio, labels_audio_pad,\n",
    "                                 names_video, names_video_pad, num_folds=4):\n",
    "    \"\"\"\n",
    "    Cross-validation function that combines video and audio features for binary classification.\n",
    "    \"\"\"\n",
    "    accuracy_scores, auc_scores = [], []\n",
    "    precision_scores_0, recall_scores_0, f1_scores_0 = [], [], []\n",
    "    precision_scores_1, recall_scores_1, f1_scores_1 = [], [], []\n",
    " \n",
    "    for i in range(num_folds):\n",
    "        print(f'\\nFold {i}')\n",
    "        \n",
    "        # Split the folds into train and test sets\n",
    "        test_features_video = data_video_pad[i]\n",
    "        test_labels_video = labels_video_pad[i]\n",
    "        test_video_names = names_video_pad[i]\n",
    "        test_features_audio, test_labels_audio = np.array(data_audio_pad[i]), np.array(labels_audio_pad[i])\n",
    "\n",
    "        train_features_video, train_labels_video, train_video_names = [], [], []\n",
    "        for j in range(num_folds):\n",
    "            if j != i:\n",
    "                train_features_video.append(data_video[j])\n",
    "                train_labels_video.append(labels_video[j])\n",
    "                train_video_names.extend(names_video[j])\n",
    "        train_features_video = np.vstack(train_features_video)\n",
    "        train_labels_video = np.hstack(train_labels_video)\n",
    "        train_features_audio = np.array([item for idx, fold in enumerate(data_audio) if idx != i for item in fold])\n",
    "        train_labels_audio = np.array([label for idx, fold in enumerate(labels_audio) if idx != i for label in fold])\n",
    "\n",
    "        \n",
    "        # Create and train the model for video features\n",
    "        input_shape = train_features_video.shape[1:]\n",
    "        learning_rate = 1e-4\n",
    "        epochs = 20\n",
    "        batch_size = 256\n",
    "        video_model = video_Conv3D_model(input_shape, learning_rate=learning_rate)\n",
    "        video_history = video_model.fit(train_features_video, train_labels_video, \n",
    "                            validation_data = (test_features_video, test_labels_video),\n",
    "                            epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        video_test_predictions = video_model.predict(test_features_video)\n",
    "        video_pred_df = pd.DataFrame({'video_id': test_video_names, 'prediction': video_test_predictions.flatten(), 'label': test_labels_video})\n",
    "        \n",
    "        def calculate_mean(preds):\n",
    "            return np.mean(preds)\n",
    "        \n",
    "        video_predictions = video_pred_df.groupby('video_id', sort=False)['prediction'].apply(calculate_mean).values\n",
    "        video_predictions = np.array([[x] for x in video_predictions])\n",
    "\n",
    "        #plot_history(video_history)\n",
    "\n",
    "        \n",
    "        # Create and train the model for audio features\n",
    "        audio_model = train_and_evaluate_audio_lstm_model(\n",
    "            train_features_audio, train_labels_audio,\n",
    "            test_features_audio, test_labels_audio,\n",
    "            test_features_audio, test_labels_audio,\n",
    "            hidden_size=128, num_epochs=20, batch_size=32, learning_rate=0.001, view_res=False\n",
    "        )\n",
    "        audio_predictions = audio_model.predict(test_features_audio)  # Audio probabilities\n",
    "        \n",
    "        # Group by video and get labels\n",
    "        video_labels = video_pred_df.groupby('video_id', sort=False)['label'].first().values\n",
    "\n",
    "        # Combine predictions with weights\n",
    "        audio_weight = 0.5\n",
    "        combined_predictions, combined_probs = combine_predictions_weighted(video_predictions, audio_predictions, audio_weight)\n",
    "        \n",
    "        # Compute metrics for the current fold\n",
    "        accuracy = accuracy_score(test_labels_audio, combined_predictions)\n",
    "        precision = precision_score(test_labels_audio, combined_predictions, zero_division=0, average=None)\n",
    "        recall = recall_score(test_labels_audio, combined_predictions, zero_division=0, average=None)\n",
    "        f1 = f1_score(test_labels_audio, combined_predictions, average=None)\n",
    "        auc = roc_auc_score(test_labels_audio, combined_probs)\n",
    " \n",
    "        # Store metrics for this fold\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores_0.append(precision[0])\n",
    "        recall_scores_0.append(recall[0])\n",
    "        f1_scores_0.append(f1[0])\n",
    "        precision_scores_1.append(precision[1])\n",
    "        recall_scores_1.append(recall[1])\n",
    "        f1_scores_1.append(f1[1])\n",
    "        auc_scores.append(auc)\n",
    " \n",
    "        print(f\"Fold {i} - Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "        # Predictions using only video\n",
    "        audio_weight = 0\n",
    "        combined_predictions, combined_probs = combine_predictions_weighted(video_predictions, audio_predictions, audio_weight)\n",
    "        print(f\"only-video acc: {accuracy_score(video_labels, combined_predictions)}\")\n",
    "\n",
    "        # Predictions using only audio\n",
    "        audio_weight = 1\n",
    "        combined_predictions, combined_probs = combine_predictions_weighted(video_predictions, audio_predictions, audio_weight)\n",
    "        print(f\"only-audio acc: {accuracy_score(test_labels_audio, combined_predictions)}\")\n",
    " \n",
    "    # Average metrics across all folds\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "    avg_precision_0, avg_recall_0, avg_f1_0 = np.mean(precision_scores_0), np.mean(recall_scores_0), np.mean(f1_scores_0)\n",
    "    avg_precision_1, avg_recall_1, avg_f1_1 = np.mean(precision_scores_1), np.mean(recall_scores_1), np.mean(f1_scores_1)\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "\n",
    "    metrics_dict[\"Mean Accuracy\"].append(avg_accuracy)\n",
    "    metrics_dict[\"Mean Precision (0)\"].append(avg_precision_0)\n",
    "    metrics_dict[\"Mean Precision (1)\"].append(avg_precision_1)\n",
    "    metrics_dict[\"Mean Recall (0)\"].append(avg_recall_0)\n",
    "    metrics_dict[\"Mean Recall (1)\"].append(avg_recall_1)\n",
    "    metrics_dict[\"Mean F1 (0)\"].append(avg_f1_0)\n",
    "    metrics_dict[\"Mean F1 (1)\"].append(avg_f1_1)\n",
    "    metrics_dict[\"Mean AUC\"].append(avg_auc)\n",
    "\n",
    "    # Standard deviation of the metrics across all folds\n",
    "    std_accuracy = np.std(accuracy_scores)\n",
    "    std_precision_0, std_recall_0, std_f1_0 = np.std(precision_scores_0), np.std(recall_scores_0), np.std(f1_scores_0)\n",
    "    std_precision_1, std_recall_1, std_f1_1 = np.std(precision_scores_1), np.std(recall_scores_1), np.std(f1_scores_1)\n",
    "    std_auc = np.std(auc_scores)\n",
    "    \n",
    "    metrics_dict[\"Std Accuracy\"].append(std_accuracy)\n",
    "    metrics_dict[\"Std Precision (0)\"].append(std_precision_0)\n",
    "    metrics_dict[\"Std Precision (1)\"].append(std_precision_1)\n",
    "    metrics_dict[\"Std Recall (0)\"].append(std_recall_0)\n",
    "    metrics_dict[\"Std Recall (1)\"].append(std_recall_1)\n",
    "    metrics_dict[\"Std F1 (0)\"].append(std_f1_0)\n",
    "    metrics_dict[\"Std F1 (1)\"].append(std_f1_1)\n",
    "    metrics_dict[\"Std AUC\"].append(std_auc)\n",
    "\n",
    "    print(f\"\\nAvg Accuracy: {avg_accuracy:.2f}\")\n",
    "    print(f\"Std Dev Accuracy: {std_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:24:02.664773Z",
     "iopub.status.busy": "2025-04-08T18:24:02.664502Z",
     "iopub.status.idle": "2025-04-08T18:24:59.956932Z",
     "shell.execute_reply": "2025-04-08T18:24:59.956107Z",
     "shell.execute_reply.started": "2025-04-08T18:24:02.664746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cross_validation_video_audio(metrics_cross, data_video_cross, data_video_cross_pad, \n",
    "                                data_audio_cross, data_audio_cross_pad,\n",
    "                                 labels_video_cross, labels_video_cross_pad, labels_audio_cross,\n",
    "                                 labels_audio_cross_pad,\n",
    "                                 names_video_cross, names_video_cross_pad, num_folds=4)\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(metrics_cross, orient='index', columns=[\"LATE FUSION (Mean)\"])\n",
    "print(\"\\n\")\n",
    "print(df)\n",
    "metrics_cross = {key: [] for key in metrics_cross}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Late Fusion (Meta Learner):\n",
    "The results of the Video and Audio classification tasks are taken, \n",
    "then given as input to a MetaLearner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:24:59.958556Z",
     "iopub.status.busy": "2025-04-08T18:24:59.958200Z",
     "iopub.status.idle": "2025-04-08T18:24:59.971808Z",
     "shell.execute_reply": "2025-04-08T18:24:59.970951Z",
     "shell.execute_reply.started": "2025-04-08T18:24:59.958517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_meta_features(video_features, audio_features, video_labels, audio_labels, video_names,\n",
    "                           n_splits=10):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    meta_features = []\n",
    "    meta_labels = []\n",
    "    set_video_names, indices = np.unique(video_names, return_index=True)\n",
    "    set_video_names = set_video_names[np.argsort(indices)]\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(skf.split(audio_features, audio_labels)):\n",
    "        print(f'Fold_{i}:')\n",
    "\n",
    "        train_video_names, val_video_names = set_video_names[train_index], set_video_names[val_index]\n",
    "        for name in train_video_names:\n",
    "            train_video_index = [j for j, n in enumerate(video_names) if n in train_video_names]\n",
    "            val_video_index = [j for j, n in enumerate(video_names) if n in val_video_names]\n",
    "        train_video, val_video, val_video_names = video_features[train_video_index], video_features[val_video_index], video_names[val_video_index]\n",
    "        train_video_labels, val_video_labels = video_labels[train_video_index], video_labels[val_video_index]\n",
    "        train_audio, val_audio = audio_features[train_index], audio_features[val_index]\n",
    "        train_audio_labels, val_audio_labels = audio_labels[train_index], audio_labels[val_index]\n",
    "\n",
    "        print(train_video.shape, train_video_labels.shape)\n",
    "        print(val_video.shape, val_video_labels.shape)\n",
    "        print(train_audio.shape, train_audio_labels.shape)\n",
    "        print(val_audio.shape, val_audio_labels.shape)\n",
    "\n",
    "        # Base models training\n",
    "        input_shape = train_video.shape[1:]\n",
    "        learning_rate = 1e-4\n",
    "        epochs = 20\n",
    "        batch_size = 256\n",
    "        video_model = video_Conv3D_model(input_shape, learning_rate=learning_rate)\n",
    "        video_history = video_model.fit(train_video, train_video_labels, \n",
    "                            validation_data = (val_video, val_video_labels),\n",
    "                            epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        audio_model = train_and_evaluate_audio_lstm_model(\n",
    "            train_audio, train_audio_labels,\n",
    "            val_audio, val_audio_labels,\n",
    "            val_audio, val_audio_labels,  \n",
    "            hidden_size=128, num_epochs=20, batch_size=32, learning_rate=0.001, view_res=False\n",
    "        )\n",
    "        \n",
    "        # Predictions on validation fold\n",
    "        video_val_predictions = video_model.predict(val_video)\n",
    "        video_pred_df = pd.DataFrame({'video_id': val_video_names, 'prediction': video_val_predictions.flatten(), 'label': val_video_labels})\n",
    "        def calculate_mean(preds):\n",
    "            return np.mean(preds)\n",
    "        video_predictions = video_pred_df.groupby('video_id', sort=False)['prediction'].apply(calculate_mean).values\n",
    "        video_predictions = np.array([[x] for x in video_predictions])\n",
    "        video_preds = video_predictions.flatten().reshape(-1, 1)\n",
    "        \n",
    "        audio_preds = audio_model.predict(val_audio).flatten().reshape(-1, 1)\n",
    "\n",
    "        val_video_labels = video_pred_df.groupby('video_id', sort=False)['label'].first().values\n",
    "        print(val_audio_labels)\n",
    "        print(val_video_labels)\n",
    "        \n",
    "        # Saving predictions and labels\n",
    "        fold_meta_features = np.hstack((video_preds, audio_preds))\n",
    "        meta_features.append(fold_meta_features)\n",
    "        meta_labels.append(val_audio_labels)\n",
    "    \n",
    "    # Concatenate results af all folds\n",
    "    meta_features = np.vstack(meta_features)\n",
    "    meta_labels = np.hstack(meta_labels)\n",
    "    \n",
    "    return video_model, audio_model, meta_features, meta_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:24:59.973282Z",
     "iopub.status.busy": "2025-04-08T18:24:59.973002Z",
     "iopub.status.idle": "2025-04-08T18:25:45.242728Z",
     "shell.execute_reply": "2025-04-08T18:25:45.241550Z",
     "shell.execute_reply.started": "2025-04-08T18:24:59.973257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pre-processes training data with padding so that it can be merged with validation data\n",
    "seq_gen_train = list(list(gen_sequence(train_dataset[train_dataset['input'] == id], \n",
    "                                     sequence_length, cols_groups, frame_step, \n",
    "                                     padding))\n",
    "                   for id in train_list)\n",
    "seq_gen_train = [x for x in seq_gen_train if len(x) > 0]\n",
    "seq_array_train = [[t[0] for t in sublist] for sublist in seq_gen_train]\n",
    "label_array_train = [[t[1] for t in sublist] for sublist in seq_gen_train]\n",
    "video_array_train = [[t[2] for t in sublist] for sublist in seq_gen_train]\n",
    "seq_array_train = np.concatenate(seq_array_train).astype(np.float32)\n",
    "train_labels_video_holdout_stack = np.concatenate(label_array_train).astype(np.float32).reshape(-1)\n",
    "train_video_names_holdout_stack = np.concatenate(video_array_train)\n",
    "train_features_video_holdout_stack = np.transpose(seq_array_train, (0, 1, 3, 2))\n",
    "train_features_video_holdout_stack = np.expand_dims(train_features_video_holdout_stack, axis=-1)\n",
    "\n",
    "new_train_list_stack, indices = np.unique(train_video_names_holdout_stack, return_index=True)\n",
    "new_train_list_stack = new_train_list_stack[np.argsort(indices)].tolist()\n",
    "\n",
    "train_audio_holdout_stack, train_labels_audio_holdout_stack = load_audio_data(train_audio_path, new_train_list_stack)\n",
    "train_features_audio_holdout_stack = audio_features_extract(train_audio_holdout_stack)\n",
    "\n",
    "# Combine training and validation features\n",
    "train_features_video_holdout_stack = np.vstack([train_features_video_holdout_stack, val_features_video_holdout])\n",
    "train_features_audio_holdout_stack = np.vstack([train_features_audio_holdout_stack, val_features_audio_holdout])\n",
    "train_video_labels_holdout_stack = np.hstack([train_labels_video_holdout_stack, val_labels_video_holdout])\n",
    "train_audio_labels_holdout_stack = np.hstack([train_labels_audio_holdout_stack, val_labels_audio_holdout])\n",
    "train_video_names_holdout_stack = np.hstack([train_video_names_holdout_stack, val_video_names_holdout])\n",
    "\n",
    "# Print dimensions\n",
    "print(train_features_video_holdout_stack.shape)\n",
    "print(test_features_video_holdout.shape)\n",
    "print()\n",
    "\n",
    "print(train_features_audio_holdout_stack.shape)\n",
    "print(test_features_audio_holdout.shape)\n",
    "print()\n",
    "\n",
    "print(train_video_labels_holdout_stack.shape)\n",
    "print(test_labels_video_holdout.shape)\n",
    "\n",
    "print()\n",
    "print(train_video_names_holdout_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:25:45.247228Z",
     "iopub.status.busy": "2025-04-08T18:25:45.246748Z",
     "iopub.status.idle": "2025-04-08T18:27:50.152565Z",
     "shell.execute_reply": "2025-04-08T18:27:50.151773Z",
     "shell.execute_reply.started": "2025-04-08T18:25:45.247174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset generation for the meta-learner\n",
    "video_model_holdout_stack, audio_model_holdout_stack, meta_features_holdout, meta_labels_holdout = generate_meta_features( \n",
    "    video_features=train_features_video_holdout_stack, \n",
    "    audio_features=train_features_audio_holdout_stack,\n",
    "    video_labels=train_video_labels_holdout_stack,\n",
    "    audio_labels=train_audio_labels_holdout_stack,\n",
    "    video_names=train_video_names_holdout_stack,\n",
    "    n_splits=5\n",
    ")\n",
    "                                                         \n",
    "# Training the meta-learner\n",
    "stacking_model_holdout = LogisticRegression()\n",
    "stacking_model_holdout.fit(meta_features_holdout, meta_labels_holdout)\n",
    "\n",
    "# Evaluation on the test set\n",
    "video_test_preds_holdout_stack = video_model_holdout_stack.predict(test_features_video_holdout)\n",
    "video_pred_df = pd.DataFrame({'video_id': test_video_names_holdout, 'prediction': video_test_preds_holdout_stack.flatten(), 'label': test_labels_video_holdout})\n",
    "\n",
    "def calculate_mean(preds):\n",
    "    return np.mean(preds)\n",
    "\n",
    "video_test_preds_holdout_stack = video_pred_df.groupby('video_id', sort=False)['prediction'].apply(calculate_mean).values\n",
    "video_test_preds_holdout_stack = np.array([[x] for x in video_test_preds_holdout_stack])\n",
    "video_test_preds_holdout_stack = video_test_preds_holdout_stack.flatten().reshape(-1, 1)\n",
    "audio_test_preds_holdout_stack = audio_model_holdout_stack.predict(test_features_audio_holdout).flatten().reshape(-1, 1)\n",
    "test_meta_features_holdout_stack = np.hstack((video_test_preds_holdout_stack, audio_test_preds_holdout_stack))\n",
    "\n",
    "# Prediction with the meta-learner\n",
    "stacking_preds_holdout = stacking_model_holdout.predict(test_meta_features_holdout_stack)  # Predicted classes\n",
    "stacking_probs_holdout = stacking_model_holdout.predict_proba(test_meta_features_holdout_stack)[:, 1]  # Probability for the positive class\n",
    "\n",
    "test_labels_video_holdout_stack = video_pred_df.groupby('video_id', sort=False)['label'].first().values\n",
    "print(test_labels_audio_holdout)\n",
    "print(test_labels_video_holdout_stack)\n",
    "\n",
    "# Metrics calculation\n",
    "accuracy_holdout_stack = accuracy_score(test_labels_audio_holdout, stacking_preds_holdout)\n",
    "precision_holdout_stack, recall_holdout_stack, f1_holdout_stack, _ = precision_recall_fscore_support(\n",
    "    test_labels_audio_holdout, stacking_preds_holdout, average=None, labels=[0, 1]\n",
    ")\n",
    "auc_holdout_stack = roc_auc_score(test_labels_audio_holdout, stacking_probs_holdout)\n",
    " \n",
    "# Save metrics to the dictionary\n",
    "metrics_holdout[\"Accuracy\"].append(accuracy_holdout_stack)\n",
    "metrics_holdout[\"Precision (0)\"].append(precision_holdout_stack[0])\n",
    "metrics_holdout[\"Precision (1)\"].append(precision_holdout_stack[1])\n",
    "metrics_holdout[\"Recall (0)\"].append(recall_holdout_stack[0])\n",
    "metrics_holdout[\"Recall (1)\"].append(recall_holdout_stack[1])\n",
    "metrics_holdout[\"F1 (0)\"].append(f1_holdout_stack[0])\n",
    "metrics_holdout[\"F1 (1)\"].append(f1_holdout_stack[1])\n",
    "metrics_holdout[\"AUC\"].append(auc_holdout_stack)\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(metrics_holdout, orient='index', columns=[\"LATE FUSION (Meta-Learner)\"])\n",
    "print(df)\n",
    "metrics_holdout = {key: [] for key in metrics_holdout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Fusion:\n",
    "We extract sequences of 64 frames from each video, then extract the audio corresponding to that part of the video and organize it into sequences too. Finally, we train a model consisting of a first part that works separately on the video and audio features, and then finish with a single MLP network for final binary classification. The commented parts were used to extract the new features, which were then downloaded so that the extraction process would not have to be repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:27:50.154287Z",
     "iopub.status.busy": "2025-04-08T18:27:50.153999Z",
     "iopub.status.idle": "2025-04-08T18:28:02.694751Z",
     "shell.execute_reply": "2025-04-08T18:28:02.693894Z",
     "shell.execute_reply.started": "2025-04-08T18:27:50.154245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_audio_data_segmented(audio_dataset_path, video_dataset, seq_len=sequence_length*frame_step, frame_step=frame_step, padding=None):\n",
    "    audios = []\n",
    "    audio_labels = []\n",
    "    audio_names_list = []\n",
    "\n",
    "    # Extract audio data\n",
    "    for root, dirs, files in os.walk(audio_dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.xlsx'):\n",
    "                \n",
    "                # Path of the Excel file\n",
    "                excel_path = os.path.join(root, file)\n",
    "                df = pd.read_excel(excel_path)\n",
    "                \n",
    "                # Extract audio names, video names and label\n",
    "                label_list = df['label'].tolist()\n",
    "                audio_names = df['audio name'].tolist()\n",
    "                video_names = df['video name'].tolist()\n",
    "                \n",
    "                # Segment the corresponding audio files\n",
    "                for label, audio_name, video_name in zip(label_list, audio_names, video_names):                    \n",
    "                    audio_path = os.path.join(audio_dataset_path, 'Audio', audio_name)\n",
    "                    video_path = os.path.join(audio_dataset_path, 'Statements', video_name)\n",
    "                    frame_dataset = video_dataset[video_dataset['input'] == video_path]\n",
    "                    total_frames = len(frame_dataset)\n",
    "                    \n",
    "                    if os.path.exists(audio_path):\n",
    "                        signal, sample_rate = librosa.load(audio_path, sr=None)  # Load audio with original sample rate\n",
    "                        total_duration_ms = librosa.get_duration(y=signal, sr=sample_rate) * 1000  # Total audio duration in milliseconds\n",
    "                        frame_rate = total_frames / total_duration_ms\n",
    "                        seq_duration_ms = seq_len / frame_rate\n",
    "                        \n",
    "                        # Extract audio segments corresponding to video sequences\n",
    "                        segments = []\n",
    "                        start_time_ms = 0\n",
    "                        while np.round(start_time_ms + seq_duration_ms, 3) <= np.round(total_duration_ms, 3):\n",
    "                            start_sample = int((start_time_ms / 1000) * sample_rate)\n",
    "                            end_sample = int(((start_time_ms + seq_duration_ms) / 1000) * sample_rate)\n",
    "                            segments.append(signal[start_sample:end_sample])\n",
    "                            start_time_ms += frame_step / frame_rate\n",
    "                            \n",
    "                        if not segments and padding:\n",
    "                            segments.append(signal)\n",
    "\n",
    "                        for segment in segments:\n",
    "                            audios.append((segment, sample_rate))\n",
    "                            audio_labels.append(label)\n",
    "                            audio_names_list.append(audio_name)\n",
    "                    else:\n",
    "                        print(f\"Audio file not found: {audio_path}\")\n",
    "    \n",
    "    return audios, np.array(audio_labels), np.array(audio_names_list)\n",
    "\n",
    "num_segments = 20\n",
    "\n",
    "# Load data from the dataset\n",
    "train_audio_holdout_inter, train_labels_audio_holdout_inter, train_audio_names_holdout_inter = load_audio_data_segmented(train_audio_path, train_dataset, sequence_length*frame_step, frame_step, padding=None)\n",
    "val_audio_holdout_inter, val_labels_audio_holdout_inter, val_audio_names_holdout_inter = load_audio_data_segmented(val_audio_path, val_dataset, sequence_length*frame_step, frame_step, padding)\n",
    "test_audio_holdout_inter, test_labels_audio_holdout_inter, test_audio_names_holdout_inter = load_audio_data_segmented(test_audio_path, test_dataset, sequence_length*frame_step, frame_step, padding)\n",
    "\n",
    "\"\"\"# Feature extraction for each audio\n",
    "train_features_audio_holdout_inter = audio_features_extract(train_audio_holdout_inter, num_segments=num_segments)\n",
    "val_features_audio_holdout_inter = audio_features_extract(val_audio_holdout_inter, num_segments=num_segments)\n",
    "test_features_audio_holdout_inter = audio_features_extract(test_audio_holdout_inter, num_segments=num_segments)\n",
    "\n",
    "# Print shapes\n",
    "print(train_features_audio_holdout_inter.shape, train_labels_audio_holdout_inter.shape)\n",
    "print(val_features_audio_holdout_inter.shape, val_labels_audio_holdout_inter.shape)\n",
    "print(test_features_audio_holdout_inter.shape, test_labels_audio_holdout_inter.shape)\"\"\"\n",
    "\n",
    "fold_dirs = [f\"{cross_audio_path}/fold_{index}\" for index in range(4)]\n",
    "data_audio_cross_inter, labels_audio_cross_inter, names_audio_cross_inter = [], [], []\n",
    "data_audio_cross_pad_inter, labels_audio_cross_pad_inter, names_audio_cross_pad_inter = [], [], []\n",
    " \n",
    "# Load and extract features for each fold\n",
    "for i in range(4):\n",
    "    print(f\"Processing fold {i}...\")\n",
    "\n",
    "    # No padding\n",
    "    fold_audio, fold_audio_labels, fold_audio_names = load_audio_data_segmented(fold_dirs[i], cross_video_datasets[i], padding=None)\n",
    "    #fold_audio_features = audio_features_extract(fold_audio, num_segments=num_segments)\n",
    "    #print(fold_audio_features.shape, fold_audio_labels.shape)\n",
    "    #data_audio_cross_inter.append(fold_audio_features)\n",
    "    #labels_audio_cross_inter.append(fold_audio_labels)\n",
    "    names_audio_cross_inter.append(fold_audio_names)\n",
    "\n",
    "    # Padding\n",
    "    fold_audio, fold_audio_labels, fold_audio_names = load_audio_data_segmented(fold_dirs[i], cross_video_datasets[i], padding=padding)\n",
    "    #fold_audio_features = audio_features_extract(fold_audio, num_segments=num_segments)\n",
    "    #print(fold_audio_features.shape, fold_audio_labels.shape)\n",
    "    #data_audio_cross_pad_inter.append(fold_audio_features)\n",
    "    #labels_audio_cross_pad_inter.append(fold_audio_labels)\n",
    "    names_audio_cross_pad_inter.append(fold_audio_names)\n",
    "\n",
    "\"\"\"os.makedirs(\"/kaggle/working/inter_array\", exist_ok=True)\n",
    "\n",
    "# Save arrays for later use\n",
    "np.save(\"inter_array/train_features_audio.npy\", train_features_audio_holdout_inter)\n",
    "np.save(\"inter_array/val_features_audio.npy\", val_features_audio_holdout_inter)\n",
    "np.save(\"inter_array/test_features_audio.npy\", test_features_audio_holdout_inter)\n",
    "np.save(\"inter_array/train_labels_audio.npy\", train_labels_audio_holdout_inter)\n",
    "np.save(\"inter_array/val_labels_audio.npy\", val_labels_audio_holdout_inter)\n",
    "np.save(\"inter_array/test_labels_audio.npy\", test_labels_audio_holdout_inter)\n",
    "for i in range(4):\n",
    "    np.save(f\"inter_array/fold_{i}_features_audio.npy\", data_audio_cross_inter[i])\n",
    "    np.save(f\"inter_array/fold_{i}_labels_audio.npy\", labels_audio_cross_inter[i])\n",
    "    np.save(f\"inter_array/fold_{i}_features_audio_pad.npy\", data_audio_cross_pad_inter[i])\n",
    "    np.save(f\"inter_array/fold_{i}_labels_audio_pad.npy\", labels_audio_cross_pad_inter[i])\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Path to the dataset folder\n",
    "dataset_dir = \"/kaggle/working/inter_array\"\n",
    " \n",
    "# Path to save the zip file in the Kaggle output directory\n",
    "output_zip = f\"/kaggle/working/truthlie_audio_original_{sequence_length}_frame_{num_segments}_30.zip\"\n",
    " \n",
    "# Create the zip file\n",
    "shutil.make_archive(output_zip.replace(\".zip\", \"\"), 'zip', dataset_dir)\n",
    " \n",
    "print(f\"The dataset has been compressed in {output_zip}. You can download it from Kaggle's output.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:28:02.696030Z",
     "iopub.status.busy": "2025-04-08T18:28:02.695754Z",
     "iopub.status.idle": "2025-04-08T18:28:05.567926Z",
     "shell.execute_reply": "2025-04-08T18:28:05.567032Z",
     "shell.execute_reply.started": "2025-04-08T18:28:02.696004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 64\n",
    "num_segments = 20\n",
    "\n",
    "features_audio_input_path = f\"/kaggle/input/truthlie-audio-features/truthlie_audio_original_{sequence_length}_frame_{num_segments}_30\"\n",
    "\n",
    "# Load previously saved arrays\n",
    "train_features_audio_holdout_inter = np.load(features_audio_input_path + \"/train_features_audio.npy\")\n",
    "val_features_audio_holdout_inter = np.load(features_audio_input_path + \"/val_features_audio.npy\")\n",
    "test_features_audio_holdout_inter = np.load(features_audio_input_path + \"/test_features_audio.npy\")\n",
    "train_labels_audio_holdout_inter = np.load(features_audio_input_path + \"/train_labels_audio.npy\")\n",
    "val_labels_audio_holdout_inter = np.load(features_audio_input_path + \"/val_labels_audio.npy\")\n",
    "test_labels_audio_holdout_inter = np.load(features_audio_input_path + \"/test_labels_audio.npy\")\n",
    "print(train_features_audio_holdout_inter.shape, train_labels_audio_holdout_inter.shape)\n",
    "print(val_features_audio_holdout_inter.shape, val_labels_audio_holdout_inter.shape)\n",
    "print(test_features_audio_holdout_inter.shape, test_labels_audio_holdout_inter.shape)\n",
    "\n",
    "data_audio_cross_inter, labels_audio_cross_inter = [], []\n",
    "data_audio_cross_pad_inter, labels_audio_cross_pad_inter = [], []\n",
    "for i in range(4):\n",
    "    data_audio_cross_inter.append(np.load(features_audio_input_path + f\"/fold_{i}_features_audio.npy\"))\n",
    "    labels_audio_cross_inter.append(np.load(features_audio_input_path + f\"/fold_{i}_labels_audio.npy\"))\n",
    "    print(data_audio_cross_inter[i].shape, labels_audio_cross_inter[i].shape)\n",
    "\n",
    "    data_audio_cross_pad_inter.append(np.load(features_audio_input_path + f\"/fold_{i}_features_audio_pad.npy\"))\n",
    "    labels_audio_cross_pad_inter.append(np.load(features_audio_input_path + f\"/fold_{i}_labels_audio_pad.npy\"))\n",
    "    print(data_audio_cross_pad_inter[i].shape, labels_audio_cross_pad_inter[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:28:05.569387Z",
     "iopub.status.busy": "2025-04-08T18:28:05.569135Z",
     "iopub.status.idle": "2025-04-08T18:28:05.715158Z",
     "shell.execute_reply": "2025-04-08T18:28:05.714217Z",
     "shell.execute_reply.started": "2025-04-08T18:28:05.569363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_new_audio_array_inter(features_list,labels_list,audios_list,video_list):\n",
    "    new_audio_dict_inter_features = {}\n",
    "    new_audio_dict_inter_labels = {}\n",
    "    \n",
    "    for feature,label,name in zip(features_list,labels_list,audios_list):\n",
    "        real_name = name[:-4]\n",
    "        if real_name not in new_audio_dict_inter_features.keys():\n",
    "            new_audio_dict_inter_features[real_name] = []\n",
    "            new_audio_dict_inter_labels[real_name] = []\n",
    "        new_audio_dict_inter_features[real_name].append(feature)\n",
    "        new_audio_dict_inter_labels[real_name].append(label)\n",
    "    \n",
    "    new_audio_test_features = []\n",
    "    new_audio_test_labels = []\n",
    "    for name in video_list:\n",
    "        real_name = name.split('/')[-1][:-4]\n",
    "        new_audio_test_features.extend(new_audio_dict_inter_features[real_name])\n",
    "        new_audio_test_labels.extend(new_audio_dict_inter_labels[real_name])\n",
    "    \n",
    "    return np.array(new_audio_test_features), np.array(new_audio_test_labels)\n",
    "\n",
    "train_features_audio_holdout_inter, train_labels_audio_holdout_inter = get_new_audio_array_inter(train_features_audio_holdout_inter,train_labels_audio_holdout_inter,train_audio_names_holdout_inter,new_train_list)\n",
    "print(train_features_audio_holdout_inter.shape, train_labels_audio_holdout_inter.shape)\n",
    "val_features_audio_holdout_inter, val_labels_audio_holdout_inter = get_new_audio_array_inter(val_features_audio_holdout_inter,val_labels_audio_holdout_inter,val_audio_names_holdout_inter,new_val_list)\n",
    "print(val_features_audio_holdout_inter.shape, val_labels_audio_holdout_inter.shape)\n",
    "test_features_audio_holdout_inter, test_labels_audio_holdout_inter = get_new_audio_array_inter(test_features_audio_holdout_inter,test_labels_audio_holdout_inter,test_audio_names_holdout_inter,new_test_list)\n",
    "print(test_features_audio_holdout_inter.shape, test_labels_audio_holdout_inter.shape)\n",
    "for i in range(4):\n",
    "    fold_features_audio_inter, fold_labels_audio_inter = get_new_audio_array_inter(data_audio_cross_inter[i],labels_audio_cross_inter[i],names_audio_cross_inter[i],new_cross_list[i])\n",
    "    print(fold_features_audio_inter.shape, fold_labels_audio_inter.shape)\n",
    "    data_audio_cross_inter[i]=fold_features_audio_inter\n",
    "    labels_audio_cross_inter[i]=fold_labels_audio_inter\n",
    "\n",
    "    fold_features_audio_inter, fold_labels_audio_inter = get_new_audio_array_inter(data_audio_cross_pad_inter[i],labels_audio_cross_pad_inter[i],names_audio_cross_pad_inter[i],new_cross_list_pad[i])\n",
    "    print(fold_features_audio_inter.shape, fold_labels_audio_inter.shape)\n",
    "    data_audio_cross_pad_inter[i]=fold_features_audio_inter\n",
    "    labels_audio_cross_pad_inter[i]=fold_labels_audio_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:28:05.716481Z",
     "iopub.status.busy": "2025-04-08T18:28:05.716198Z",
     "iopub.status.idle": "2025-04-08T18:28:05.723862Z",
     "shell.execute_reply": "2025-04-08T18:28:05.722976Z",
     "shell.execute_reply.started": "2025-04-08T18:28:05.716455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(all(train_labels_audio_holdout_inter == train_labels_video_holdout))\n",
    "print(all(val_labels_audio_holdout_inter == val_labels_video_holdout))\n",
    "print(all(test_labels_audio_holdout_inter == test_labels_video_holdout))\n",
    "\n",
    "for i in range(4):\n",
    "    print(all(labels_audio_cross_inter[i] == labels_video_cross[i]))\n",
    "    print(all(labels_audio_cross_pad_inter[i] == labels_video_cross_pad[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:28:05.725234Z",
     "iopub.status.busy": "2025-04-08T18:28:05.724925Z",
     "iopub.status.idle": "2025-04-08T18:28:05.736528Z",
     "shell.execute_reply": "2025-04-08T18:28:05.735718Z",
     "shell.execute_reply.started": "2025-04-08T18:28:05.725210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_combined_model(video_feature_size, max_seq_len_audio, audio_feature_size):\n",
    "    # Video model\n",
    "    shape = video_feature_size[2]\n",
    "    video_input = Input(shape=video_feature_size, name=\"Video_Input\")\n",
    "    video_conv3D = Conv3D(filters=32, kernel_size=(3, 3, shape), activation='relu', name=\"Video_Conv3D\")(video_input)\n",
    "    video_batchnorm = BatchNormalization(name=\"Video_Batchnorm\")(video_conv3D)\n",
    "    video_pooling = MaxPooling3D(pool_size=(2, shape, 1), name=\"Video_Pooling\")(video_batchnorm)\n",
    "    video_flatten = Flatten(name=\"Video_Flatten\")(video_pooling)\n",
    "     \n",
    "    # Audio model\n",
    "    audio_input = Input(shape=(max_seq_len_audio, audio_feature_size), name=\"Audio_Input\")\n",
    "    audio_lstm = LSTM(128, return_sequences=False, name=\"Audio_LSTM\")(audio_input)\n",
    "     \n",
    "    # Concatenation and classification\n",
    "    combined = Concatenate(name=\"Concatenate\")([video_flatten, audio_lstm])\n",
    "    combined_bn = BatchNormalization(name=\"BatchNormalization\")(combined)\n",
    "    final_dense = Dense(32, activation=\"relu\", name=\"Final_Dense\")(combined_bn)\n",
    "    final_dropout = Dropout(0.3, name=\"Final_Dropout\")(final_dense)\n",
    "    output = Dense(1, activation=\"sigmoid\", name=\"Output\")(final_dropout)\n",
    "     \n",
    "    # Final model\n",
    "    model = Model(inputs=[video_input, audio_input], outputs=output)\n",
    "     \n",
    "    # Model compilation\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:28:05.737948Z",
     "iopub.status.busy": "2025-04-08T18:28:05.737644Z",
     "iopub.status.idle": "2025-04-08T18:28:19.698317Z",
     "shell.execute_reply": "2025-04-08T18:28:19.697430Z",
     "shell.execute_reply.started": "2025-04-08T18:28:05.737924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model creation\n",
    "combined_model_holdout = create_combined_model(train_features_video_holdout.shape[1:],\n",
    "                                       train_features_audio_holdout_inter.shape[1],\n",
    "                                       train_features_audio_holdout_inter.shape[2]\n",
    "                                      )\n",
    "\n",
    "# Model training\n",
    "combined_history_holdout = combined_model_holdout.fit(\n",
    "    [train_features_video_holdout, train_features_audio_holdout_inter], train_labels_video_holdout,\n",
    "    validation_data=([val_features_video_holdout, val_features_audio_holdout_inter], val_labels_video_holdout),\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    shuffle=True\n",
    ")\n",
    "plot_history(combined_history_holdout)\n",
    "\n",
    "# Model evaluation\n",
    "test_predictions_holdout_proba_common = combined_model_holdout.predict([test_features_video_holdout, test_features_audio_holdout_inter])\n",
    "test_predictions_holdout_common = (test_predictions_holdout_proba_common > 0.5).astype(int)\n",
    " \n",
    "# Metric calculation\n",
    "accuracy_holdout_common = accuracy_score(test_labels_video_holdout, test_predictions_holdout_common)\n",
    "precision_holdout_common = precision_score(test_labels_video_holdout, test_predictions_holdout_common, zero_division=0, average=None)\n",
    "recall_holdout_common = recall_score(test_labels_video_holdout, test_predictions_holdout_common, zero_division=0, average=None)\n",
    "f1_holdout_common = f1_score(test_labels_video_holdout, test_predictions_holdout_common, average=None)\n",
    "auc_holdout_common = roc_auc_score(test_labels_video_holdout, test_predictions_holdout_proba_common)\n",
    "\n",
    "# Displaying results\n",
    "print(\"***** FRAME LEVEL RESULTS *****\\n\")\n",
    "conf_matrix_holdout_common = confusion_matrix(test_labels_video_holdout, test_predictions_holdout_common)\n",
    "plot_confusion_matrix(conf_matrix_holdout_common)\n",
    " \n",
    "# Saving metrics in the dictionary\n",
    "frame_metrics_holdout[\"Accuracy\"].append(accuracy_holdout_common)\n",
    "frame_metrics_holdout[\"Precision (0)\"].append(precision_holdout_common[0])\n",
    "frame_metrics_holdout[\"Precision (1)\"].append(precision_holdout_common[1])\n",
    "frame_metrics_holdout[\"Recall (0)\"].append(recall_holdout_common[0])\n",
    "frame_metrics_holdout[\"Recall (1)\"].append(recall_holdout_common[1])\n",
    "frame_metrics_holdout[\"F1 (0)\"].append(f1_holdout_common[0])\n",
    "frame_metrics_holdout[\"F1 (1)\"].append(f1_holdout_common[1])\n",
    "frame_metrics_holdout[\"AUC\"].append(auc_holdout_common)\n",
    "\n",
    "# Create a dataframe with predictions and video ids\n",
    "pred_df = pd.DataFrame({'video_id': test_video_names_holdout, 'prediction': test_predictions_holdout_proba_common.flatten(), 'label': test_labels_video_holdout})\n",
    " \n",
    "# Group by video and calculate the average\n",
    "video_predictions_mean = pred_df.groupby('video_id', sort=False)['prediction'].apply(calculate_mean).values\n",
    "video_predictions_binary_mean = (video_predictions_mean > 0.5).astype(int)\n",
    "\n",
    "# Group by video and apply the majority voting rule\n",
    "video_predictions_binary_majority = pred_df.groupby('video_id', sort=False)['prediction'].apply(calculate_majority).values\n",
    "\n",
    "# Group by video and apply the aggregation rule\n",
    "video_predictions_binary_threshold = pred_df.groupby('video_id', sort=False)['prediction'].apply(aggregate_by_threshold).values    \n",
    "\n",
    "# Group by video and get labels\n",
    "video_labels = pred_df.groupby('video_id', sort=False)['label'].first().values\n",
    "\n",
    "# Calculate video-level metrics\n",
    "video_accuracy = accuracy_score(video_labels, video_predictions_binary_mean)\n",
    "video_f1 = f1_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "video_precision = precision_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "video_recall = recall_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "video_auc = roc_auc_score(video_labels, video_predictions_mean)\n",
    "\n",
    "video_metrics_holdout_mean[\"Accuracy\"].append(video_accuracy)\n",
    "video_metrics_holdout_mean[\"Precision (0)\"].append(video_precision[0])\n",
    "video_metrics_holdout_mean[\"Precision (1)\"].append(video_precision[1])\n",
    "video_metrics_holdout_mean[\"Recall (0)\"].append(video_recall[0])\n",
    "video_metrics_holdout_mean[\"Recall (1)\"].append(video_recall[1])\n",
    "video_metrics_holdout_mean[\"F1 (0)\"].append(video_f1[0])\n",
    "video_metrics_holdout_mean[\"F1 (1)\"].append(video_f1[1])\n",
    "video_metrics_holdout_mean[\"AUC\"].append(video_auc)\n",
    "\n",
    "print(\"\\n***** VIDEO LEVEL RESULTS (MEAN) *****\\n\")\n",
    "plot_confusion_matrix_2(video_labels, video_predictions_binary_mean)\n",
    "\n",
    "# Calculate video-level metrics\n",
    "video_accuracy = accuracy_score(video_labels, video_predictions_binary_majority)\n",
    "video_f1 = f1_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "video_precision = precision_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "video_recall = recall_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "video_auc = roc_auc_score(video_labels, video_predictions_binary_majority)\n",
    "\n",
    "video_metrics_holdout_majority[\"Accuracy\"].append(video_accuracy)\n",
    "video_metrics_holdout_majority[\"Precision (0)\"].append(video_precision[0])\n",
    "video_metrics_holdout_majority[\"Precision (1)\"].append(video_precision[1])\n",
    "video_metrics_holdout_majority[\"Recall (0)\"].append(video_recall[0])\n",
    "video_metrics_holdout_majority[\"Recall (1)\"].append(video_recall[1])\n",
    "video_metrics_holdout_majority[\"F1 (0)\"].append(video_f1[0])\n",
    "video_metrics_holdout_majority[\"F1 (1)\"].append(video_f1[1])\n",
    "video_metrics_holdout_majority[\"AUC\"].append(video_auc)\n",
    "\n",
    "print(\"\\n***** VIDEO LEVEL RESULTS (MAJORITY) *****\\n\")\n",
    "plot_confusion_matrix_2(video_labels, video_predictions_binary_majority)\n",
    "\n",
    "# Calculate video-level metrics\n",
    "video_accuracy = accuracy_score(video_labels, video_predictions_binary_threshold)\n",
    "video_f1 = f1_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "video_precision = precision_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "video_recall = recall_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "video_auc = roc_auc_score(video_labels, video_predictions_binary_threshold)\n",
    "\n",
    "video_metrics_holdout_threshold[\"Accuracy\"].append(video_accuracy)\n",
    "video_metrics_holdout_threshold[\"Precision (0)\"].append(video_precision[0])\n",
    "video_metrics_holdout_threshold[\"Precision (1)\"].append(video_precision[1])\n",
    "video_metrics_holdout_threshold[\"Recall (0)\"].append(video_recall[0])\n",
    "video_metrics_holdout_threshold[\"Recall (1)\"].append(video_recall[1])\n",
    "video_metrics_holdout_threshold[\"F1 (0)\"].append(video_f1[0])\n",
    "video_metrics_holdout_threshold[\"F1 (1)\"].append(video_f1[1])\n",
    "video_metrics_holdout_threshold[\"AUC\"].append(video_auc)\n",
    "\n",
    "print(\"\\n***** VIDEO LEVEL RESULTS (THRESHOLD) *****\\n\")\n",
    "plot_confusion_matrix_2(video_labels, video_predictions_binary_threshold)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame.from_dict(frame_metrics_holdout, orient='index', columns=[f\"INTER FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** FRAME LEVEL RESULTS *****\\n\")\n",
    "print(df)\n",
    "frame_metrics_holdout = {key: [] for key in frame_metrics_holdout}\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame.from_dict(video_metrics_holdout_mean, orient='index', columns=[f\"INTER FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (MEAN) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_holdout_mean = {key: [] for key in video_metrics_holdout_mean}\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame.from_dict(video_metrics_holdout_majority, orient='index', columns=[f\"INTER FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (MAJORITY) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_holdout_majority = {key: [] for key in video_metrics_holdout_majority}\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame.from_dict(video_metrics_holdout_threshold, orient='index', columns=[f\"INTER FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (THRESHOLD) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_holdout_threshold = {key: [] for key in video_metrics_holdout_threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:28:19.699842Z",
     "iopub.status.busy": "2025-04-08T18:28:19.699547Z",
     "iopub.status.idle": "2025-04-08T18:28:19.734889Z",
     "shell.execute_reply": "2025-04-08T18:28:19.733992Z",
     "shell.execute_reply.started": "2025-04-08T18:28:19.699815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_video_audio_combined(frame_metrics_dict, video_metrics_dict_mean,\n",
    "                                          video_metrics_dict_majority, video_metrics_dict_threshold,\n",
    "                                          data_video, data_video_pad, data_audio, data_audio_pad,\n",
    "                                          labels, labels_pad, names_video, names_video_pad,\n",
    "                                          num_folds=4):\n",
    "    # Performance List Videos\n",
    "    frame_accuracy_score = [] \n",
    "    frame_f1_score_0 = [] \n",
    "    frame_precision_score_0 = [] \n",
    "    frame_recall_score_0 = []\n",
    "    frame_f1_score_1 = [] \n",
    "    frame_precision_score_1 = [] \n",
    "    frame_recall_score_1 = []\n",
    "    frame_auc_score = []\n",
    "    \n",
    "    # Performance List Videos\n",
    "    video_accuracy_score_mean = [] \n",
    "    video_f1_score_mean_0 = [] \n",
    "    video_precision_score_mean_0 = [] \n",
    "    video_recall_score_mean_0 = []\n",
    "    video_f1_score_mean_1 = [] \n",
    "    video_precision_score_mean_1 = [] \n",
    "    video_recall_score_mean_1 = []\n",
    "    video_auc_score_mean = []\n",
    "\n",
    "    # Performance List Videos\n",
    "    video_accuracy_score_majority = [] \n",
    "    video_f1_score_majority_0 = [] \n",
    "    video_precision_score_majority_0 = [] \n",
    "    video_recall_score_majority_0 = []\n",
    "    video_f1_score_majority_1 = [] \n",
    "    video_precision_score_majority_1 = [] \n",
    "    video_recall_score_majority_1 = []\n",
    "    video_auc_score_majority = []\n",
    "\n",
    "    # Performance List Videos\n",
    "    video_accuracy_score_threshold = [] \n",
    "    video_f1_score_threshold_0 = [] \n",
    "    video_precision_score_threshold_0 = [] \n",
    "    video_recall_score_threshold_0 = []\n",
    "    video_f1_score_threshold_1 = [] \n",
    "    video_precision_score_threshold_1 = [] \n",
    "    video_recall_score_threshold_1 = []\n",
    "    video_auc_score_threshold = []\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        print(f'\\nFold {i+1}/{num_folds}')\n",
    "        \n",
    "        # Fold splitting in train and test\n",
    "        test_video, test_audio, test_labels, test_video_names = data_video_pad[i], data_audio_pad[i], labels_pad[i], names_video_pad[i]\n",
    "        train_video = np.array([item for idx, fold in enumerate(data_video) if idx != i for item in fold])\n",
    "        train_audio = np.array([item for idx, fold in enumerate(data_audio) if idx != i for item in fold])\n",
    "        train_labels = np.array([label for idx, fold in enumerate(labels) if idx != i for label in fold])\n",
    "        train_video_names = np.array([item for idx, fold in enumerate(names_video) if idx != i for item in fold])\n",
    "        \n",
    "        print(train_video.shape, train_audio.shape, train_labels.shape)\n",
    "        print(test_video.shape, test_audio.shape, test_labels.shape)\n",
    "    \n",
    "        # Model creation\n",
    "        combined_model_holdout = create_combined_model(train_video.shape[1:],\n",
    "                                               train_audio.shape[1],\n",
    "                                               train_audio.shape[2]\n",
    "                                              )\n",
    "        \n",
    "        # Model training\n",
    "        combined_history_holdout = combined_model_holdout.fit(\n",
    "            [train_video, train_audio], train_labels,\n",
    "            epochs=20,\n",
    "            batch_size=256,\n",
    "            shuffle=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fold evaluation\n",
    "        predictions_proba = combined_model_holdout.predict([test_video, test_audio])\n",
    "        predictions = (predictions_proba > 0.5).astype(int)  \n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        precision = precision_score(test_labels, predictions, zero_division=0, average=None)\n",
    "        recall = recall_score(test_labels, predictions, zero_division=0, average=None)\n",
    "        f1 = f1_score(test_labels, predictions, average=None)\n",
    "        auc = roc_auc_score(test_labels, predictions_proba)\n",
    "        \n",
    "        # Saving metrics\n",
    "        frame_accuracy_score.append(accuracy)\n",
    "        frame_precision_score_0.append(precision[0])\n",
    "        frame_recall_score_0.append(recall[0])\n",
    "        frame_f1_score_0.append(f1[0])\n",
    "        frame_precision_score_1.append(precision[1])\n",
    "        frame_recall_score_1.append(recall[1])\n",
    "        frame_f1_score_1.append(f1[1])\n",
    "        frame_auc_score.append(auc)\n",
    "\n",
    "        # Create a dataframe with predictions and video ids\n",
    "        pred_df = pd.DataFrame({'video_id': test_video_names, 'prediction': predictions_proba.flatten(), 'label': test_labels})\n",
    "\n",
    "        # Group by video and calculate the average\n",
    "        video_predictions_mean = pred_df.groupby('video_id')['prediction'].apply(calculate_mean).values\n",
    "        video_predictions_binary_mean = (video_predictions_mean > 0.5).astype(int)\n",
    "        \n",
    "        # Group by video and apply the majority voting rule\n",
    "        video_predictions_binary_majority = pred_df.groupby('video_id')['prediction'].apply(calculate_majority).values\n",
    "    \n",
    "        # Group by video and apply the aggregation rule\n",
    "        video_predictions_binary_threshold = pred_df.groupby('video_id')['prediction'].apply(aggregate_by_threshold).values    \n",
    "        \n",
    "        # Group by video and get labels\n",
    "        video_labels = pred_df.groupby('video_id')['label'].first().values\n",
    "\n",
    "        # Calculate metrics for videos\n",
    "        mean_video_accuracy = accuracy_score(video_labels, video_predictions_binary_mean)\n",
    "        mean_video_f1 = f1_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "        mean_video_precision = precision_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "        mean_video_recall = recall_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "        mean_video_auc = roc_auc_score(video_labels, video_predictions_mean)\n",
    "\n",
    "        majority_video_accuracy = accuracy_score(video_labels, video_predictions_binary_majority)\n",
    "        majority_video_f1 = f1_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "        majority_video_precision = precision_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "        majority_video_recall = recall_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "        majority_video_auc = roc_auc_score(video_labels, video_predictions_binary_majority)\n",
    "\n",
    "        threshold_video_accuracy = accuracy_score(video_labels, video_predictions_binary_threshold)\n",
    "        threshold_video_f1 = f1_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "        threshold_video_precision = precision_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "        threshold_video_recall = recall_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "        threshold_video_auc = roc_auc_score(video_labels, video_predictions_binary_threshold)\n",
    "\n",
    "        # Saving metrics\n",
    "        video_accuracy_score_mean.append(mean_video_accuracy)\n",
    "        video_f1_score_mean_0.append(mean_video_f1[0])\n",
    "        video_precision_score_mean_0.append(mean_video_precision[0])\n",
    "        video_recall_score_mean_0.append(mean_video_recall[0])\n",
    "        video_f1_score_mean_1.append(mean_video_f1[1])\n",
    "        video_precision_score_mean_1.append(mean_video_precision[1])\n",
    "        video_recall_score_mean_1.append(mean_video_recall[1])\n",
    "        video_auc_score_mean.append(mean_video_auc)\n",
    "\n",
    "        video_accuracy_score_majority.append(majority_video_accuracy)\n",
    "        video_f1_score_majority_0.append(majority_video_f1[0])\n",
    "        video_precision_score_majority_0.append(majority_video_precision[0])\n",
    "        video_recall_score_majority_0.append(majority_video_recall[0])\n",
    "        video_f1_score_majority_1.append(majority_video_f1[1])\n",
    "        video_precision_score_majority_1.append(majority_video_precision[1])\n",
    "        video_recall_score_majority_1.append(majority_video_recall[1])\n",
    "        video_auc_score_majority.append(majority_video_auc)\n",
    "\n",
    "        video_accuracy_score_threshold.append(threshold_video_accuracy)\n",
    "        video_f1_score_threshold_0.append(threshold_video_f1[0])\n",
    "        video_precision_score_threshold_0.append(threshold_video_precision[0])\n",
    "        video_recall_score_threshold_0.append(threshold_video_recall[0])\n",
    "        video_f1_score_threshold_1.append(threshold_video_f1[1])\n",
    "        video_precision_score_threshold_1.append(threshold_video_precision[1])\n",
    "        video_recall_score_threshold_1.append(threshold_video_recall[1])\n",
    "        video_auc_score_threshold.append(threshold_video_auc)\n",
    "\n",
    "        print(f\"Accuracy fold (frame-based): {accuracy}\")\n",
    "        print(f\"Accuracy fold (video-based-mean): {mean_video_accuracy}\")\n",
    "        print(f\"Accuracy fold (video-based-majority): {majority_video_accuracy}\")\n",
    "        print(f\"Accuracy fold (video-based-threshold): {threshold_video_accuracy}\")\n",
    "    \n",
    "    # Metrics average on all frames\n",
    "    frame_avg_accuracy = np.mean(frame_accuracy_score)\n",
    "    frame_avg_precision_0, frame_avg_recall_0, frame_avg_f1_0 = np.mean(frame_precision_score_0), np.mean(frame_recall_score_0), np.mean(frame_f1_score_0)\n",
    "    frame_avg_precision_1, frame_avg_recall_1, frame_avg_f1_1 = np.mean(frame_precision_score_1), np.mean(frame_recall_score_1), np.mean(frame_f1_score_1)\n",
    "    frame_avg_auc = np.mean(frame_auc_score)\n",
    "\n",
    "    frame_metrics_dict[\"Mean Accuracy\"].append(frame_avg_accuracy)\n",
    "    frame_metrics_dict[\"Mean Precision (0)\"].append(frame_avg_precision_0)\n",
    "    frame_metrics_dict[\"Mean Precision (1)\"].append(frame_avg_precision_1)\n",
    "    frame_metrics_dict[\"Mean Recall (0)\"].append(frame_avg_recall_0)\n",
    "    frame_metrics_dict[\"Mean Recall (1)\"].append(frame_avg_recall_1)\n",
    "    frame_metrics_dict[\"Mean F1 (0)\"].append(frame_avg_f1_0)\n",
    "    frame_metrics_dict[\"Mean F1 (1)\"].append(frame_avg_f1_1)\n",
    "    frame_metrics_dict[\"Mean AUC\"].append(frame_avg_auc)\n",
    "\n",
    "    # Metrics standard deviation on all frames\n",
    "    frame_std_accuracy = np.std(frame_accuracy_score)\n",
    "    frame_std_precision_0, frame_std_recall_0, frame_std_f1_0 = np.std(frame_precision_score_0), np.std(frame_recall_score_0), np.std(frame_f1_score_0)\n",
    "    frame_std_precision_1, frame_std_recall_1, frame_std_f1_1 = np.std(frame_precision_score_1), np.std(frame_recall_score_1), np.std(frame_f1_score_1)\n",
    "    frame_std_auc = np.std(frame_auc_score)\n",
    "\n",
    "    frame_metrics_dict[\"Std Accuracy\"].append(frame_std_accuracy)\n",
    "    frame_metrics_dict[\"Std Precision (0)\"].append(frame_std_precision_0)\n",
    "    frame_metrics_dict[\"Std Precision (1)\"].append(frame_std_precision_1)\n",
    "    frame_metrics_dict[\"Std Recall (0)\"].append(frame_std_recall_0)\n",
    "    frame_metrics_dict[\"Std Recall (1)\"].append(frame_std_recall_1)\n",
    "    frame_metrics_dict[\"Std F1 (0)\"].append(frame_std_f1_0)\n",
    "    frame_metrics_dict[\"Std F1 (1)\"].append(frame_std_f1_1)\n",
    "    frame_metrics_dict[\"Std AUC\"].append(frame_std_auc)\n",
    "        \n",
    "    print(\"\\n\\nRESULTS on FRAMES:\\n\")\n",
    "\n",
    "    print(\"Mean Accuracy:\", frame_avg_accuracy)\n",
    "    print(\"Accuracy std:\", frame_std_accuracy)\n",
    "\n",
    "    # Metrics average on all videos\n",
    "    video_avg_accuracy = np.mean(video_accuracy_score_mean)\n",
    "    video_avg_precision_0, video_avg_recall_0, video_avg_f1_0 = np.mean(video_precision_score_mean_0), np.mean(video_recall_score_mean_0), np.mean(video_f1_score_mean_0)\n",
    "    video_avg_precision_1, video_avg_recall_1, video_avg_f1_1 = np.mean(video_precision_score_mean_1), np.mean(video_recall_score_mean_1), np.mean(video_f1_score_mean_1)\n",
    "    video_avg_auc = np.mean(video_auc_score_mean)\n",
    "\n",
    "    video_metrics_dict_mean[\"Mean Accuracy\"].append(video_avg_accuracy)\n",
    "    video_metrics_dict_mean[\"Mean Precision (0)\"].append(video_avg_precision_0)\n",
    "    video_metrics_dict_mean[\"Mean Precision (1)\"].append(video_avg_precision_1)\n",
    "    video_metrics_dict_mean[\"Mean Recall (0)\"].append(video_avg_recall_0)\n",
    "    video_metrics_dict_mean[\"Mean Recall (1)\"].append(video_avg_recall_1)\n",
    "    video_metrics_dict_mean[\"Mean F1 (0)\"].append(video_avg_f1_0)\n",
    "    video_metrics_dict_mean[\"Mean F1 (1)\"].append(video_avg_f1_1)\n",
    "    video_metrics_dict_mean[\"Mean AUC\"].append(video_avg_auc)\n",
    "\n",
    "    # Metrics standard deviation on all videos\n",
    "    video_std_accuracy = np.std(video_accuracy_score_mean)\n",
    "    video_std_precision_0, video_std_recall_0, video_std_f1_0 = np.std(video_precision_score_mean_0), np.std(video_recall_score_mean_0), np.std(video_f1_score_mean_0)\n",
    "    video_std_precision_1, video_std_recall_1, video_std_f1_1 = np.std(video_precision_score_mean_1), np.std(video_recall_score_mean_1), np.std(video_f1_score_mean_1)\n",
    "    video_std_auc = np.std(video_auc_score_mean)\n",
    "\n",
    "    video_metrics_dict_mean[\"Std Accuracy\"].append(video_std_accuracy)\n",
    "    video_metrics_dict_mean[\"Std Precision (0)\"].append(video_std_precision_0)\n",
    "    video_metrics_dict_mean[\"Std Precision (1)\"].append(video_std_precision_1)\n",
    "    video_metrics_dict_mean[\"Std Recall (0)\"].append(video_std_recall_0)\n",
    "    video_metrics_dict_mean[\"Std Recall (1)\"].append(video_std_recall_1)\n",
    "    video_metrics_dict_mean[\"Std F1 (0)\"].append(video_std_f1_0)\n",
    "    video_metrics_dict_mean[\"Std F1 (1)\"].append(video_std_f1_1)\n",
    "    video_metrics_dict_mean[\"Std AUC\"].append(video_std_auc)\n",
    "    \n",
    "    print(\"\\n\\nRESULTS on VIDEOS (MEAN):\\n\")\n",
    "    \n",
    "    print(\"Mean Accuracy:\", video_avg_accuracy)\n",
    "    print(\"Accuracy std:\", video_std_accuracy)\n",
    "\n",
    "    # Metrics average on all videos\n",
    "    video_avg_accuracy = np.mean(video_accuracy_score_majority)\n",
    "    video_avg_precision_0, video_avg_recall_0, video_avg_f1_0 = np.mean(video_precision_score_majority_0), np.mean(video_recall_score_majority_0), np.mean(video_f1_score_majority_0)\n",
    "    video_avg_precision_1, video_avg_recall_1, video_avg_f1_1 = np.mean(video_precision_score_majority_1), np.mean(video_recall_score_majority_1), np.mean(video_f1_score_majority_1)\n",
    "    video_avg_auc = np.mean(video_auc_score_majority)\n",
    "\n",
    "    video_metrics_dict_majority[\"Mean Accuracy\"].append(video_avg_accuracy)\n",
    "    video_metrics_dict_majority[\"Mean Precision (0)\"].append(video_avg_precision_0)\n",
    "    video_metrics_dict_majority[\"Mean Precision (1)\"].append(video_avg_precision_1)\n",
    "    video_metrics_dict_majority[\"Mean Recall (0)\"].append(video_avg_recall_0)\n",
    "    video_metrics_dict_majority[\"Mean Recall (1)\"].append(video_avg_recall_1)\n",
    "    video_metrics_dict_majority[\"Mean F1 (0)\"].append(video_avg_f1_0)\n",
    "    video_metrics_dict_majority[\"Mean F1 (1)\"].append(video_avg_f1_1)\n",
    "    video_metrics_dict_majority[\"Mean AUC\"].append(video_avg_auc)\n",
    "\n",
    "    # Metrics standard deviation on all videos\n",
    "    video_std_accuracy = np.std(video_accuracy_score_majority)\n",
    "    video_std_precision_0, video_std_recall_0, video_std_f1_0 = np.std(video_precision_score_majority_0), np.std(video_recall_score_majority_0), np.std(video_f1_score_majority_0)\n",
    "    video_std_precision_1, video_std_recall_1, video_std_f1_1 = np.std(video_precision_score_majority_1), np.std(video_recall_score_majority_1), np.std(video_f1_score_majority_1)\n",
    "    video_std_auc = np.std(video_auc_score_majority)\n",
    "\n",
    "    video_metrics_dict_majority[\"Std Accuracy\"].append(video_std_accuracy)\n",
    "    video_metrics_dict_majority[\"Std Precision (0)\"].append(video_std_precision_0)\n",
    "    video_metrics_dict_majority[\"Std Precision (1)\"].append(video_std_precision_1)\n",
    "    video_metrics_dict_majority[\"Std Recall (0)\"].append(video_std_recall_0)\n",
    "    video_metrics_dict_majority[\"Std Recall (1)\"].append(video_std_recall_1)\n",
    "    video_metrics_dict_majority[\"Std F1 (0)\"].append(video_std_f1_0)\n",
    "    video_metrics_dict_majority[\"Std F1 (1)\"].append(video_std_f1_1)\n",
    "    video_metrics_dict_majority[\"Std AUC\"].append(video_std_auc)\n",
    "    \n",
    "    print(\"\\n\\nRESULTS on VIDEOS (MAJORITY):\\n\")\n",
    "    \n",
    "    print(\"Mean Accuracy:\", video_avg_accuracy)\n",
    "    print(\"Accuracy std:\", video_std_accuracy)\n",
    "\n",
    "    # Metrics average on all videos\n",
    "    video_avg_accuracy = np.mean(video_accuracy_score_threshold)\n",
    "    video_avg_precision_0, video_avg_recall_0, video_avg_f1_0 = np.mean(video_precision_score_threshold_0), np.mean(video_recall_score_threshold_0), np.mean(video_f1_score_threshold_0)\n",
    "    video_avg_precision_1, video_avg_recall_1, video_avg_f1_1 = np.mean(video_precision_score_threshold_1), np.mean(video_recall_score_threshold_1), np.mean(video_f1_score_threshold_1)\n",
    "    video_avg_auc = np.mean(video_auc_score_threshold)\n",
    "\n",
    "    video_metrics_dict_threshold[\"Mean Accuracy\"].append(video_avg_accuracy)\n",
    "    video_metrics_dict_threshold[\"Mean Precision (0)\"].append(video_avg_precision_0)\n",
    "    video_metrics_dict_threshold[\"Mean Precision (1)\"].append(video_avg_precision_1)\n",
    "    video_metrics_dict_threshold[\"Mean Recall (0)\"].append(video_avg_recall_0)\n",
    "    video_metrics_dict_threshold[\"Mean Recall (1)\"].append(video_avg_recall_1)\n",
    "    video_metrics_dict_threshold[\"Mean F1 (0)\"].append(video_avg_f1_0)\n",
    "    video_metrics_dict_threshold[\"Mean F1 (1)\"].append(video_avg_f1_1)\n",
    "    video_metrics_dict_threshold[\"Mean AUC\"].append(video_avg_auc)\n",
    "\n",
    "    # Metrics standard deviation on all videos\n",
    "    video_std_accuracy = np.std(video_accuracy_score_threshold)\n",
    "    video_std_precision_0, video_std_recall_0, video_std_f1_0 = np.std(video_precision_score_threshold_0), np.std(video_recall_score_threshold_0), np.std(video_f1_score_threshold_0)\n",
    "    video_std_precision_1, video_std_recall_1, video_std_f1_1 = np.std(video_precision_score_threshold_1), np.std(video_recall_score_threshold_1), np.std(video_f1_score_threshold_1)\n",
    "    video_std_auc = np.std(video_auc_score_threshold)\n",
    "\n",
    "    video_metrics_dict_threshold[\"Std Accuracy\"].append(video_std_accuracy)\n",
    "    video_metrics_dict_threshold[\"Std Precision (0)\"].append(video_std_precision_0)\n",
    "    video_metrics_dict_threshold[\"Std Precision (1)\"].append(video_std_precision_1)\n",
    "    video_metrics_dict_threshold[\"Std Recall (0)\"].append(video_std_recall_0)\n",
    "    video_metrics_dict_threshold[\"Std Recall (1)\"].append(video_std_recall_1)\n",
    "    video_metrics_dict_threshold[\"Std F1 (0)\"].append(video_std_f1_0)\n",
    "    video_metrics_dict_threshold[\"Std F1 (1)\"].append(video_std_f1_1)\n",
    "    video_metrics_dict_threshold[\"Std AUC\"].append(video_std_auc)\n",
    "    \n",
    "    print(\"\\n\\nRESULTS on VIDEOS (THRESHOLD):\\n\")\n",
    "    \n",
    "    print(\"Mean Accuracy:\", video_avg_accuracy)\n",
    "    print(\"Accuracy std:\", video_std_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:28:19.738938Z",
     "iopub.status.busy": "2025-04-08T18:28:19.738664Z",
     "iopub.status.idle": "2025-04-08T18:29:02.738083Z",
     "shell.execute_reply": "2025-04-08T18:29:02.737356Z",
     "shell.execute_reply.started": "2025-04-08T18:28:19.738913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cross_validation_video_audio_combined(frame_metrics_cross, video_metrics_cross_mean,\n",
    "                                          video_metrics_cross_majority, video_metrics_cross_threshold,\n",
    "                                         data_video_cross, data_video_cross_pad,\n",
    "                                         data_audio_cross_inter, data_audio_cross_pad_inter,\n",
    "                                         labels_video_cross, labels_video_cross_pad,\n",
    "                                         names_video_cross, names_video_cross_pad,\n",
    "                                         num_folds=4)\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(frame_metrics_cross, orient='index', columns=[f\"INTER FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** FRAME LEVEL RESULTS *****\\n\")\n",
    "print(df)\n",
    "frame_metrics_cross = {key: [] for key in frame_metrics_cross}\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(video_metrics_cross_mean, orient='index', columns=[f\"INTER FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (MEAN) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_cross_mean = {key: [] for key in video_metrics_cross_mean}\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(video_metrics_cross_majority, orient='index', columns=[f\"INTER FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (MAJORITY) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_cross_majority = {key: [] for key in video_metrics_cross_majority}\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(video_metrics_cross_threshold, orient='index', columns=[f\"INTER FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (THRESHOLD) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_cross_threshold = {key: [] for key in video_metrics_cross_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Fusion:\n",
    "We extract sequences of 64 or 20 frames from each video, for a total of 20(aux) + 7(emotions) = 27 features for each frame. Next, we extract the audio corresponding to that part of the video and organize it too into sequences of 64x43 or 20x43. Finally, we combine the two types of features into a vector of shape (num_seq,70) and train a unique model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:02.739448Z",
     "iopub.status.busy": "2025-04-08T18:29:02.739182Z",
     "iopub.status.idle": "2025-04-08T18:29:03.142519Z",
     "shell.execute_reply": "2025-04-08T18:29:03.141507Z",
     "shell.execute_reply.started": "2025-04-08T18:29:02.739422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 20 # or 64\n",
    "frame_step = 1\n",
    "cols_groups = [aus_cols_aux+aus_cols_emotions]\n",
    "padding = 'const'\n",
    "\n",
    "# Generate sequences for train, val and test\n",
    "seq_gen_train = list(list(gen_sequence(train_dataset[train_dataset['input'] == id], \n",
    "                                       sequence_length, cols_groups, frame_step, \n",
    "                                       padding=None))\n",
    "                     for id in train_list)\n",
    "\n",
    "seq_gen_val = list(list(gen_sequence(val_dataset[val_dataset['input'] == id], \n",
    "                                     sequence_length, cols_groups, frame_step, \n",
    "                                     padding))\n",
    "                   for id in val_list)\n",
    "\n",
    "seq_gen_test = list(list(gen_sequence(test_dataset[test_dataset['input'] == id], \n",
    "                                      sequence_length, cols_groups, frame_step, \n",
    "                                      padding))\n",
    "                    for id in test_list)\n",
    "\n",
    "# Remove empty lists\n",
    "seq_gen_train = [x for x in seq_gen_train if len(x) > 0]\n",
    "seq_gen_val = [x for x in seq_gen_val if len(x) > 0]\n",
    "seq_gen_test = [x for x in seq_gen_test if len(x) > 0]\n",
    "\n",
    "# Extract data from generators\n",
    "seq_array_train = [[t[0] for t in sublist] for sublist in seq_gen_train]\n",
    "label_array_train = [[t[1] for t in sublist] for sublist in seq_gen_train]\n",
    "video_array_train = [[t[2] for t in sublist] for sublist in seq_gen_train]\n",
    "\n",
    "seq_array_val = [[t[0] for t in sublist] for sublist in seq_gen_val]\n",
    "label_array_val = [[t[1] for t in sublist] for sublist in seq_gen_val]\n",
    "video_array_val = [[t[2] for t in sublist] for sublist in seq_gen_val]\n",
    "\n",
    "seq_array_test = [[t[0] for t in sublist] for sublist in seq_gen_test]\n",
    "label_array_test = [[t[1] for t in sublist] for sublist in seq_gen_test]\n",
    "video_array_test = [[t[2] for t in sublist] for sublist in seq_gen_test]\n",
    "\n",
    "# Transforms lists in arrays\n",
    "seq_array_train = np.concatenate(seq_array_train).astype(np.float32)\n",
    "train_labels_video_holdout_early = np.concatenate(label_array_train).astype(np.float32).reshape(-1)\n",
    "train_video_names_holdout_early = np.concatenate(video_array_train)\n",
    "\n",
    "seq_array_val = np.concatenate(seq_array_val).astype(np.float32)\n",
    "val_labels_video_holdout_early = np.concatenate(label_array_val).astype(np.float32).reshape(-1)\n",
    "val_video_names_holdout_early = np.concatenate(video_array_val)\n",
    "\n",
    "seq_array_test = np.concatenate(seq_array_test).astype(np.float32)\n",
    "test_labels_video_holdout_early = np.concatenate(label_array_test).astype(np.float32).reshape(-1)\n",
    "test_video_names_holdout_early = np.concatenate(video_array_test)\n",
    "\n",
    "# Transpose and expand arrays\n",
    "train_features_video_holdout_early = np.transpose(seq_array_train, (0, 1, 3, 2))\n",
    "train_features_video_holdout_early = np.squeeze(train_features_video_holdout_early, axis=-1)\n",
    "print(train_features_video_holdout_early.shape, train_labels_video_holdout_early.shape)\n",
    "\n",
    "val_features_video_holdout_early = np.transpose(seq_array_val, (0, 1, 3, 2))\n",
    "val_features_video_holdout_early = np.squeeze(val_features_video_holdout_early, axis=-1)\n",
    "print(val_features_video_holdout_early.shape, val_labels_video_holdout_early.shape)\n",
    "\n",
    "test_features_video_holdout_early = np.transpose(seq_array_test, (0, 1, 3, 2))\n",
    "test_features_video_holdout_early = np.squeeze(test_features_video_holdout_early, axis=-1)\n",
    "print(test_features_video_holdout_early.shape, test_labels_video_holdout_early.shape)\n",
    "\n",
    "new_train_list_early, indices = np.unique(train_video_names_holdout_early, return_index=True)\n",
    "new_train_list_early = new_train_list_early[np.argsort(indices)].tolist()\n",
    "new_val_list_early, indices = np.unique(val_video_names_holdout_early, return_index=True)\n",
    "new_val_list_early = new_val_list_early[np.argsort(indices)].tolist()\n",
    "new_test_list_early, indices = np.unique(test_video_names_holdout_early, return_index=True)\n",
    "new_test_list_early = new_test_list_early[np.argsort(indices)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:03.143810Z",
     "iopub.status.busy": "2025-04-08T18:29:03.143524Z",
     "iopub.status.idle": "2025-04-08T18:29:03.877940Z",
     "shell.execute_reply": "2025-04-08T18:29:03.877045Z",
     "shell.execute_reply.started": "2025-04-08T18:29:03.143784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_video_cross_early, labels_video_cross_early, names_video_cross_early = [],[],[]\n",
    "data_video_cross_pad_early, labels_video_cross_pad_early, names_video_cross_pad_early = [],[],[]\n",
    "new_cross_list_early, new_cross_list_pad_early = [],[]\n",
    "\n",
    "for i in range(4):\n",
    "    fold_dataset = cross_video_datasets[i]\n",
    "    fold_list = cross_video_lists[i]\n",
    "    \n",
    "    seq_gen_fold = list(list(gen_sequence(fold_dataset[fold_dataset['input'] == id], \n",
    "                                          sequence_length, cols_groups, frame_step, \n",
    "                                          padding=None))\n",
    "                 for id in fold_list)\n",
    "    seq_gen_fold = [x for x in seq_gen_fold if len(x)>0]\n",
    "\n",
    "    # Extract data from generators\n",
    "    seq_array_fold = [[t[0] for t in sublist] for sublist in seq_gen_fold]\n",
    "    label_array_fold = [[t[1] for t in sublist] for sublist in seq_gen_fold]\n",
    "    video_array_fold = [[t[2] for t in sublist] for sublist in seq_gen_fold]\n",
    "    \n",
    "    # Transform lists in arrays\n",
    "    fold_sequences = np.concatenate(seq_array_fold).astype(np.float32)\n",
    "    fold_labels = np.concatenate(label_array_fold).astype(np.float32).reshape(-1)\n",
    "    fold_video_names = np.concatenate(video_array_fold)\n",
    "\n",
    "    # Transpose and expand\n",
    "    fold_sequences = np.transpose(fold_sequences, (0, 1, 3, 2))\n",
    "    fold_sequences = np.squeeze(fold_sequences, axis=-1)\n",
    "    \n",
    "    print(fold_sequences.shape, fold_labels.shape)    \n",
    "    \n",
    "    data_video_cross_early.append(fold_sequences)\n",
    "    labels_video_cross_early.append(fold_labels)\n",
    "    names_video_cross_early.append(fold_video_names)\n",
    "\n",
    "    fold_video_names, indices = np.unique(fold_video_names, return_index=True)\n",
    "    fold_video_names = fold_video_names[np.argsort(indices)].tolist()\n",
    "    new_cross_list_early.append(fold_video_names)\n",
    "\n",
    "    # Padding\n",
    "    seq_gen_fold = list(list(gen_sequence(fold_dataset[fold_dataset['input'] == id], \n",
    "                                          sequence_length, cols_groups, frame_step, \n",
    "                                          padding))\n",
    "                 for id in fold_list)\n",
    "    seq_gen_fold = [x for x in seq_gen_fold if len(x)>0]\n",
    "\n",
    "    # Extract data from generators\n",
    "    seq_array_fold = [[t[0] for t in sublist] for sublist in seq_gen_fold]\n",
    "    label_array_fold = [[t[1] for t in sublist] for sublist in seq_gen_fold]\n",
    "    video_array_fold = [[t[2] for t in sublist] for sublist in seq_gen_fold]\n",
    "    \n",
    "    # Transforms lists in arrays\n",
    "    fold_sequences = np.concatenate(seq_array_fold).astype(np.float32)\n",
    "    fold_labels = np.concatenate(label_array_fold).astype(np.float32).reshape(-1)\n",
    "    fold_video_names = np.concatenate(video_array_fold)\n",
    "\n",
    "    # Transpose and expand\n",
    "    fold_sequences = np.transpose(fold_sequences, (0, 1, 3, 2))\n",
    "    fold_sequences = np.squeeze(fold_sequences, axis=-1)\n",
    "    \n",
    "    print(fold_sequences.shape, fold_labels.shape)    \n",
    "    \n",
    "    data_video_cross_pad_early.append(fold_sequences)\n",
    "    labels_video_cross_pad_early.append(fold_labels)\n",
    "    names_video_cross_pad_early.append(fold_video_names)\n",
    "\n",
    "    fold_video_names, indices = np.unique(fold_video_names, return_index=True)\n",
    "    fold_video_names = fold_video_names[np.argsort(indices)].tolist()\n",
    "    new_cross_list_pad_early.append(fold_video_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features Audio\n",
    "The commented parts were used to extract the new features, which were then downloaded so that the extraction process would not have to be repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:03.879251Z",
     "iopub.status.busy": "2025-04-08T18:29:03.878978Z",
     "iopub.status.idle": "2025-04-08T18:29:16.331856Z",
     "shell.execute_reply": "2025-04-08T18:29:16.331047Z",
     "shell.execute_reply.started": "2025-04-08T18:29:03.879223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_audio_data_segmented(audio_dataset_path, video_dataset, seq_len=sequence_length*frame_step, frame_step=frame_step, padding=None):\n",
    "    audios = []\n",
    "    audio_labels = []\n",
    "    audio_names_list = []\n",
    "\n",
    "    # Extract audio data\n",
    "    for root, dirs, files in os.walk(audio_dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.xlsx'):\n",
    "                \n",
    "                # Path of the Excel file\n",
    "                excel_path = os.path.join(root, file)\n",
    "                df = pd.read_excel(excel_path)\n",
    "                \n",
    "                # Extract audio names, video names, and labels\n",
    "                label_list = df['label'].tolist()\n",
    "                audio_names = df['audio name'].tolist()\n",
    "                video_names = df['video name'].tolist()\n",
    "                \n",
    "                # Segment the corresponding audio files\n",
    "                for label, audio_name, video_name in zip(label_list, audio_names, video_names):                    \n",
    "                    audio_path = os.path.join(audio_dataset_path, 'Audio', audio_name)\n",
    "                    video_path = os.path.join(audio_dataset_path, 'Statements', video_name)\n",
    "                    frame_dataset = video_dataset[video_dataset['input'] == video_path]\n",
    "                    total_frames = len(frame_dataset)\n",
    "                    \n",
    "                    if os.path.exists(audio_path):\n",
    "                        signal, sample_rate = librosa.load(audio_path, sr=None)  # Load audio with original sampling rate\n",
    "                        total_duration_ms = librosa.get_duration(y=signal, sr=sample_rate) * 1000  # Total audio duration in milliseconds\n",
    "                        frame_rate = total_frames / total_duration_ms\n",
    "                        seq_duration_ms = seq_len / frame_rate\n",
    "                        \n",
    "                        # Extract audio segments corresponding to video sequences\n",
    "                        segments = []\n",
    "                        start_time_ms = 0\n",
    "                        while np.round(start_time_ms + seq_duration_ms, 3) <= np.round(total_duration_ms, 3):\n",
    "                            start_sample = int((start_time_ms / 1000) * sample_rate)\n",
    "                            end_sample = int(((start_time_ms + seq_duration_ms) / 1000) * sample_rate)\n",
    "                            segments.append(signal[start_sample:end_sample])\n",
    "                            start_time_ms += frame_step / frame_rate\n",
    "                            \n",
    "                        if not segments and padding:\n",
    "                            segments.append(signal)\n",
    "\n",
    "                        for segment in segments:\n",
    "                            audios.append((segment, sample_rate))\n",
    "                            audio_labels.append(label)\n",
    "                            audio_names_list.append(audio_name)\n",
    "                    else:\n",
    "                        print(f\"Audio file not found: {audio_path}\")\n",
    "    \n",
    "    return audios, np.array(audio_labels), np.array(audio_names_list)\n",
    "\n",
    "# Load data from dataset\n",
    "train_audio_holdout_early, train_labels_audio_holdout_early, train_audio_names_holdout_early = load_audio_data_segmented(train_audio_path, train_dataset, sequence_length*frame_step, frame_step, padding=None)\n",
    "val_audio_holdout_early, val_labels_audio_holdout_early, val_audio_names_holdout_early = load_audio_data_segmented(val_audio_path, val_dataset, sequence_length*frame_step, frame_step, padding)\n",
    "test_audio_holdout_early, test_labels_audio_holdout_early, test_audio_names_holdout_early = load_audio_data_segmented(test_audio_path, test_dataset, sequence_length*frame_step, frame_step, padding)\n",
    "\n",
    "\"\"\"# Feature extraction for each audio\n",
    "train_features_audio_holdout_early = audio_features_extract(train_audio_holdout_early, num_segments=20)\n",
    "val_features_audio_holdout_early = audio_features_extract(val_audio_holdout_early, num_segments=20)\n",
    "test_features_audio_holdout_early = audio_features_extract(test_audio_holdout_early, num_segments=20)\n",
    "\n",
    "# Print dimensions\n",
    "print(train_features_audio_holdout_early.shape, train_labels_audio_holdout_early.shape)\n",
    "print(val_features_audio_holdout_early.shape, val_labels_audio_holdout_early.shape)\n",
    "print(test_features_audio_holdout_early.shape, test_labels_audio_holdout_early.shape)\"\"\"\n",
    "\n",
    "fold_dirs = [f\"{cross_audio_path}/fold_{index}\" for index in range(4)]\n",
    "data_audio_cross_early, labels_audio_cross_early, names_audio_cross_early = [], [], []\n",
    "data_audio_cross_pad_early, labels_audio_cross_pad_early, names_audio_cross_pad_early = [], [], []\n",
    " \n",
    "# Loading and feature extraction for each fold\n",
    "for i in range(4):\n",
    "    print(f\"Processing fold {i}...\")\n",
    "\n",
    "    # No padding\n",
    "    fold_audio, fold_audio_labels, fold_audio_names = load_audio_data_segmented(fold_dirs[i], cross_video_datasets[i], padding=None)\n",
    "    #fold_audio_features = audio_features_extract(fold_audio, num_segments=20)\n",
    "    #print(fold_audio_features.shape, fold_audio_labels.shape)\n",
    "    #data_audio_cross_early.append(fold_audio_features)\n",
    "    #labels_audio_cross_early.append(fold_audio_labels)\n",
    "    names_audio_cross_early.append(fold_audio_names)\n",
    "\n",
    "    # Padding\n",
    "    fold_audio, fold_audio_labels, fold_audio_names = load_audio_data_segmented(fold_dirs[i], cross_video_datasets[i], padding=padding)\n",
    "    #fold_audio_features = audio_features_extract(fold_audio, num_segments=20)\n",
    "    #print(fold_audio_features.shape, fold_audio_labels.shape)\n",
    "    #data_audio_cross_pad_early.append(fold_audio_features)\n",
    "    #labels_audio_cross_pad_early.append(fold_audio_labels)\n",
    "    names_audio_cross_pad_early.append(fold_audio_names)\n",
    "\n",
    "\"\"\"os.makedirs(\"/kaggle/working/early_array\", exist_ok=True)\n",
    "\n",
    "# Save the arrays for later use\n",
    "np.save(\"early_array/train_features_audio.npy\", train_features_audio_holdout_early)\n",
    "np.save(\"early_array/val_features_audio.npy\", val_features_audio_holdout_early)\n",
    "np.save(\"early_array/test_features_audio.npy\", test_features_audio_holdout_early)\n",
    "np.save(\"early_array/train_labels_audio.npy\", train_labels_audio_holdout_early)\n",
    "np.save(\"early_array/val_labels_audio.npy\", val_labels_audio_holdout_early)\n",
    "np.save(\"early_array/test_labels_audio.npy\", test_labels_audio_holdout_early)\n",
    "for i in range(4):\n",
    "    np.save(f\"early_array/fold_{i}_features_audio.npy\", data_audio_cross_early[i])\n",
    "    np.save(f\"early_array/fold_{i}_labels_audio.npy\", labels_audio_cross_early[i])\n",
    "    np.save(f\"early_array/fold_{i}_features_audio_pad.npy\", data_audio_cross_pad_early[i])\n",
    "    np.save(f\"early_array/fold_{i}_labels_audio_pad.npy\", labels_audio_cross_pad_early[i])\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Path to the dataset folder\n",
    "dataset_dir = \"/kaggle/working/early_array\"\n",
    " \n",
    "# Path to save the zip file in the Kaggle output directory\n",
    "output_zip = \"/kaggle/working/early_array.zip\"\n",
    " \n",
    "# Create the zip file\n",
    "shutil.make_archive(output_zip.replace(\".zip\", \"\"), 'zip', dataset_dir)\n",
    " \n",
    "print(f\"The dataset has been compressed into {output_zip}. You can download it from the Kaggle output.\")#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:16.333116Z",
     "iopub.status.busy": "2025-04-08T18:29:16.332874Z",
     "iopub.status.idle": "2025-04-08T18:29:20.073152Z",
     "shell.execute_reply": "2025-04-08T18:29:20.072214Z",
     "shell.execute_reply.started": "2025-04-08T18:29:16.333093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_segments = 20\n",
    "features_audio_input_path = f\"/kaggle/input/truthlie-audio-features/truthlie_audio_original_{sequence_length}_frame_{num_segments}_30\"\n",
    "\n",
    "# Load previously saved arrays\n",
    "train_features_audio_holdout_early = np.load(features_audio_input_path + \"/train_features_audio.npy\")\n",
    "val_features_audio_holdout_early = np.load(features_audio_input_path + \"/val_features_audio.npy\")\n",
    "test_features_audio_holdout_early = np.load(features_audio_input_path + \"/test_features_audio.npy\")\n",
    "train_labels_audio_holdout_early = np.load(features_audio_input_path + \"/train_labels_audio.npy\")\n",
    "val_labels_audio_holdout_early = np.load(features_audio_input_path + \"/val_labels_audio.npy\")\n",
    "test_labels_audio_holdout_early = np.load(features_audio_input_path + \"/test_labels_audio.npy\")\n",
    "print(train_features_audio_holdout_early.shape, train_labels_audio_holdout_early.shape)\n",
    "print(val_features_audio_holdout_early.shape, val_labels_audio_holdout_early.shape)\n",
    "print(test_features_audio_holdout_early.shape, test_labels_audio_holdout_early.shape)\n",
    "\n",
    "data_audio_cross_early, labels_audio_cross_early = [], []\n",
    "data_audio_cross_pad_early, labels_audio_cross_pad_early = [], []\n",
    "for i in range(4):\n",
    "    data_audio_cross_early.append(np.load(features_audio_input_path + f\"/fold_{i}_features_audio.npy\"))\n",
    "    labels_audio_cross_early.append(np.load(features_audio_input_path + f\"/fold_{i}_labels_audio.npy\"))\n",
    "    print(data_audio_cross_early[i].shape, labels_audio_cross_early[i].shape)\n",
    "\n",
    "    data_audio_cross_pad_early.append(np.load(features_audio_input_path + f\"/fold_{i}_features_audio_pad.npy\"))\n",
    "    labels_audio_cross_pad_early.append(np.load(features_audio_input_path + f\"/fold_{i}_labels_audio_pad.npy\"))\n",
    "    print(data_audio_cross_pad_early[i].shape, labels_audio_cross_pad_early[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:20.074968Z",
     "iopub.status.busy": "2025-04-08T18:29:20.074636Z",
     "iopub.status.idle": "2025-04-08T18:29:20.271146Z",
     "shell.execute_reply": "2025-04-08T18:29:20.270173Z",
     "shell.execute_reply.started": "2025-04-08T18:29:20.074928Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_new_audio_array_early(features_list,labels_list,audios_list,video_list):\n",
    "    new_audio_dict_early_features = {}\n",
    "    new_audio_dict_early_labels = {}\n",
    "    \n",
    "    for feature,label,name in zip(features_list,labels_list,audios_list):\n",
    "        real_name = name[:-4]\n",
    "        if real_name not in new_audio_dict_early_features.keys():\n",
    "            new_audio_dict_early_features[real_name] = []\n",
    "            new_audio_dict_early_labels[real_name] = []\n",
    "        new_audio_dict_early_features[real_name].append(feature)\n",
    "        new_audio_dict_early_labels[real_name].append(label)\n",
    "    \n",
    "    new_audio_test_features = []\n",
    "    new_audio_test_labels = []\n",
    "    for name in video_list:\n",
    "        real_name = name.split('/')[-1][:-4]\n",
    "        new_audio_test_features.extend(new_audio_dict_early_features[real_name])\n",
    "        new_audio_test_labels.extend(new_audio_dict_early_labels[real_name])\n",
    "    \n",
    "    return np.array(new_audio_test_features), np.array(new_audio_test_labels)\n",
    "\n",
    "train_features_audio_holdout_early, train_labels_audio_holdout_early = get_new_audio_array_early(train_features_audio_holdout_early,train_labels_audio_holdout_early,train_audio_names_holdout_early,new_train_list_early)\n",
    "print(train_features_audio_holdout_early.shape, train_labels_audio_holdout_early.shape)\n",
    "val_features_audio_holdout_early, val_labels_audio_holdout_early = get_new_audio_array_early(val_features_audio_holdout_early,val_labels_audio_holdout_early,val_audio_names_holdout_early,new_val_list_early)\n",
    "print(val_features_audio_holdout_early.shape, val_labels_audio_holdout_early.shape)\n",
    "test_features_audio_holdout_early, test_labels_audio_holdout_early = get_new_audio_array_early(test_features_audio_holdout_early,test_labels_audio_holdout_early,test_audio_names_holdout_early,new_test_list_early)\n",
    "print(test_features_audio_holdout_early.shape, test_labels_audio_holdout_early.shape)\n",
    "for i in range(4):\n",
    "    fold_features_audio_early, fold_labels_audio_early = get_new_audio_array_early(data_audio_cross_early[i],labels_audio_cross_early[i],names_audio_cross_early[i],new_cross_list_early[i])\n",
    "    print(fold_features_audio_early.shape, fold_labels_audio_early.shape)\n",
    "    data_audio_cross_early[i]=fold_features_audio_early\n",
    "    labels_audio_cross_early[i]=fold_labels_audio_early\n",
    "\n",
    "    fold_features_audio_early, fold_labels_audio_early = get_new_audio_array_early(data_audio_cross_pad_early[i],labels_audio_cross_pad_early[i],names_audio_cross_pad_early[i],new_cross_list_pad_early[i])\n",
    "    print(fold_features_audio_early.shape, fold_labels_audio_early.shape)\n",
    "    data_audio_cross_pad_early[i]=fold_features_audio_early\n",
    "    labels_audio_cross_pad_early[i]=fold_labels_audio_early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:20.273162Z",
     "iopub.status.busy": "2025-04-08T18:29:20.272394Z",
     "iopub.status.idle": "2025-04-08T18:29:20.281086Z",
     "shell.execute_reply": "2025-04-08T18:29:20.280170Z",
     "shell.execute_reply.started": "2025-04-08T18:29:20.273120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(all(train_labels_audio_holdout_early == train_labels_video_holdout_early))\n",
    "print(all(val_labels_audio_holdout_early == val_labels_video_holdout_early))\n",
    "print(all(test_labels_audio_holdout_early == test_labels_video_holdout_early))\n",
    "\n",
    "for i in range(4):\n",
    "    print(all(labels_audio_cross_early[i] == labels_video_cross_early[i]))\n",
    "    print(all(labels_audio_cross_pad_early[i] == labels_video_cross_pad_early[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:20.282405Z",
     "iopub.status.busy": "2025-04-08T18:29:20.282148Z",
     "iopub.status.idle": "2025-04-08T18:29:20.291242Z",
     "shell.execute_reply": "2025-04-08T18:29:20.290495Z",
     "shell.execute_reply.started": "2025-04-08T18:29:20.282380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine_features(features_video, features_audio):\n",
    "    return np.concatenate([features_video, features_audio], axis=-1)\n",
    "\n",
    "def create_lstm_model(input_shape, hidden_size=128, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        InputLayer(shape=input_shape),\n",
    "        Masking(mask_value=0.0),\n",
    "        LSTM(hidden_size, return_sequences=False, use_cudnn=False),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:20.292998Z",
     "iopub.status.busy": "2025-04-08T18:29:20.292282Z",
     "iopub.status.idle": "2025-04-08T18:29:20.381284Z",
     "shell.execute_reply": "2025-04-08T18:29:20.380530Z",
     "shell.execute_reply.started": "2025-04-08T18:29:20.292960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_features_combined_holdout = combine_features(train_features_video_holdout_early, train_features_audio_holdout_early)\n",
    "val_features_combined_holdout = combine_features(val_features_video_holdout_early, val_features_audio_holdout_early)\n",
    "test_features_combined_holdout = combine_features(test_features_video_holdout_early, test_features_audio_holdout_early)\n",
    "\n",
    "print(train_features_combined_holdout.shape, train_labels_video_holdout_early.shape)\n",
    "print(val_features_combined_holdout.shape, val_labels_video_holdout_early.shape)\n",
    "print(test_features_combined_holdout.shape, test_labels_video_holdout_early.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:20.383040Z",
     "iopub.status.busy": "2025-04-08T18:29:20.382735Z",
     "iopub.status.idle": "2025-04-08T18:29:53.231603Z",
     "shell.execute_reply": "2025-04-08T18:29:53.230747Z",
     "shell.execute_reply.started": "2025-04-08T18:29:20.383008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model creation\n",
    "combined_model_2_holdout = create_lstm_model(\n",
    "    input_shape=(train_features_combined_holdout.shape[1], train_features_combined_holdout.shape[2]),\n",
    "                 hidden_size=128, learning_rate=0.001)\n",
    "\n",
    "# Model training\n",
    "combined_history_2_holdout = combined_model_2_holdout.fit(\n",
    "    train_features_combined_holdout, train_labels_video_holdout_early,\n",
    "    validation_data=(val_features_combined_holdout, val_labels_video_holdout_early),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(combined_history_2_holdout)\n",
    "\n",
    "# Model evaluation\n",
    "test_predictions_holdout_proba_common_2 = combined_model_2_holdout.predict(test_features_combined_holdout)\n",
    "test_predictions_holdout_common_2 = (test_predictions_holdout_proba_common_2 > 0.5).astype(int)\n",
    " \n",
    "# Calculation of metrics\n",
    "accuracy_holdout_common_2 = accuracy_score(test_labels_video_holdout_early, test_predictions_holdout_common_2)\n",
    "precision_holdout_common_2 = precision_score(test_labels_video_holdout_early, test_predictions_holdout_common_2, zero_division=0, average=None)\n",
    "recall_holdout_common_2 = recall_score(test_labels_video_holdout_early, test_predictions_holdout_common_2, zero_division=0, average=None)\n",
    "f1_holdout_common_2 = f1_score(test_labels_video_holdout_early, test_predictions_holdout_common_2, average=None)\n",
    "auc_holdout_common_2 = roc_auc_score(test_labels_video_holdout_early, test_predictions_holdout_proba_common_2)\n",
    "\n",
    "# Displaying results\n",
    "print(\"***** SEQUENCE LEVEL RESULTS *****\\n\")\n",
    "conf_matrix_holdout_common_2 = confusion_matrix(test_labels_video_holdout_early, test_predictions_holdout_common_2)\n",
    "plot_confusion_matrix(conf_matrix_holdout_common_2)\n",
    " \n",
    "# Saving metrics in dictionary\n",
    "frame_metrics_holdout[\"Accuracy\"].append(accuracy_holdout_common_2)\n",
    "frame_metrics_holdout[\"Precision (0)\"].append(precision_holdout_common_2[0])\n",
    "frame_metrics_holdout[\"Precision (1)\"].append(precision_holdout_common_2[1])\n",
    "frame_metrics_holdout[\"Recall (0)\"].append(recall_holdout_common_2[0])\n",
    "frame_metrics_holdout[\"Recall (1)\"].append(recall_holdout_common_2[1])\n",
    "frame_metrics_holdout[\"F1 (0)\"].append(f1_holdout_common_2[0])\n",
    "frame_metrics_holdout[\"F1 (1)\"].append(f1_holdout_common_2[1])\n",
    "frame_metrics_holdout[\"AUC\"].append(auc_holdout_common_2)\n",
    "\n",
    "# Create a dataframe with predictions and video ids\n",
    "pred_df = pd.DataFrame({'video_id': test_video_names_holdout_early, 'prediction': test_predictions_holdout_proba_common_2.flatten(), 'label': test_labels_video_holdout_early})\n",
    "\n",
    "# Group by video and calculate the average\n",
    "video_predictions_mean = pred_df.groupby('video_id')['prediction'].apply(calculate_mean).values\n",
    "video_predictions_binary_mean = (video_predictions_mean > 0.5).astype(int)\n",
    "\n",
    "# Group by video and apply the majority voting rule\n",
    "video_predictions_binary_majority = pred_df.groupby('video_id')['prediction'].apply(calculate_majority).values\n",
    "\n",
    "# Group by video and apply the aggregation rule\n",
    "video_predictions_binary_threshold = pred_df.groupby('video_id')['prediction'].apply(aggregate_by_threshold).values    \n",
    "\n",
    "# Group by video and get labels\n",
    "video_labels = pred_df.groupby('video_id')['label'].first().values\n",
    "\n",
    "# Calculate metrics for video\n",
    "video_accuracy = accuracy_score(video_labels, video_predictions_binary_mean)\n",
    "video_f1 = f1_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "video_precision = precision_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "video_recall = recall_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "video_auc = roc_auc_score(video_labels, video_predictions_mean)\n",
    "\n",
    "video_metrics_holdout_mean[\"Accuracy\"].append(video_accuracy)\n",
    "video_metrics_holdout_mean[\"Precision (0)\"].append(video_precision[0])\n",
    "video_metrics_holdout_mean[\"Precision (1)\"].append(video_precision[1])\n",
    "video_metrics_holdout_mean[\"Recall (0)\"].append(video_recall[0])\n",
    "video_metrics_holdout_mean[\"Recall (1)\"].append(video_recall[1])\n",
    "video_metrics_holdout_mean[\"F1 (0)\"].append(video_f1[0])\n",
    "video_metrics_holdout_mean[\"F1 (1)\"].append(video_f1[1])\n",
    "video_metrics_holdout_mean[\"AUC\"].append(video_auc)\n",
    "\n",
    "print(\"\\n***** VIDEO LEVEL RESULTS (MEAN) *****\\n\")\n",
    "plot_confusion_matrix_2(video_labels, video_predictions_binary_mean)\n",
    "\n",
    "# Calculate metrics for video\n",
    "video_accuracy = accuracy_score(video_labels, video_predictions_binary_majority)\n",
    "video_f1 = f1_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "video_precision = precision_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "video_recall = recall_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "video_auc = roc_auc_score(video_labels, video_predictions_binary_majority)\n",
    "\n",
    "video_metrics_holdout_majority[\"Accuracy\"].append(video_accuracy)\n",
    "video_metrics_holdout_majority[\"Precision (0)\"].append(video_precision[0])\n",
    "video_metrics_holdout_majority[\"Precision (1)\"].append(video_precision[1])\n",
    "video_metrics_holdout_majority[\"Recall (0)\"].append(video_recall[0])\n",
    "video_metrics_holdout_majority[\"Recall (1)\"].append(video_recall[1])\n",
    "video_metrics_holdout_majority[\"F1 (0)\"].append(video_f1[0])\n",
    "video_metrics_holdout_majority[\"F1 (1)\"].append(video_f1[1])\n",
    "video_metrics_holdout_majority[\"AUC\"].append(video_auc)\n",
    "\n",
    "print(\"\\n***** VIDEO LEVEL RESULTS (MAJORITY) *****\\n\")\n",
    "plot_confusion_matrix_2(video_labels, video_predictions_binary_majority)\n",
    "\n",
    "# Calculate metrics for video\n",
    "video_accuracy = accuracy_score(video_labels, video_predictions_binary_threshold)\n",
    "video_f1 = f1_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "video_precision = precision_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "video_recall = recall_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "video_auc = roc_auc_score(video_labels, video_predictions_binary_threshold)\n",
    "\n",
    "video_metrics_holdout_threshold[\"Accuracy\"].append(video_accuracy)\n",
    "video_metrics_holdout_threshold[\"Precision (0)\"].append(video_precision[0])\n",
    "video_metrics_holdout_threshold[\"Precision (1)\"].append(video_precision[1])\n",
    "video_metrics_holdout_threshold[\"Recall (0)\"].append(video_recall[0])\n",
    "video_metrics_holdout_threshold[\"Recall (1)\"].append(video_recall[1])\n",
    "video_metrics_holdout_threshold[\"F1 (0)\"].append(video_f1[0])\n",
    "video_metrics_holdout_threshold[\"F1 (1)\"].append(video_f1[1])\n",
    "video_metrics_holdout_threshold[\"AUC\"].append(video_auc)\n",
    "\n",
    "print(\"\\n***** VIDEO LEVEL RESULTS (THRESHOLD) *****\\n\")\n",
    "plot_confusion_matrix_2(video_labels, video_predictions_binary_threshold)\n",
    "\n",
    "# Creation of dataframe\n",
    "df = pd.DataFrame.from_dict(frame_metrics_holdout, orient='index', columns=[f\"EARLY FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** FRAME LEVEL RESULTS *****\\n\")\n",
    "print(df)\n",
    "frame_metrics_holdout = {key: [] for key in frame_metrics_holdout}\n",
    "\n",
    "# Creation of dataframe\n",
    "df = pd.DataFrame.from_dict(video_metrics_holdout_mean, orient='index', columns=[f\"EARLY FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (MEAN) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_holdout_mean = {key: [] for key in video_metrics_holdout_mean}\n",
    "\n",
    "# Creation of dataframe\n",
    "df = pd.DataFrame.from_dict(video_metrics_holdout_majority, orient='index', columns=[f\"EARLY FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (MAJORITY) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_holdout_majority = {key: [] for key in video_metrics_holdout_majority}\n",
    "\n",
    "# Creation of dataframe\n",
    "df = pd.DataFrame.from_dict(video_metrics_holdout_threshold, orient='index', columns=[f\"EARLY FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (THRESHOLD) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_holdout_threshold = {key: [] for key in video_metrics_holdout_threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:53.233018Z",
     "iopub.status.busy": "2025-04-08T18:29:53.232776Z",
     "iopub.status.idle": "2025-04-08T18:29:53.268025Z",
     "shell.execute_reply": "2025-04-08T18:29:53.267162Z",
     "shell.execute_reply.started": "2025-04-08T18:29:53.232995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_video_audio_combined_2(frame_metrics_dict, video_metrics_dict_mean,\n",
    "                                            video_metrics_dict_majority, video_metrics_dict_threshold,\n",
    "                                            data_video, data_video_pad, data_audio, data_audio_pad,\n",
    "                                            labels, labels_pad, names_video, names_video_pad,\n",
    "                                            num_folds=4):\n",
    "    # Performance List Videos\n",
    "    frame_accuracy_score = [] \n",
    "    frame_f1_score_0 = [] \n",
    "    frame_precision_score_0 = [] \n",
    "    frame_recall_score_0 = []\n",
    "    frame_f1_score_1 = [] \n",
    "    frame_precision_score_1 = [] \n",
    "    frame_recall_score_1 = []\n",
    "    frame_auc_score = []\n",
    "    \n",
    "    # Performance List Videos\n",
    "    video_accuracy_score_mean = [] \n",
    "    video_f1_score_mean_0 = [] \n",
    "    video_precision_score_mean_0 = [] \n",
    "    video_recall_score_mean_0 = []\n",
    "    video_f1_score_mean_1 = [] \n",
    "    video_precision_score_mean_1 = [] \n",
    "    video_recall_score_mean_1 = []\n",
    "    video_auc_score_mean = []\n",
    "\n",
    "    # Performance List Videos\n",
    "    video_accuracy_score_majority = [] \n",
    "    video_f1_score_majority_0 = [] \n",
    "    video_precision_score_majority_0 = [] \n",
    "    video_recall_score_majority_0 = []\n",
    "    video_f1_score_majority_1 = [] \n",
    "    video_precision_score_majority_1 = [] \n",
    "    video_recall_score_majority_1 = []\n",
    "    video_auc_score_majority = []\n",
    "\n",
    "    # Performance List Videos\n",
    "    video_accuracy_score_threshold = [] \n",
    "    video_f1_score_threshold_0 = [] \n",
    "    video_precision_score_threshold_0 = [] \n",
    "    video_recall_score_threshold_0 = []\n",
    "    video_f1_score_threshold_1 = [] \n",
    "    video_precision_score_threshold_1 = [] \n",
    "    video_recall_score_threshold_1 = []\n",
    "    video_auc_score_threshold = []\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        print(f'\\nFold {i+1}/{num_folds}')\n",
    "        \n",
    "        # Fold splitting in train and test\n",
    "        test_video, test_audio, test_labels, test_video_names = data_video_pad[i], data_audio_pad[i], labels_pad[i], names_video_pad[i]\n",
    "        train_video = np.array([item for idx, fold in enumerate(data_video) if idx != i for item in fold])\n",
    "        train_audio = np.array([item for idx, fold in enumerate(data_audio) if idx != i for item in fold])\n",
    "        train_labels = np.array([label for idx, fold in enumerate(labels) if idx != i for label in fold])\n",
    "        train_video_names = np.array([item for idx, fold in enumerate(names_video) if idx != i for item in fold])\n",
    "    \n",
    "        # Features combination\n",
    "        train_combined = combine_features(train_video, train_audio)\n",
    "        test_combined = combine_features(test_video, test_audio)\n",
    "\n",
    "        print(train_combined.shape, train_labels.shape)\n",
    "        print(test_combined.shape, test_labels.shape)\n",
    "        \n",
    "        # Model creation\n",
    "        model = create_lstm_model(\n",
    "            input_shape=(train_combined.shape[1], train_combined.shape[2]),\n",
    "                         hidden_size=128, learning_rate=0.001)\n",
    "        \n",
    "        # Model training\n",
    "        history = model.fit(\n",
    "            train_combined, train_labels,\n",
    "            epochs=20,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fold evaluation\n",
    "        predictions_proba = model.predict(test_combined)\n",
    "        predictions = (predictions_proba > 0.5).astype(int)\n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        precision = precision_score(test_labels, predictions, zero_division=0, average=None)\n",
    "        recall = recall_score(test_labels, predictions, zero_division=0, average=None)\n",
    "        f1 = f1_score(test_labels, predictions, average=None)\n",
    "        auc = roc_auc_score(test_labels, predictions_proba)\n",
    "        \n",
    "        # Saving metrics\n",
    "        frame_accuracy_score.append(accuracy)\n",
    "        frame_precision_score_0.append(precision[0])\n",
    "        frame_recall_score_0.append(recall[0])\n",
    "        frame_f1_score_0.append(f1[0])\n",
    "        frame_precision_score_1.append(precision[1])\n",
    "        frame_recall_score_1.append(recall[1])\n",
    "        frame_f1_score_1.append(f1[1])\n",
    "        frame_auc_score.append(auc)\n",
    "\n",
    "        # Create a dataframe with predictions and video ids\n",
    "        pred_df = pd.DataFrame({'video_id': test_video_names, 'prediction': predictions_proba.flatten(), 'label': test_labels})\n",
    "\n",
    "        # Group by video and calculate the average\n",
    "        video_predictions_mean = pred_df.groupby('video_id')['prediction'].apply(calculate_mean).values\n",
    "        video_predictions_binary_mean = (video_predictions_mean > 0.5).astype(int)\n",
    "        \n",
    "        # Group by video and apply the majority voting rule\n",
    "        video_predictions_binary_majority = pred_df.groupby('video_id')['prediction'].apply(calculate_majority).values\n",
    "    \n",
    "        # Group by video and apply the aggregation rule\n",
    "        video_predictions_binary_threshold = pred_df.groupby('video_id')['prediction'].apply(aggregate_by_threshold).values    \n",
    "        \n",
    "        # Group by video and get labels\n",
    "        video_labels = pred_df.groupby('video_id')['label'].first().values\n",
    "\n",
    "        # Calculate metrics for videos\n",
    "        mean_video_accuracy = accuracy_score(video_labels, video_predictions_binary_mean)\n",
    "        mean_video_f1 = f1_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "        mean_video_precision = precision_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "        mean_video_recall = recall_score(video_labels, video_predictions_binary_mean, zero_division=0, average=None)\n",
    "        mean_video_auc = roc_auc_score(video_labels, video_predictions_mean)\n",
    "\n",
    "        majority_video_accuracy = accuracy_score(video_labels, video_predictions_binary_majority)\n",
    "        majority_video_f1 = f1_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "        majority_video_precision = precision_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "        majority_video_recall = recall_score(video_labels, video_predictions_binary_majority, zero_division=0, average=None)\n",
    "        majority_video_auc = roc_auc_score(video_labels, video_predictions_binary_majority)\n",
    "\n",
    "        threshold_video_accuracy = accuracy_score(video_labels, video_predictions_binary_threshold)\n",
    "        threshold_video_f1 = f1_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "        threshold_video_precision = precision_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "        threshold_video_recall = recall_score(video_labels, video_predictions_binary_threshold, zero_division=0, average=None)\n",
    "        threshold_video_auc = roc_auc_score(video_labels, video_predictions_binary_threshold)\n",
    "\n",
    "        # Saving metrics\n",
    "        video_accuracy_score_mean.append(mean_video_accuracy)\n",
    "        video_f1_score_mean_0.append(mean_video_f1[0])\n",
    "        video_precision_score_mean_0.append(mean_video_precision[0])\n",
    "        video_recall_score_mean_0.append(mean_video_recall[0])\n",
    "        video_f1_score_mean_1.append(mean_video_f1[1])\n",
    "        video_precision_score_mean_1.append(mean_video_precision[1])\n",
    "        video_recall_score_mean_1.append(mean_video_recall[1])\n",
    "        video_auc_score_mean.append(mean_video_auc)\n",
    "\n",
    "        video_accuracy_score_majority.append(majority_video_accuracy)\n",
    "        video_f1_score_majority_0.append(majority_video_f1[0])\n",
    "        video_precision_score_majority_0.append(majority_video_precision[0])\n",
    "        video_recall_score_majority_0.append(majority_video_recall[0])\n",
    "        video_f1_score_majority_1.append(majority_video_f1[1])\n",
    "        video_precision_score_majority_1.append(majority_video_precision[1])\n",
    "        video_recall_score_majority_1.append(majority_video_recall[1])\n",
    "        video_auc_score_majority.append(majority_video_auc)\n",
    "\n",
    "        video_accuracy_score_threshold.append(threshold_video_accuracy)\n",
    "        video_f1_score_threshold_0.append(threshold_video_f1[0])\n",
    "        video_precision_score_threshold_0.append(threshold_video_precision[0])\n",
    "        video_recall_score_threshold_0.append(threshold_video_recall[0])\n",
    "        video_f1_score_threshold_1.append(threshold_video_f1[1])\n",
    "        video_precision_score_threshold_1.append(threshold_video_precision[1])\n",
    "        video_recall_score_threshold_1.append(threshold_video_recall[1])\n",
    "        video_auc_score_threshold.append(threshold_video_auc)\n",
    "\n",
    "        print(f\"Accuracy fold (frame-based): {accuracy}\")\n",
    "        print(f\"Accuracy fold (video-based-mean): {mean_video_accuracy}\")\n",
    "        print(f\"Accuracy fold (video-based-majority): {majority_video_accuracy}\")\n",
    "        print(f\"Accuracy fold (video-based-threshold): {threshold_video_accuracy}\")\n",
    "    \n",
    "    # Metrics average on all frames\n",
    "    frame_avg_accuracy = np.mean(frame_accuracy_score)\n",
    "    frame_avg_precision_0, frame_avg_recall_0, frame_avg_f1_0 = np.mean(frame_precision_score_0), np.mean(frame_recall_score_0), np.mean(frame_f1_score_0)\n",
    "    frame_avg_precision_1, frame_avg_recall_1, frame_avg_f1_1 = np.mean(frame_precision_score_1), np.mean(frame_recall_score_1), np.mean(frame_f1_score_1)\n",
    "    frame_avg_auc = np.mean(frame_auc_score)\n",
    "\n",
    "    frame_metrics_dict[\"Mean Accuracy\"].append(frame_avg_accuracy)\n",
    "    frame_metrics_dict[\"Mean Precision (0)\"].append(frame_avg_precision_0)\n",
    "    frame_metrics_dict[\"Mean Precision (1)\"].append(frame_avg_precision_1)\n",
    "    frame_metrics_dict[\"Mean Recall (0)\"].append(frame_avg_recall_0)\n",
    "    frame_metrics_dict[\"Mean Recall (1)\"].append(frame_avg_recall_1)\n",
    "    frame_metrics_dict[\"Mean F1 (0)\"].append(frame_avg_f1_0)\n",
    "    frame_metrics_dict[\"Mean F1 (1)\"].append(frame_avg_f1_1)\n",
    "    frame_metrics_dict[\"Mean AUC\"].append(frame_avg_auc)\n",
    "\n",
    "    # Metrics standard deviation on all frames\n",
    "    frame_std_accuracy = np.std(frame_accuracy_score)\n",
    "    frame_std_precision_0, frame_std_recall_0, frame_std_f1_0 = np.std(frame_precision_score_0), np.std(frame_recall_score_0), np.std(frame_f1_score_0)\n",
    "    frame_std_precision_1, frame_std_recall_1, frame_std_f1_1 = np.std(frame_precision_score_1), np.std(frame_recall_score_1), np.std(frame_f1_score_1)\n",
    "    frame_std_auc = np.std(frame_auc_score)\n",
    "\n",
    "    frame_metrics_dict[\"Std Accuracy\"].append(frame_std_accuracy)\n",
    "    frame_metrics_dict[\"Std Precision (0)\"].append(frame_std_precision_0)\n",
    "    frame_metrics_dict[\"Std Precision (1)\"].append(frame_std_precision_1)\n",
    "    frame_metrics_dict[\"Std Recall (0)\"].append(frame_std_recall_0)\n",
    "    frame_metrics_dict[\"Std Recall (1)\"].append(frame_std_recall_1)\n",
    "    frame_metrics_dict[\"Std F1 (0)\"].append(frame_std_f1_0)\n",
    "    frame_metrics_dict[\"Std F1 (1)\"].append(frame_std_f1_1)\n",
    "    frame_metrics_dict[\"Std AUC\"].append(frame_std_auc)\n",
    "        \n",
    "    print(\"\\n\\nRESULTS on FRAMES:\\n\")\n",
    "\n",
    "    print(\"Mean Accuracy:\", frame_avg_accuracy)\n",
    "    print(\"Accuracy std:\", frame_std_accuracy)\n",
    "\n",
    "    # Metrics average on all videos\n",
    "    video_avg_accuracy = np.mean(video_accuracy_score_mean)\n",
    "    video_avg_precision_0, video_avg_recall_0, video_avg_f1_0 = np.mean(video_precision_score_mean_0), np.mean(video_recall_score_mean_0), np.mean(video_f1_score_mean_0)\n",
    "    video_avg_precision_1, video_avg_recall_1, video_avg_f1_1 = np.mean(video_precision_score_mean_1), np.mean(video_recall_score_mean_1), np.mean(video_f1_score_mean_1)\n",
    "    video_avg_auc = np.mean(video_auc_score_mean)\n",
    "\n",
    "    video_metrics_dict_mean[\"Mean Accuracy\"].append(video_avg_accuracy)\n",
    "    video_metrics_dict_mean[\"Mean Precision (0)\"].append(video_avg_precision_0)\n",
    "    video_metrics_dict_mean[\"Mean Precision (1)\"].append(video_avg_precision_1)\n",
    "    video_metrics_dict_mean[\"Mean Recall (0)\"].append(video_avg_recall_0)\n",
    "    video_metrics_dict_mean[\"Mean Recall (1)\"].append(video_avg_recall_1)\n",
    "    video_metrics_dict_mean[\"Mean F1 (0)\"].append(video_avg_f1_0)\n",
    "    video_metrics_dict_mean[\"Mean F1 (1)\"].append(video_avg_f1_1)\n",
    "    video_metrics_dict_mean[\"Mean AUC\"].append(video_avg_auc)\n",
    "\n",
    "    # Metrics standard deviation on all videos\n",
    "    video_std_accuracy = np.std(video_accuracy_score_mean)\n",
    "    video_std_precision_0, video_std_recall_0, video_std_f1_0 = np.std(video_precision_score_mean_0), np.std(video_recall_score_mean_0), np.std(video_f1_score_mean_0)\n",
    "    video_std_precision_1, video_std_recall_1, video_std_f1_1 = np.std(video_precision_score_mean_1), np.std(video_recall_score_mean_1), np.std(video_f1_score_mean_1)\n",
    "    video_std_auc = np.std(video_auc_score_mean)\n",
    "\n",
    "    video_metrics_dict_mean[\"Std Accuracy\"].append(video_std_accuracy)\n",
    "    video_metrics_dict_mean[\"Std Precision (0)\"].append(video_std_precision_0)\n",
    "    video_metrics_dict_mean[\"Std Precision (1)\"].append(video_std_precision_1)\n",
    "    video_metrics_dict_mean[\"Std Recall (0)\"].append(video_std_recall_0)\n",
    "    video_metrics_dict_mean[\"Std Recall (1)\"].append(video_std_recall_1)\n",
    "    video_metrics_dict_mean[\"Std F1 (0)\"].append(video_std_f1_0)\n",
    "    video_metrics_dict_mean[\"Std F1 (1)\"].append(video_std_f1_1)\n",
    "    video_metrics_dict_mean[\"Std AUC\"].append(video_std_auc)\n",
    "    \n",
    "    print(\"\\n\\nRESULTS on VIDEOS (MEAN):\\n\")\n",
    "    \n",
    "    print(\"Mean Accuracy:\", video_avg_accuracy)\n",
    "    print(\"Accuracy std:\", video_std_accuracy)\n",
    "\n",
    "    # Metrics average on all videos\n",
    "    video_avg_accuracy = np.mean(video_accuracy_score_majority)\n",
    "    video_avg_precision_0, video_avg_recall_0, video_avg_f1_0 = np.mean(video_precision_score_majority_0), np.mean(video_recall_score_majority_0), np.mean(video_f1_score_majority_0)\n",
    "    video_avg_precision_1, video_avg_recall_1, video_avg_f1_1 = np.mean(video_precision_score_majority_1), np.mean(video_recall_score_majority_1), np.mean(video_f1_score_majority_1)\n",
    "    video_avg_auc = np.mean(video_auc_score_majority)\n",
    "\n",
    "    video_metrics_dict_majority[\"Mean Accuracy\"].append(video_avg_accuracy)\n",
    "    video_metrics_dict_majority[\"Mean Precision (0)\"].append(video_avg_precision_0)\n",
    "    video_metrics_dict_majority[\"Mean Precision (1)\"].append(video_avg_precision_1)\n",
    "    video_metrics_dict_majority[\"Mean Recall (0)\"].append(video_avg_recall_0)\n",
    "    video_metrics_dict_majority[\"Mean Recall (1)\"].append(video_avg_recall_1)\n",
    "    video_metrics_dict_majority[\"Mean F1 (0)\"].append(video_avg_f1_0)\n",
    "    video_metrics_dict_majority[\"Mean F1 (1)\"].append(video_avg_f1_1)\n",
    "    video_metrics_dict_majority[\"Mean AUC\"].append(video_avg_auc)\n",
    "\n",
    "    # Metrics standard deviation on all videos\n",
    "    video_std_accuracy = np.std(video_accuracy_score_majority)\n",
    "    video_std_precision_0, video_std_recall_0, video_std_f1_0 = np.std(video_precision_score_majority_0), np.std(video_recall_score_majority_0), np.std(video_f1_score_majority_0)\n",
    "    video_std_precision_1, video_std_recall_1, video_std_f1_1 = np.std(video_precision_score_majority_1), np.std(video_recall_score_majority_1), np.std(video_f1_score_majority_1)\n",
    "    video_std_auc = np.std(video_auc_score_majority)\n",
    "\n",
    "    video_metrics_dict_majority[\"Std Accuracy\"].append(video_std_accuracy)\n",
    "    video_metrics_dict_majority[\"Std Precision (0)\"].append(video_std_precision_0)\n",
    "    video_metrics_dict_majority[\"Std Precision (1)\"].append(video_std_precision_1)\n",
    "    video_metrics_dict_majority[\"Std Recall (0)\"].append(video_std_recall_0)\n",
    "    video_metrics_dict_majority[\"Std Recall (1)\"].append(video_std_recall_1)\n",
    "    video_metrics_dict_majority[\"Std F1 (0)\"].append(video_std_f1_0)\n",
    "    video_metrics_dict_majority[\"Std F1 (1)\"].append(video_std_f1_1)\n",
    "    video_metrics_dict_majority[\"Std AUC\"].append(video_std_auc)\n",
    "    \n",
    "    print(\"\\n\\nRESULTS on VIDEOS (MAJORITY):\\n\")\n",
    "    \n",
    "    print(\"Mean Accuracy:\", video_avg_accuracy)\n",
    "    print(\"Accuracy std:\", video_std_accuracy)\n",
    "\n",
    "    # Metrics average on all videos\n",
    "    video_avg_accuracy = np.mean(video_accuracy_score_threshold)\n",
    "    video_avg_precision_0, video_avg_recall_0, video_avg_f1_0 = np.mean(video_precision_score_threshold_0), np.mean(video_recall_score_threshold_0), np.mean(video_f1_score_threshold_0)\n",
    "    video_avg_precision_1, video_avg_recall_1, video_avg_f1_1 = np.mean(video_precision_score_threshold_1), np.mean(video_recall_score_threshold_1), np.mean(video_f1_score_threshold_1)\n",
    "    video_avg_auc = np.mean(video_auc_score_threshold)\n",
    "\n",
    "    video_metrics_dict_threshold[\"Mean Accuracy\"].append(video_avg_accuracy)\n",
    "    video_metrics_dict_threshold[\"Mean Precision (0)\"].append(video_avg_precision_0)\n",
    "    video_metrics_dict_threshold[\"Mean Precision (1)\"].append(video_avg_precision_1)\n",
    "    video_metrics_dict_threshold[\"Mean Recall (0)\"].append(video_avg_recall_0)\n",
    "    video_metrics_dict_threshold[\"Mean Recall (1)\"].append(video_avg_recall_1)\n",
    "    video_metrics_dict_threshold[\"Mean F1 (0)\"].append(video_avg_f1_0)\n",
    "    video_metrics_dict_threshold[\"Mean F1 (1)\"].append(video_avg_f1_1)\n",
    "    video_metrics_dict_threshold[\"Mean AUC\"].append(video_avg_auc)\n",
    "\n",
    "    # Metrics standard deviation on all videos\n",
    "    video_std_accuracy = np.std(video_accuracy_score_threshold)\n",
    "    video_std_precision_0, video_std_recall_0, video_std_f1_0 = np.std(video_precision_score_threshold_0), np.std(video_recall_score_threshold_0), np.std(video_f1_score_threshold_0)\n",
    "    video_std_precision_1, video_std_recall_1, video_std_f1_1 = np.std(video_precision_score_threshold_1), np.std(video_recall_score_threshold_1), np.std(video_f1_score_threshold_1)\n",
    "    video_std_auc = np.std(video_auc_score_threshold)\n",
    "\n",
    "    video_metrics_dict_threshold[\"Std Accuracy\"].append(video_std_accuracy)\n",
    "    video_metrics_dict_threshold[\"Std Precision (0)\"].append(video_std_precision_0)\n",
    "    video_metrics_dict_threshold[\"Std Precision (1)\"].append(video_std_precision_1)\n",
    "    video_metrics_dict_threshold[\"Std Recall (0)\"].append(video_std_recall_0)\n",
    "    video_metrics_dict_threshold[\"Std Recall (1)\"].append(video_std_recall_1)\n",
    "    video_metrics_dict_threshold[\"Std F1 (0)\"].append(video_std_f1_0)\n",
    "    video_metrics_dict_threshold[\"Std F1 (1)\"].append(video_std_f1_1)\n",
    "    video_metrics_dict_threshold[\"Std AUC\"].append(video_std_auc)\n",
    "    \n",
    "    print(\"\\n\\nRESULTS on VIDEOS (THRESHOLD):\\n\")\n",
    "    \n",
    "    print(\"Mean Accuracy:\", video_avg_accuracy)\n",
    "    print(\"Accuracy std:\", video_std_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:29:53.270052Z",
     "iopub.status.busy": "2025-04-08T18:29:53.269288Z",
     "iopub.status.idle": "2025-04-08T18:32:06.845061Z",
     "shell.execute_reply": "2025-04-08T18:32:06.844211Z",
     "shell.execute_reply.started": "2025-04-08T18:29:53.270012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cross_validation_video_audio_combined_2(frame_metrics_cross, video_metrics_cross_mean,\n",
    "                                            video_metrics_cross_majority, video_metrics_cross_threshold,\n",
    "                                         data_video_cross_early, data_video_cross_pad_early,\n",
    "                                         data_audio_cross_early, data_audio_cross_pad_early,\n",
    "                                         labels_video_cross_early, labels_video_cross_pad_early,\n",
    "                                         names_video_cross_early, names_video_cross_pad_early,\n",
    "                                         num_folds=4)\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(frame_metrics_cross, orient='index', columns=[f\"EARLY FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** FRAME LEVEL RESULTS *****\\n\")\n",
    "print(df)\n",
    "frame_metrics_cross = {key: [] for key in frame_metrics_cross}\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(video_metrics_cross_mean, orient='index', columns=[f\"EARLY FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (MEAN) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_cross_mean = {key: [] for key in video_metrics_cross_mean}\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(video_metrics_cross_majority, orient='index', columns=[f\"EARLY FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (MAJORITY) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_cross_majority = {key: [] for key in video_metrics_cross_majority}\n",
    "\n",
    "# Dataframe creation\n",
    "df = pd.DataFrame.from_dict(video_metrics_cross_threshold, orient='index', columns=[f\"EARLY FUSION {sequence_length}-{num_segments}\"])\n",
    "print(\"***** VIDEO LEVEL RESULTS (THRESHOLD) *****\\n\")\n",
    "print(df)\n",
    "video_metrics_cross_threshold = {key: [] for key in video_metrics_cross_threshold}"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5806343,
     "sourceId": 9944759,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5303547,
     "sourceId": 10059115,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5806090,
     "sourceId": 10129767,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6607967,
     "sourceId": 10803896,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
